{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sample Work This repository is a small sample of some work I've done both in my free time as as part of my MS in Data Science. Collaborative Projects Traffic Analysis Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SKLearn Methods: Data exploration, merging multiple data sources, data visualization, subsetting, feature engineering, automated multivariate feature selection, correlation matrixes, test/training splits, grid search, confusion matrixes, hyperparameter tuning, decision tree analysis, logistic regression, k-nearest neighbors, stacked ensemble, model scoring. Summary: Using publicly available traffic data from the city of Portland, OR, my partners and I developed an analysis of key predictors for speeding behavior in the Portland Metro area. My role focused on the data exploration, data source merging, and final model generation. EV Charging Optimization Tools: R, RMarkdown, lessR, ggplot, Kable, ROI, OMPR, dplyr Methods: Data exploration, outlier identification & removal, data visualization, model formulation & optimization Summary: Using sanitized data provided by Portland State University's Transportation & Parking department, my team and I created an optimization model for installing new EV chargers on campus to meet current and future demands, including an alternative solution that may have greater benefit. My role involved formulating and implementing both optimization models as well as variable development. Individual Projects Fraudulent Account Detection Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SKLearn Methods: Data exploration, merging multiple data sources, feature engineering, VIF, multivariate feature selection, correlation matrixes, test/training split, hyperparameter tuning, data scaling, k-nearest-neighbors, model scoring. Summary: Using data provided from StrataScratch, I developed a model to predict fraudulently created banking accounts. AB Testing Game Changes Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SciPy, Plotly Methods: Data exploration, outlier management, statistical testing/t-test, data visualization Summary: Using publicly available data I analyzed the impact on player retention of changing a key componant to an online game. Database Updates Tools: MySQL Methods: Stored procedures, temporal database updates Summary: A small example of managing updates to a customer database to maintain records while aging out inactive accounts. Housing Price Prediction Tools: R, RMarkdown, lessR, dplyr, tidyr, stringr, glmnet, Kable Methods: Data exploration, data visualization, correlation matrixes, logistic regression, hypothesis testing, confidence intervals, model fit, colllinearity evaluation, prediction intervals. Summary: Using data from Kaggle, I conducted a modely statistical analysis of housing features & prices to generate a model that predicts a house's price based on the selected features. HBR Accounting Reports Tools: Word, Excel Methods: Activity based costing, single cost drivers, billing models, pooled cost drivers, salary calculations Summary: These are a few examples of some accounting reports generated from Harvard Business Review documents. I'm mostly including this for more of an example of my writing style than anything else.","title":"Sample Work"},{"location":"#sample-work","text":"This repository is a small sample of some work I've done both in my free time as as part of my MS in Data Science.","title":"Sample Work"},{"location":"#collaborative-projects","text":"Traffic Analysis Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SKLearn Methods: Data exploration, merging multiple data sources, data visualization, subsetting, feature engineering, automated multivariate feature selection, correlation matrixes, test/training splits, grid search, confusion matrixes, hyperparameter tuning, decision tree analysis, logistic regression, k-nearest neighbors, stacked ensemble, model scoring. Summary: Using publicly available traffic data from the city of Portland, OR, my partners and I developed an analysis of key predictors for speeding behavior in the Portland Metro area. My role focused on the data exploration, data source merging, and final model generation. EV Charging Optimization Tools: R, RMarkdown, lessR, ggplot, Kable, ROI, OMPR, dplyr Methods: Data exploration, outlier identification & removal, data visualization, model formulation & optimization Summary: Using sanitized data provided by Portland State University's Transportation & Parking department, my team and I created an optimization model for installing new EV chargers on campus to meet current and future demands, including an alternative solution that may have greater benefit. My role involved formulating and implementing both optimization models as well as variable development.","title":"Collaborative Projects"},{"location":"#individual-projects","text":"Fraudulent Account Detection Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SKLearn Methods: Data exploration, merging multiple data sources, feature engineering, VIF, multivariate feature selection, correlation matrixes, test/training split, hyperparameter tuning, data scaling, k-nearest-neighbors, model scoring. Summary: Using data provided from StrataScratch, I developed a model to predict fraudulently created banking accounts. AB Testing Game Changes Tools: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, SciPy, Plotly Methods: Data exploration, outlier management, statistical testing/t-test, data visualization Summary: Using publicly available data I analyzed the impact on player retention of changing a key componant to an online game. Database Updates Tools: MySQL Methods: Stored procedures, temporal database updates Summary: A small example of managing updates to a customer database to maintain records while aging out inactive accounts. Housing Price Prediction Tools: R, RMarkdown, lessR, dplyr, tidyr, stringr, glmnet, Kable Methods: Data exploration, data visualization, correlation matrixes, logistic regression, hypothesis testing, confidence intervals, model fit, colllinearity evaluation, prediction intervals. Summary: Using data from Kaggle, I conducted a modely statistical analysis of housing features & prices to generate a model that predicts a house's price based on the selected features.","title":"Individual Projects"},{"location":"#hbr-accounting-reports","text":"Tools: Word, Excel Methods: Activity based costing, single cost drivers, billing models, pooled cost drivers, salary calculations Summary: These are a few examples of some accounting reports generated from Harvard Business Review documents. I'm mostly including this for more of an example of my writing style than anything else.","title":"HBR Accounting Reports"},{"location":"Database_Updates/Database_Updates/","text":"Database Updates - Validation Establish database & tables -- setup database CREATE DATABASE Customer; SHOW DATABASES; USE Customer; -- setup staging table CREATE TABLE Customer.CustomerChurn_Stage ( CustomerId INT PRIMARY KEY, Surname VARCHAR(50), CreditScore SMALLINT, Geography VARCHAR(50), Gender VARCHAR(15), Age TINYINT, Balance DECIMAL(10,2), Exited TINYINT ); DESCRIBE Customer.CustomerChurn_Stage; -- setup persistent table CREATE TABLE Customer.CustomerChurn ( CustomerId INT PRIMARY KEY, Surname VARCHAR(50), CreditScore SMALLINT, Geography VARCHAR(50), Gender VARCHAR(15), Age TINYINT, Balance DECIMAL(10,2), Exited TINYINT, SourceSystemNm CHAR(20) NOT NULL, CreateAgentId CHAR(20) NOT NULL, CreateDtm DATETIME NOT NULL, ChangeAgentId CHAR(20) NOT NULL, ChangeDtm DATETIME NOT NULL); DESCRIBE Customer.CustomerChurn; -- get row count from stage table SELECT COUNT(*) FROM Customer.CustomerChurn_Stage; -- get last rows from stage SELECT * FROM Customer.CustomerChurn_Stage ORDER BY CustomerID DESC LIMIT 3; Run stored procedure & compare -- call procedure CALL `Customer`.`Customer.PrCustomerChurn`(); -- confirm row counts SELECT COUNT(*) AS 'Persistent Table Rows' FROM Customer.CustomerChurn; SELECT COUNT(*) AS 'Staging Table Rows' FROM Customer.CustomerChurn_Stage; -- get last rows from persistent SELECT * FROM Customer.CustomerChurn ORDER BY CustomerID DESC LIMIT 3; -- create new table duplicating persistent table as V1 CREATE TABLE Customer.CustomerChurn_Version1 AS SELECT * FROM Customer.CustomerChurn; DESCRIBE Customer.CustomerChurn_Version1; SELECT COUNT(*) FROM Customer.CustomerChurn_Version1; SELECT * FROM Customer.CustomerChurn_Version1 ORDER BY CustomerID DESC LIMIT 3; -- empty the staging table TRUNCATE TABLE Customer.CustomerChurn_Stage; SELECT * FROM Customer.CustomerChurn_Stage; -- row info from new data in staging SELECT COUNT(*) FROM Customer.CustomerChurn_Stage; SELECT * FROM Customer.CustomerChurn_Stage ORDER BY CustomerID DESC LIMIT 3; Rerun stored procedure -- rerun PR CALL `Customer`.`Customer.PrCustomerChurn`(); -- compare updated persistent to V1 SELECT COUNT(*) AS 'Persistent Table Rows' FROM Customer.CustomerChurn; SELECT COUNT(*) AS 'Version 1 Table Rows' FROM Customer.CustomerChurn_Version1; -- rows in V1 not in new persistent SELECT * FROM Customer.CustomerChurn_Version1 LEFT JOIN Customer.CustomerChurn ON CustomerChurn_Version1.CustomerId = CustomerChurn.CustomerId WHERE CustomerChurn.CustomerId IS NULL ORDER BY CustomerChurn_Version1.CustomerId; -- rows updated between V1 and persistent SELECT * FROM Customer.CustomerChurn_Version1 INNER JOIN Customer.CustomerChurn ON CustomerChurn_Version1.CustomerId = CustomerChurn.CustomerId WHERE CustomerChurn.Surname <> CustomerChurn_Version1.Surname OR CustomerChurn.CreditScore <> CustomerChurn_Version1.CreditScore OR CustomerChurn.Geography <> CustomerChurn_Version1.Geography OR CustomerChurn.Gender <> CustomerChurn_Version1.Gender OR CustomerChurn.Age <> CustomerChurn_Version1.Age OR CustomerChurn.Balance <> CustomerChurn_Version1.Balance OR CustomerChurn.Exited <> CustomerChurn_Version1.Exited ORDER BY CustomerChurn_Version1.CustomerId; -- last few rows from persistent SELECT * FROM Customer.CustomerChurn ORDER BY CustomerID DESC LIMIT 3; -- rows in persistent that weren't in V1 SELECT * FROM Customer.CustomerChurn LEFT JOIN Customer.CustomerChurn_Version1 ON CustomerChurn.CustomerId = CustomerChurn_Version1.CustomerId WHERE CustomerChurn_Version1.CustomerId IS NULL ORDER BY CustomerChurn.CustomerId;","title":"Database Updates-Validation"},{"location":"Database_Updates/Database_Updates/#database-updates-validation","text":"","title":"Database Updates - Validation"},{"location":"Database_Updates/Database_Updates/#establish-database-tables","text":"-- setup database CREATE DATABASE Customer; SHOW DATABASES; USE Customer; -- setup staging table CREATE TABLE Customer.CustomerChurn_Stage ( CustomerId INT PRIMARY KEY, Surname VARCHAR(50), CreditScore SMALLINT, Geography VARCHAR(50), Gender VARCHAR(15), Age TINYINT, Balance DECIMAL(10,2), Exited TINYINT ); DESCRIBE Customer.CustomerChurn_Stage; -- setup persistent table CREATE TABLE Customer.CustomerChurn ( CustomerId INT PRIMARY KEY, Surname VARCHAR(50), CreditScore SMALLINT, Geography VARCHAR(50), Gender VARCHAR(15), Age TINYINT, Balance DECIMAL(10,2), Exited TINYINT, SourceSystemNm CHAR(20) NOT NULL, CreateAgentId CHAR(20) NOT NULL, CreateDtm DATETIME NOT NULL, ChangeAgentId CHAR(20) NOT NULL, ChangeDtm DATETIME NOT NULL); DESCRIBE Customer.CustomerChurn; -- get row count from stage table SELECT COUNT(*) FROM Customer.CustomerChurn_Stage; -- get last rows from stage SELECT * FROM Customer.CustomerChurn_Stage ORDER BY CustomerID DESC LIMIT 3;","title":"Establish database &amp; tables"},{"location":"Database_Updates/Database_Updates/#run-stored-procedure-compare","text":"-- call procedure CALL `Customer`.`Customer.PrCustomerChurn`(); -- confirm row counts SELECT COUNT(*) AS 'Persistent Table Rows' FROM Customer.CustomerChurn; SELECT COUNT(*) AS 'Staging Table Rows' FROM Customer.CustomerChurn_Stage; -- get last rows from persistent SELECT * FROM Customer.CustomerChurn ORDER BY CustomerID DESC LIMIT 3; -- create new table duplicating persistent table as V1 CREATE TABLE Customer.CustomerChurn_Version1 AS SELECT * FROM Customer.CustomerChurn; DESCRIBE Customer.CustomerChurn_Version1; SELECT COUNT(*) FROM Customer.CustomerChurn_Version1; SELECT * FROM Customer.CustomerChurn_Version1 ORDER BY CustomerID DESC LIMIT 3; -- empty the staging table TRUNCATE TABLE Customer.CustomerChurn_Stage; SELECT * FROM Customer.CustomerChurn_Stage; -- row info from new data in staging SELECT COUNT(*) FROM Customer.CustomerChurn_Stage; SELECT * FROM Customer.CustomerChurn_Stage ORDER BY CustomerID DESC LIMIT 3;","title":"Run stored procedure &amp; compare"},{"location":"Database_Updates/Database_Updates/#rerun-stored-procedure","text":"-- rerun PR CALL `Customer`.`Customer.PrCustomerChurn`(); -- compare updated persistent to V1 SELECT COUNT(*) AS 'Persistent Table Rows' FROM Customer.CustomerChurn; SELECT COUNT(*) AS 'Version 1 Table Rows' FROM Customer.CustomerChurn_Version1; -- rows in V1 not in new persistent SELECT * FROM Customer.CustomerChurn_Version1 LEFT JOIN Customer.CustomerChurn ON CustomerChurn_Version1.CustomerId = CustomerChurn.CustomerId WHERE CustomerChurn.CustomerId IS NULL ORDER BY CustomerChurn_Version1.CustomerId; -- rows updated between V1 and persistent SELECT * FROM Customer.CustomerChurn_Version1 INNER JOIN Customer.CustomerChurn ON CustomerChurn_Version1.CustomerId = CustomerChurn.CustomerId WHERE CustomerChurn.Surname <> CustomerChurn_Version1.Surname OR CustomerChurn.CreditScore <> CustomerChurn_Version1.CreditScore OR CustomerChurn.Geography <> CustomerChurn_Version1.Geography OR CustomerChurn.Gender <> CustomerChurn_Version1.Gender OR CustomerChurn.Age <> CustomerChurn_Version1.Age OR CustomerChurn.Balance <> CustomerChurn_Version1.Balance OR CustomerChurn.Exited <> CustomerChurn_Version1.Exited ORDER BY CustomerChurn_Version1.CustomerId; -- last few rows from persistent SELECT * FROM Customer.CustomerChurn ORDER BY CustomerID DESC LIMIT 3; -- rows in persistent that weren't in V1 SELECT * FROM Customer.CustomerChurn LEFT JOIN Customer.CustomerChurn_Version1 ON CustomerChurn.CustomerId = CustomerChurn_Version1.CustomerId WHERE CustomerChurn_Version1.CustomerId IS NULL ORDER BY CustomerChurn.CustomerId;","title":"Rerun stored procedure"},{"location":"Database_Updates/Database_Updates_PR/","text":"Database Updates - Stored Procedure This is a stored procedure used to make updates to a database of customers. The procedure will complete the following tasks: Verify new information is relevant Delete rows for customers who no longer have accounts Update existing customer rows that have been modified Add in new customer rows Setup CREATE DEFINER=`nilson`@`localhost` PROCEDURE `Customer.PrCustomerChurn`() BEGIN -- **establish variables** DECLARE VarCurrentTimestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP; DECLARE VarSourceRowCount, VarTargetRowCount, VarThresholdNbr INTEGER DEFAULT 0; DECLARE VarTinyIntVal TINYINT; -- **count rows in source & target tables** SELECT COUNT(*) INTO VarSourceRowCount FROM Customer.CustomerChurn_Stage; SELECT COUNT(*) INTO VarTargetRowCount FROM Customer.CustomerChurn; -- **set threshold at 20% of target row count** SELECT CAST((VarTargetRowCount * .2) AS UNSIGNED INTEGER) INTO VarThresholdNbr; -- **end procedure if source row count < threshold** IF VarSourceRowCount < VarThresholdNbr THEN SELECT -129 INTO VarTinyIntVal; END IF; Remove deleted rows -- **disable safe updates** SET SQL_SAFE_UPDATES = 0; -- **remove rows from target that aren't in source** DELETE FROM Customer.CustomerChurn AS TrgtTbl WHERE EXISTS ( SELECT * FROM ( SELECT TT.CustomerId FROM Customer.CustomerChurn AS TT LEFT JOIN Customer.CustomerChurn_Stage AS ST ON TT.CustomerId = ST.CustomerId WHERE ST.CustomerId IS NULL ) AS SrcTbl WHERE TrgtTbl.CustomerId = SrcTbl.CustomerId ); Update customers rows as needed -- **update rows in target that changed in source** UPDATE Customer.CustomerChurn AS TrgtTbl INNER JOIN Customer.CustomerChurn_Stage AS SrcTbl ON TrgtTbl.CustomerId = SrcTbl.CustomerId SET TrgtTbl.Surname = SrcTbl.Surname, TrgtTbl.CreditScore = SrcTbl.CreditScore, TrgtTbl.Geography = SrcTbl.Geography, TrgtTbl.Gender = SrcTbl.Gender, TrgtTbl.Age = SrcTbl.Age, TrgtTbl.Balance = SrcTbl.Balance, TrgtTbl.Exited = SrcTbl.Exited, TrgtTbl.ChangeDtm = VarCurrentTimestamp WHERE ( COALESCE(TrgtTbl.Surname,'*') <> COALESCE(SrcTbl.Surname,'*') OR COALESCE(TrgtTbl.CreditScore,'*') <> COALESCE(SrcTbl.CreditScore,'*') OR COALESCE(TrgtTbl.Geography,'*') <> COALESCE(SrcTbl.Geography,'*') OR COALESCE(TrgtTbl.Gender,'*') <> COALESCE(SrcTbl.Gender,'*') OR COALESCE(TrgtTbl.Age,'*') <> COALESCE(SrcTbl.Age,'*') OR COALESCE(TrgtTbl.Balance,'*') <> COALESCE(SrcTbl.Balance,'*') OR COALESCE(TrgtTbl.Exited,'*') <> COALESCE(SrcTbl.Exited,'*') ); Add new rows -- **insert new rows from source that aren't in target** INSERT INTO Customer.CustomerChurn ( CustomerId, Surname, CreditScore, Geography, Gender, Age, Balance, Exited, SourceSystemNm, CreateAgentId, CreateDtm, ChangeAgentId, ChangeDtm ) SELECT SrcTbl.CustomerId, SrcTbl.Surname, SrcTbl.CreditScore, SrcTbl.Geography, SrcTbl.Gender, SrcTbl.Age, SrcTbl.Balance, SrcTbl.Exited, 'Kaggle-CSV' AS SourceSystemNm, current_user() AS CreateAgentId, VarCurrentTimestamp AS CreateDtm, current_user() AS ChangeAgentId, VarCurrentTimestamp AS ChangeDtm FROM Customer.CustomerChurn_Stage AS SrcTbl INNER JOIN ( SELECT ST.CustomerId FROM Customer.CustomerChurn_Stage AS ST LEFT JOIN Customer.CustomerChurn AS TT ON ST.CustomerId = TT.CustomerId WHERE TT.CustomerId IS NULL ) AS ChgdNew ON SrcTbl.CustomerId = ChgdNew.CustomerId; -- **re-enable safe updates & end** SET SQL_SAFE_UPDATES = 1; END","title":"Database Updates-PR"},{"location":"Database_Updates/Database_Updates_PR/#database-updates-stored-procedure","text":"This is a stored procedure used to make updates to a database of customers. The procedure will complete the following tasks: Verify new information is relevant Delete rows for customers who no longer have accounts Update existing customer rows that have been modified Add in new customer rows","title":"Database Updates - Stored Procedure"},{"location":"Database_Updates/Database_Updates_PR/#setup","text":"CREATE DEFINER=`nilson`@`localhost` PROCEDURE `Customer.PrCustomerChurn`() BEGIN -- **establish variables** DECLARE VarCurrentTimestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP; DECLARE VarSourceRowCount, VarTargetRowCount, VarThresholdNbr INTEGER DEFAULT 0; DECLARE VarTinyIntVal TINYINT; -- **count rows in source & target tables** SELECT COUNT(*) INTO VarSourceRowCount FROM Customer.CustomerChurn_Stage; SELECT COUNT(*) INTO VarTargetRowCount FROM Customer.CustomerChurn; -- **set threshold at 20% of target row count** SELECT CAST((VarTargetRowCount * .2) AS UNSIGNED INTEGER) INTO VarThresholdNbr; -- **end procedure if source row count < threshold** IF VarSourceRowCount < VarThresholdNbr THEN SELECT -129 INTO VarTinyIntVal; END IF;","title":"Setup"},{"location":"Database_Updates/Database_Updates_PR/#remove-deleted-rows","text":"-- **disable safe updates** SET SQL_SAFE_UPDATES = 0; -- **remove rows from target that aren't in source** DELETE FROM Customer.CustomerChurn AS TrgtTbl WHERE EXISTS ( SELECT * FROM ( SELECT TT.CustomerId FROM Customer.CustomerChurn AS TT LEFT JOIN Customer.CustomerChurn_Stage AS ST ON TT.CustomerId = ST.CustomerId WHERE ST.CustomerId IS NULL ) AS SrcTbl WHERE TrgtTbl.CustomerId = SrcTbl.CustomerId );","title":"Remove deleted rows"},{"location":"Database_Updates/Database_Updates_PR/#update-customers-rows-as-needed","text":"-- **update rows in target that changed in source** UPDATE Customer.CustomerChurn AS TrgtTbl INNER JOIN Customer.CustomerChurn_Stage AS SrcTbl ON TrgtTbl.CustomerId = SrcTbl.CustomerId SET TrgtTbl.Surname = SrcTbl.Surname, TrgtTbl.CreditScore = SrcTbl.CreditScore, TrgtTbl.Geography = SrcTbl.Geography, TrgtTbl.Gender = SrcTbl.Gender, TrgtTbl.Age = SrcTbl.Age, TrgtTbl.Balance = SrcTbl.Balance, TrgtTbl.Exited = SrcTbl.Exited, TrgtTbl.ChangeDtm = VarCurrentTimestamp WHERE ( COALESCE(TrgtTbl.Surname,'*') <> COALESCE(SrcTbl.Surname,'*') OR COALESCE(TrgtTbl.CreditScore,'*') <> COALESCE(SrcTbl.CreditScore,'*') OR COALESCE(TrgtTbl.Geography,'*') <> COALESCE(SrcTbl.Geography,'*') OR COALESCE(TrgtTbl.Gender,'*') <> COALESCE(SrcTbl.Gender,'*') OR COALESCE(TrgtTbl.Age,'*') <> COALESCE(SrcTbl.Age,'*') OR COALESCE(TrgtTbl.Balance,'*') <> COALESCE(SrcTbl.Balance,'*') OR COALESCE(TrgtTbl.Exited,'*') <> COALESCE(SrcTbl.Exited,'*') );","title":"Update customers rows as needed"},{"location":"Database_Updates/Database_Updates_PR/#add-new-rows","text":"-- **insert new rows from source that aren't in target** INSERT INTO Customer.CustomerChurn ( CustomerId, Surname, CreditScore, Geography, Gender, Age, Balance, Exited, SourceSystemNm, CreateAgentId, CreateDtm, ChangeAgentId, ChangeDtm ) SELECT SrcTbl.CustomerId, SrcTbl.Surname, SrcTbl.CreditScore, SrcTbl.Geography, SrcTbl.Gender, SrcTbl.Age, SrcTbl.Balance, SrcTbl.Exited, 'Kaggle-CSV' AS SourceSystemNm, current_user() AS CreateAgentId, VarCurrentTimestamp AS CreateDtm, current_user() AS ChangeAgentId, VarCurrentTimestamp AS ChangeDtm FROM Customer.CustomerChurn_Stage AS SrcTbl INNER JOIN ( SELECT ST.CustomerId FROM Customer.CustomerChurn_Stage AS ST LEFT JOIN Customer.CustomerChurn AS TT ON ST.CustomerId = TT.CustomerId WHERE TT.CustomerId IS NULL ) AS ChgdNew ON SrcTbl.CustomerId = ChgdNew.CustomerId; -- **re-enable safe updates & end** SET SQL_SAFE_UPDATES = 1; END","title":"Add new rows"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/","text":"EV Charging Optimization at Portland State University Problem Overview: Portland State University is an urban commuter university. With over 20,000 students enrolled, 61% from the Portland area, and over 5,000 employees, the university attracts thousands of individuals to its campus on a daily basis (@PSUFacts and @PSUHeadcount). While multi-modal transportation is encouraged, driving remains a popular enough option for 15 buildings to offer parking, three of which are dedicated parking structures [@PSUBuildings]. Within those parking options, there are a total of 24 electronic vehicle (EV) charging stations available to PSU commuters [@PSUParking]. In recent years, electric vehicles have seen a surge in popularity, propelled primarily by greater operating affordability and decreased emissions impact [@zhou2021]. Ownership is only expected to rise with global estimates of 25% growth in new electric car sales year-over-year [@IEA]. Within the state of Oregon alone, EV sales accounted for 16% of all new vehicle sales within the first quarter of 2023, and the state is striving to reach 250,000 EV registrations by 2025 [@ODE2023]. As EV ownership increases, it becomes all the more critical to address charging infrastructure, which is often cited as one of the most pressing issues facing EV adoption [@Ahmad2022]. Bearing this in mind, how does Portland State University\u2019s charging infrastructure support demand on campus and how should further development of EV charging stations proceed? Literature Review: In a review of fifty different optimization studies, Ahmad et al. outline three main approaches taken when optimizing placement of EV chargers: that of the distribution network operator (DNO), the EV user, and the charging station owner (CSO) (2022). Each approach is unique in its perspective, and as such, requires varying information for its problem formulation. The DNO approach is generally technical and expansive with considerations towards entire transportation networks and grid power balance [@Ahmad2022]. The EV user approach is focused more specifically on driver needs by minimizing travelling times, waiting times, charging times, and all associated costs [@Ahmad2022]. Finally, the CSO approach is focused on maximizing profits and return on investments from EV charging by minimizing installation, operation, and maintenance costs [@Ahmad2022]. Considering the available data and the goals of PSU\u2019s Transportation and Parking Services, this project takes a CSO type approach, seeking to optimize build-out and investment from the perspective of PSU. From the CSO perspective, costs are generally the driving functions in EV charger placement optimization. One-time costs typically included in EV optimization models were building, construction, installation, and equipment costs [@Ahmad2022]. Recurring costs included electricity costs, equipment service costs, and maintenance [@Ahmad2022]. Within these models, costs are constrained by limits on investments and revenues from use-fees [@Ahmad2022]. Research also suggests station owners may receive revenue by way of vehicle to grid storage in off-peak times or through incentive-based demand response programs, although these incentives are determined at a network level (@moradijoz2018 and @Simorgh2018). There are 3 types of EV chargers, each with varying electricity requirements, charging speeds, and resulting usage patterns. The charger types, in order of ascending charging speed, are as follows: Level 1, Level 2, and Direct Current Fast Charging (DCFC) [@USDoT]. Level 1 chargers can charge a full electric vehicle from 0% to 80% in an average of 40-50 hours, Level 2 chargers in 4-10 hours, and DCFC in just 20 minutes to an hour [@USDoT]. Level 1 chargers are the simplest and most affordable type fo charging as they operate through standard 120V outlets, such as those found in homes, and most EVs come with a Level 1 cord requiring no additional equipment [@USDoE]. Level 2 chargers operate on 240V residential or 208V commercial services and require a dedicated 40-Amp circuit, therefore may require additional infrastructure changes to install [@USDoE]. DC Fast Chargers operate at a much higher 400V to 1000V direct current with outputs ranging from 50 to 350kW, but many electric vehicles may not be able to handle outputs greater than 150 kW (@USDoT and @HOWELL2017). Although the quick charging time associated with DCFC is convenient, research suggests this type of charging is typically reserved for long distance trips, with Level 1 or Level 2 charging preferred for use at home or work [@LEE2020]. In fact, 50-80% of charging has been found to occur at home with another 15-20% occurring at work, and just 5% occurring at public locations such as parking lots [@LEE2020]. As such, it appears that Level 1 or 2 charging is much more important for a location like Portland State University where dwell times are higher, especially for faculty, but ultimately demand for charging may not be all that high with patterns of charging at home in the forefront. Interview Insights: An interview with representatives from PSU Transportation and Parking Services provided anecdotal feedback that current charging infrastructure at PSU seems to exceed current demand. Using upgraded Blink and PlugsIO features, parking managers can track that only a handful of individuals are consistent users, and furthermore, employee survey results indicate that more than half of EV driving faculty do not need to charge on campus. Conversely, the demand for standard parking is very high with parking structures 1 and 2 consistently full throughout the week. PSU is still planning on building out EV charging capabilities, however the focus will be on Level 1 chargers due to lower costs to build and maintain. Objectives: The following objectives were developed for this project after a review of the literature, the interview with PSU Transportation and Parking Services, and the available data: Determine the demand for electric vehicle charging on campus through an analysis of the Blink charging data provided by PSU Transportation and Parking Services. Develop an optimized build-out plan for the relocation of current demand to parking structure 3 over the course of one term. As demand for standard parking in structures 1 and 2 is high, the team will focus on an optimized plan to meet existing demand in the underutilized parking structure 3. Methodology At a high level, the project methodology included a literature review, interview, data analysis, and finally model implementation using demand and key trends from the data. Relevant EV optimization research was reviewed during the literature review process for key project considerations. Data and PSU specific insights were extracted from the interview process and used to inform model direction. Data Analysis Data Background: The data used in this project was collected via Blink during the months of 08/31/2023 through 10/31/2023 across for 8 different EV chargers. These chargers are located in a single parking structure and are used as a representative sample of greater parking and EV parking trends on campus. Summary: The sample data provided to us by Transportation and Parking Service provided 237 instances across a span of approximately 9 weeks (62 days). In this sample, Saturday was the weekday with the most occurrences across the data (50) and Sunday was the lowest occurrences (19). The average plug duration was 4 hours 5 minutes and total energy duration for those 8 chargers was 1138 hours (a total of 2637 kW\u2019s). The median occurrences on any given day was 4, the maximum occurrence on any given day was Saturday (10) and lowest occurrences across Sunday, Monday, Thursday, Tuesday and Friday (1). Demand: From the data sample, a minimum conservative projection could be estimated. Knowing that the sample data provided 8 chargers out of 24, we could take the sample average (4) and multiply it by remaining chargers not accounted for. Despite not understanding the demand of the other chargers, the average projection multiplied by 3 (= 84 weekly connections) estimates lower than a theoretical maximum given the observed sample (141 weekly). Therefore at a minimum, the estimated value might live within it\u2019s normal distribution. Lots Available: Given an estimated weekly demand, we could look at the percentages of lots used given the demand and the actual data observed. This percentage of demand calculated were used to estimate lots available. Data Processing: # Read in Blink data Blink <- Read(\"data/EV-blink.csv\", quiet = TRUE) # Convert relevant columns to duration measurements Blink$Total.Time <- as.duration(hms(Blink$Total.Time)) Blink$Occupancy.Billable.Time <- as.duration(hms(Blink$Occupancy.Billable.Time)) Blink$Total.Occupancy.Time <- as.duration(hms(Blink$Total.Occupancy.Time)) Blink$Billable.Time <- as.duration(hms(Blink$Billable.Time)) Blink$Plug.Duration <- as.duration(hms(Blink$Plug.Duration)) Blink$Energy.Duration <- as.duration(hms(Blink$Energy.Duration)) # Removing Outliers quartiles <- quantile(Blink$Total.Time.hrs, probs = c(0.25, 0.75), na.rm = FALSE) IQR <- IQR(c(Blink$Total.Time.hrs, Blink$Energy.Duration.hrs)) Lower <- quartiles[1] - 1.5 * IQR Upper <- quartiles[2] + 1.5 * IQR Blink2 <- subset(Blink, Blink$Total.Time.hrs > Lower & Blink$Total.Time.hrs < Upper) Blink2 <- subset(Blink, Blink$Energy.Duration.hrs > Lower & Blink$Energy.Duration.hrs < Upper) Summary of Blink Data by Parking Location Location.Name Count of Charging Instances Total Time (hrs) Total Energy Duration (hrs) Total Energy Use (kW) Portland State University 148 586.53 351.64 1549.79 Portland State University - Parking One 25 140.06 56.08 120.32 Portland State University - RMNC 64 235.10 154.60 765.17 # Maximum and Minimum values of numerical columns max_total_time <- max(Blink2$Total.Time.hrs, na.rm = TRUE) min_total_time <- min(Blink2$Total.Time.hrs, na.rm = TRUE) max_plug_duration <- max(Blink2$Plug.Duration, na.rm = TRUE) min_plug_duration <- min(Blink2$Plug.Duration, na.rm = TRUE) max_energy_duration <- max(Blink2$Energy.Duration.hrs, na.rm = TRUE) min_energy_duration <- min(Blink2$Energy.Duration.hrs, na.rm = TRUE) # Print the maximum and minimum values cat(\"Maximum Total.Time.hrs:\", max_total_time, \"\\n\") ## Maximum Total.Time.hrs: 22.36639 cat(\"Minimum Total.Time.hrs:\", min_total_time, \"\\n\") ## Minimum Total.Time.hrs: 0.007777778 cat(\"Maximum Plug.Duration (seconds...divide by 3600 for hrs):\", max_plug_duration, \"\\n\") ## Maximum Plug.Duration (seconds...divide by 3600 for hrs): 80519 cat(\"Minimum Plug.Duration (seconds...-divide by 3600 for hrs):\", min_plug_duration, \"\\n\") ## Minimum Plug.Duration (seconds...-divide by 3600 for hrs): 28 cat(\"Maximum Energy.Duration.hrs:\", max_energy_duration, \"\\n\") ## Maximum Energy.Duration.hrs: 9.451944 cat(\"Minimum Energy.Duration.hrs:\", min_energy_duration, \"\\n\") ## Minimum Energy.Duration.hrs: 0 # column to a date format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # day of the week Blink2$Day_of_Week <- weekdays(Blink2$Connection) # count occurrences of each day of the week day_counts <- table(Blink2$Day_of_Week) # display print(day_counts) ## < table of extent 0 > # converting to date format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # extract the day of the week Blink2$Day_of_Week <- weekdays(Blink2$Connection) # occurrences of each day of the week day_counts <- table(Blink2$Day_of_Week) # converting day_counts to a df day_counts_df <- as.data.frame(day_counts) names(day_counts_df) <- c(\"Day_of_Week\", \"Count\") # bar plot ggplot(day_counts_df, aes(x = Day_of_Week, y = Count, fill = Day_of_Week)) + geom_bar(stat = \"identity\") + labs(title = \"Occurrences on each day of the week\", x = \"DOW\", y = \"Count\") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_fill_discrete(name = \"Day of the Week\") # correct format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # group data by date and count occurrences per date date_occurrences <- Blink2 %>% group_by(Connection) %>% summarise(occurrences = n()) # display print(date_occurrences) ## # A tibble: 1 \u00d7 2 ## Connection occurrences ## <date> <int> ## 1 NA 237 # maximum, median, and minimum occurrences occurrences_stats <- date_occurrences %>% summarise(max_occurrences = max(occurrences), median_occurrences = median(occurrences), min_occurrences = min(occurrences)) # statistics print(occurrences_stats) ## # A tibble: 1 \u00d7 3 ## max_occurrences median_occurrences min_occurrences ## <int> <int> <int> ## 1 237 237 237 # correct format Blink2$CTime <- as.POSIXct(Blink2$CTime, format = \"%I:%M:%S %p\") # plot ggplot(Blink2, aes(x = CTime, fill = Day_of_Week)) + geom_histogram(binwidth = 3600, #(1 hour = 3600 seconds) color = \"black\", alpha = 0.7) + facet_wrap(~Day_of_Week, ncol = 3) + labs(title = \"Occurrences by DOW and Time\", x = \"Time\", y = \"Count\") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # correct format: character format, convert it to POSIXct Blink2$CTime <- as.POSIXct(Blink2$CTime, format = \"%I:%M:%S %p\") # group data by connection column and calculate mmm time_summary <- Blink2 %>% group_by(Day_of_Week) %>% summarise(greatest_time = max(CTime), median_time = median(CTime, na.rm = TRUE), least_time = min(CTime)) print(time_summary) Data Analysis Summary: From this analysis we conclude that the optimal number of chargers is 12 because it would sustain current demand and accomdate fluctuational demand. Based on this analysis, the estimated general trend of demand over 10 weeks is around 840 connections. Using Blink charging data, our main objective was to identify demand for electric vehicle charging. The blink charge data was collected from PSU Transportation and parking Services. We analyzed data through August, September and October. Three different PSU charging sites are listed in our data report. Portland State University, Portland State University-Parking One, and Portland State University RMNC are the three locations. We were able to determine the duration of the car\u2019s connection to the charging port based on the provided data, which also included the day of the week, date, and time of the connection as well as information about the disconnection. As a result, we could observe the entire amount of time spent charging each car. The Data Report also included additional invoicing information and the maximum power used by each car. Every record in the data had a serial number that corresponded to the car\u2019s connection to the charging port. Based on the data given the car takes an average of four hours and five minutes to charge. In our data, \u201coccurrences\u201d refers to the total number of times the cars have used the charger connection. The resulting list of occurrences showed that there were 20 on Monday, 42 on Tuesday, 37 on Wednesday, 33 on Thursday, 36 on Friday, 50 on Saturday, and 19 on Sunday.The total number of occurrences was 237, and when compared to the other days of the week, we observed that Saturday has the highest number of occurrences. Installation Optimization Model To ensure we meet business needs as identified by our interview with PSU\u2019s Transportation and Parking department, our initial optimization model focuses on centralizing PSU\u2019s current charging demand in Parking Structure 3. The model is heavily simplified to meet our capacity within this project, and so does not take into account every factor that would apply to the installation and costing process. Instead, we are focusing on the following elements: Purchase costs and charger storage Installation labor costs Timeline requirements Available space for construction Additionally, we have severe limitations on our data availability. The Blink charging dataset is a representative subset of PSU\u2019s full charging needs and parking demand. However, we did not have access to additional details on PSU\u2019s parking demand and charging needs. As such, we used the Blink data to extrapolate assumptions about the charging and parking demand on campus, but additional details would be needed to complete the picture. Along with that limitation, we did not have information on authentic labor costs for installation when using PSU\u2019s construciton services, and so generated estimates based on our understanding of construction expenses and from conversations with individuals who have had other work completed by PSU in the past. Model Formulation: Our model\u2019s objective is to minimize costs, which are the sum of installation and purchase costs. Our first constraint is that we must install our required number of chargers (the demand) within our required time limit. Outside of the constraints that set limitations on variables (eg. binary vs. integer), the remaining constraints can be grouped as: Purchasing : The total purchase price each week is the number of chargers purchased multiplied by the cost of each charger. Additionally, we are adding in a bulk-purchase discount, where purchasing 5 chargers provides a discount of 20% (i.e. the price of a charger). The total purchase cost per week subtracts the applicable discount from the cost of the purchased chargers for the week. The cost of the chargers, $300, was provided from our interview with Transportation and Parking, however the discount was an added feature included due to research into various charging companies. Storage : Inventory management is an important element of any installation process, and so we wanted to be sure to include it in this project. However, this was an area where we had limitations on data. As such, we set an estimated limit of the number of chargers available to store at one time as a means of controlling how many could be purchased and installed each week. We also estimated that there were already a certain number of chargers in storage, available for install on week 1. We then established constraints around charger inventory at the end of each week, based on the number of chargers at the start of the week, how many were installed, and how many were purchased. Installation : Our installation costs were another area of limited data. However, it is important to take into account the cost of labor in a model such as this, as skilled installation is often quite a bit more than the cost of the product itself. Using our familiarity with general construction practices, we established a base installation fee which is charged no matter the number of chargers installed (so long as it\u2019s more than 0), to account for the work that must be done whether 1 or 20 chargers are installed in a given week. Added to that is a per-charger installation cost to account for the increase in work. We also set a limit on the number of chargers that could be installed each week, as well as limitations on how much space could be used for construction, identified as a count of parking spaces (assuming a certain number of surrounding spaced would be used for each install) with the limitation based on an estimated percent of available space in the parking garage for a given week during the term. Variables Reference: Charger Tracking: D = Total chargers to install B S = Starting inventory of chargers (week 0) x t = Chargers installed on week t y t = Chargers purchased in week t z t = Stored chargers at the end of week t Costs: B I = Base cost for install C I = Per-charger install cost C Ti t = Total install cost for week t C P = Per-charger purchase cost C Tp t = Total purchase costs for week t Limits: L I = Weekly install limit L S = Charger storage limit L A t = Limit available area for construction for week t Weights: W I t = Install binary for week t W P t = Discount binary for week t W A = Weight of spaces to be used Model Implementation: For our implementation, we are setting external variables based on calculated (from the Blink), provided (from the PSU interview), or estimated values as previously discussed. The initial model uses Parking Structure 3 as the location for installation, chosen due to our conversation with Transportation and Parking. Our limits on how much of the parking structure is available for construction, as well as our demand, are extrapolations from the Blink charging data while the per-charger purchase cost was provided by Transportation and Parking. All other inputs are estimates. lot <- 1300 #total spaces in parking structure 3 D <- 12 #total chargers to install BS <- 2 #starting inventory of chargers LI <- 3 #max chargers to install each week LS <- 5 #max inventory capacity for chargers CI <- 2000 #per charger install labor cost CP <- 300 #per charger purchase cost WA <- 5 #average area used during install (parking spaces) BI <- 5000 #base cost of install #percent of lot available for construction, capped at 50% LA <- c(lot*.5, #pre-term lot*0, lot*.0, lot*.19, lot*.29, lot*.40, #weeks 1-5 of term lot*.37, lot*.35, lot*.39, lot*.31, lot*.32, #weeks 6-10 of term lot*.0) #finals week weeks <- length(LA) #completion time (weeks) model <- MIPModel() |> add_variable(Vx[t], t=1:weeks, type=\"integer\", lb = 0, ub = LI) |> #var: install/week add_variable(Vy[t], t=1:weeks, type=\"integer\", lb = 0) |> #var: purchases/week add_variable(Vz[t], t=1:weeks, type=\"integer\", lb = 0, ub = LS) |> #var: chargers stored/week add_variable(VCTi[t], t=1:weeks, type=\"continuous\", lb = 0) |> #var: total install/week add_variable(VCTp[t], t=1:weeks, type=\"continuous\", lb = 0) |> #var: total purchase/week add_variable(VWp[t], t=1:weeks, type=\"binary\") |> #var: bulk discount activator add_variable(VWi[t], t=1:weeks, type=\"binary\") |> #var: base install binary set_objective(sum_expr(VCTi[t]+VCTp[t], t=1:weeks), \"min\") |> #obj: min costs add_constraint(sum_expr(Vx[t], t=1:weeks)==D) |> #con: install total demand add_constraint(Vz[t-1]+Vy[t]-Vx[t] == Vz[t], t = 2:weeks) |> #con: weeks 2+ stored chargers add_constraint(BS+Vy[t]-Vx[t] == Vz[t], t = 1) |> #con: week 1 stored chargers add_constraint(Vx[t] <= Vz[t-1], t = 2:weeks) |> #con: weeks 2+ install limit add_constraint(Vx[t] <= BS, t = 1) |> #con: week 1 install limit add_constraint(Vx[t]*WA <= LA[t], t = 1:weeks) |> #con: construction area limit add_constraint(Vx[t]-D*VWi[t] <= 0, t = 1:weeks) |> #con: base charge if installing add_constraint(Vx[t]*CI+VWi[t]*BI == VCTi[t], t = 1:weeks) |> #con: installation cost/week add_constraint(Vy[t]-5*VWp[t]>=0, t = 1:weeks) |> #con: bulk discount add_constraint(Vy[t]*CP - CP*VWp[t] == VCTp[t], t = 1:weeks) #con: purchase cost/week model_res <- solve_model(model, with_ROI(solver = \"glpk\")) Model Solution: The solution for this model has four installation weeks, with each week using the full limit of possible installations (3). This minimizes the number of base installation charges incurred, which are one of the primary cost drivers for the installation costs. For purchasing chargers, the model only purchases enough to meet the discount threshold once, indicating that the purchase costs are not an overall cost driver to the installation plan. The total expense for installation using this model would be $46,700, including both installation and purchase costs, with a completion date of the last week of term. One thing to note with this plan is that the total space avaiable for construction, excluding the weeks when we set availability to 0, did not appear to be a limiting factor. By capping installations at 3 per week, we are at maximum using 15 spaces for a given week\u2019s installation. Outside of the weeks when availability was set to 0, the week with the least available space was week 3 of term, with 19% of the lot available for construction. In a lot with 1,300 spaces, that\u2019s 247 spaces available for construction, well above the threshold established by our installation limits. One Term Charger Install Plan, Parking Structure 3 No Classes Week 1 Week 2 Week 3 Week 4 Mid Terms Week 6 Week 7 Week 8 Week 9 Week 10 Finals TOTAL Install Count 0 0 0 3 3 3 0 0 0 0 3 0 12 Purchased Chargers 1 0 0 3 5 1 0 0 0 0 0 0 10 Stored Chargers 3 3 3 3 5 3 3 3 3 3 0 0 - Labor Costs 0 0 0 11000 11000 11000 0 0 0 0 11000 0 44000 Purchase Costs 300 0 0 900 1200 300 0 0 0 0 0 0 2700 TOTAL Costs 300 0 0 11900 12200 11300 0 0 0 0 11000 0 46700 Alternative Solution: We wanted to posit an alternative solution to moving all charging demand to Parking Structure 3, though. That parking structure has severe limitations related to location and open hours which could present barriers for commuters to use that as their primary parking location. There is, however, another parking structure closer to the center of campus, the underground Fourth Avenue Building (FAB) parking lot. This lot is smaller in capacity, with a total of 430 spaces, but also contains both hourly and monthly parking options and (based on our personal observances) is rarely filled to capacity. As such, this could be an excellent alternative location for centralized EV charging. The formulation for the model remains as listed above, as this is a generalized model. As such, for the sake of space, we will not repeat the model formulation here. Alternative Location Model: With the implementation, there are also no adjustments to the actual model build itself. Rather, all adjustments come from the external variables. As such, again for the sake of space, we will omit displaying the implementation for the alternative model, and instead focus on the changes to the external variables. When working with the FAB parking structure, there are two things we need to take into account. The first is that is it a much smaller structure, with 430 spaces compared to Parking Structure 3\u2019s 1300 spaces. The other is that this parking structure has much higher employee commuter usage and reduced public usage, due to it\u2019s location and lack of publicity. To support both of these concerns, we drastically reduced the amount of available space for construction during the term to be 2% of the total lot size, and adjusted the off-term availability to be 5% of the total lot size. This is because, while there are student commuters that use this lot, we wanted to ensure enough standard parking was available for employees who continue to commute to campus even when school is not in session. lot <- 430 #total spaces in parking structure 3 D <- 12 #total chargers to install BS <- 2 #starting inventory of chargers LI <- 3 #max chargers to install each week LS <- 5 #max inventory capacity for chargers CI <- 2000 #per charger install labor cost CP <- 300 #per charger purchase cost WA <- 5 #average area used during install (parking spaces) BI <- 5000 #base cost of install #percent of lot available for construction, capped at 50% LA <- c(lot*.05, #pre-term lot*.02, lot*.02, lot*.02, lot*.02, lot*.02, #weeks 1-5 of term lot*.02, lot*.02, lot*.02, lot*.02, lot*.02, #weeks 6-10 of term lot*.02) #finals week weeks <- length(LA) #completion time (weeks) Alternative Location Solution: By forcing the single term install in the FAB parking structure, as a reflection of our original modeling for Parking Structure 3, our total costs increased by approximately $25,000. This is due to the cap on available space throughout the term, which forced the model to have more frequent installs of just a single charger, thereby increasing the total count of base install fees. To minimize costs as best as possible the model moved to two bulk purchases of charges (rather than more frequent smaller purchases). However, that is a minimal savings ($600) when compared to the total cost of the installation project. One Term Charger Install Plan, FAB Parking No Classes Week 1 Week 2 Week 3 Week 4 Mid Terms Week 6 Week 7 Week 8 Week 9 Week 10 Finals TOTAL Install Count 2 1 1 1 1 1 1 1 0 1 1 1 12 Purchased Chargers 5 0 0 0 0 5 0 0 0 0 0 0 10 Stored Chargers 5 4 3 2 1 5 4 3 3 2 1 0 - Labor Costs 9000 7000 7000 7000 7000 7000 7000 7000 0 7000 7000 7000 79000 Purchase Costs 1200 0 0 0 0 1200 0 0 0 0 0 0 2400 TOTAL Costs 10200 7000 7000 7000 7000 8200 7000 7000 0 7000 7000 7000 81400 Discussion At first blush, the alternative location appears to be a worse choice due to the added limitations on construction space. After all, based on our two implementation options, we can determine that a fast, single-term, roll-out of charger installations in an under-utilized parking structure is feasible given our assumptions of charging demand. And when given the option between two installations that vary by about $25,00, it seems like the optimal choice is obvious: centralize EV charging locations in Parking Structure 3. However, the initial numbers are only part of the story. With that understanding, we reached out to various EV drivers who commute to campus on a regular basis to understand their barriers to charging on campus, and their impression of the current infrastructure. From those conversations, we have garnered the impression that the biggest barrier to EV charging on campus for commuters is lack of availability for monthly permit holders. According to one individual who holds a monthly permit, all EV spaces are in hourly parking and when parking with their permit in those spaces they were ticketed for doing so, since they hadn\u2019t paid for the hourly parking. It does appear that the bulk, if not all, of PSU\u2019s EV charging spaces are relegated to hourly parking locations. Regular campus commuters are more likely to use monthly or term-based parking permits, rather than using hourly parking. If they are already paying for a space, there is no incentive to then pay a second time for charging at a Level 1 charger. As indicated in our literary reviews, EV ownership is on the rise, particularly in the Portland Metro Area. We as a university need to determine a charging solution that fits the authentic needs of campus commuters, rather than focusing on data collected from hourly parking locations as that is less likely to demonstrate the needs of our daily commuters. Our recommendation would be to spend additional time learning about the driving habits of the true campus commuters, ones who hold monthly or term-based parking permits, to determine and work to meet their EV charging needs. From that, we would recommend creating a charger location and installation plan that supports those commuters, ensuring that monthly or term-based permit holders have equitable access to EV charging. While considering these options, we would also encourage the university to conduct a deeper analysis into options related to some of our smaller garages, rather than the main parking structures, as those lots would allow the university to continue to focus on general parking in the main lots and potentially re-purpose some of the smaller lots to support the growing base of EV drivers. References \u201c2023 Biennial Zero-Emission Vehicle Report.\u201d 2023, September. https://www.oregon.gov/energy/Dataand-Reports/Documents/2023-Biennial-Zero-Emission-Vehicle-Report.pdf . Ahmad, Fareed, Atif Iqbal, Imtiaz Ashraf, Mousa Marzband, and Irfan khan. 2022. \u201cOptimal Location of Electric Vehicle Charging Station and Its Impact on Distribution Network: A Review.\u201d Energy Reports 8: 2314\u201333. https://doi.org/https://doi.org/10.1016/j.egyr.2022.01.180 . \u201cCAMPUS BUILDINGS.\u201d n.d. Portland State University. https://www.pdx.edu/buildings/?facility_features%5B701%5D=701&page=0 . \u201cCharger Types and Speeds.\u201d 2023. US Department of Transportation. https://www.transportation.gov/rural/ev/toolkit/ev-basics/charging-speeds . David Howell, Brian Cunningham, Steven Boyd. 2017. \u201cEnabling Fast Charging: A Technology Gap Assessment.\u201d Office of Energy Efficiency & Renewable Energy, September. https://www.energy.gov/sites/prod/files/2017/10/f38/XFC%20Technology%20Gap%20Assessment%20Report_FINAL_10202017.pdf . \u201cDeveloping Infrastructure to Charge Electric Vehicles.\u201d n.d. US Department of Energy. https://afdc.energy.gov/fuels/electricity_infrastructure.html . \u201cElectric Vehicles.\u201d n.d. International Energy Agency. https://www.iea.org/energy-system/transport/electric-vehicles . \u201cFACTS : PSU BY THE NUMBERS.\u201d n.d. Portland State University. https://www.pdx.edu/portlandstate-university-facts.12 \u201cHOURLY & VISITOR PARKING.\u201d n.d. Portland State University. https://www.pdx.edu/transportation/hourly-visitor-parking . Lee, Jae Hyun, Debapriya Chakraborty, Scott J. Hardman, and Gil Tal. 2020. \u201cExploring Electric Vehicle Charging Patterns: Mixed Usage of Charging Infrastructure.\u201d Transportation Research Part D: Transport and Environment 79: 102249. https://doi.org/https://doi.org/10.1016/j.trd.2020.102249 . Moradijoz, M., F. Moazzen, S. Allahmoradi, M. Parsa Moghaddam, and M. R Haghifam. 2018. \u201cA Two Stage Model for Optimum Allocation of Electric Vehicle Parking Lots in Smart Grids.\u201d In 2018 Smart Grid Conference (SGC), 1\u20135. https://doi.org/10.1109/SGC.2018.8777877 . \u201cPSU Employee Headcount.\u201d 2023. Portland State University. https://public.tableau.com/app/profile/portland.state/viz/PSUEmployeeHeadcount/About . Simorgh, Hamid, Hasan Doagou-Mojarrad, Hadi Razmi, and Gevork B. Gharehpetian. 2018. \u201cCostBased Optimal Siting and Sizing of Electric Vehicle Charging Stations Considering Demand Response Programmes.\u201d IET Generation, Transmission & Distribution 12 (8): 1712\u201320. https://doi.org/https://doi.org/10.1049/iet-gtd.2017.1663 . Zhou, Min, Piao Long, Nan Kong, Lindu Zhao, Fu Jia, and Kathryn S Campy. 2021. \u201cCharacterizing theMotivational Mechanism Behind Taxi Driver\u2019s Adoption of Electric Vehicles for Living: Insights from China.\u201d Transportation Research Part A: Policy and Practice 144: 134\u201352.","title":"EV Charging Optimization"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#ev-charging-optimization-at-portland-state-university","text":"","title":"EV Charging Optimization at Portland State University"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#problem-overview","text":"Portland State University is an urban commuter university. With over 20,000 students enrolled, 61% from the Portland area, and over 5,000 employees, the university attracts thousands of individuals to its campus on a daily basis (@PSUFacts and @PSUHeadcount). While multi-modal transportation is encouraged, driving remains a popular enough option for 15 buildings to offer parking, three of which are dedicated parking structures [@PSUBuildings]. Within those parking options, there are a total of 24 electronic vehicle (EV) charging stations available to PSU commuters [@PSUParking]. In recent years, electric vehicles have seen a surge in popularity, propelled primarily by greater operating affordability and decreased emissions impact [@zhou2021]. Ownership is only expected to rise with global estimates of 25% growth in new electric car sales year-over-year [@IEA]. Within the state of Oregon alone, EV sales accounted for 16% of all new vehicle sales within the first quarter of 2023, and the state is striving to reach 250,000 EV registrations by 2025 [@ODE2023]. As EV ownership increases, it becomes all the more critical to address charging infrastructure, which is often cited as one of the most pressing issues facing EV adoption [@Ahmad2022]. Bearing this in mind, how does Portland State University\u2019s charging infrastructure support demand on campus and how should further development of EV charging stations proceed?","title":"Problem Overview:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#literature-review","text":"In a review of fifty different optimization studies, Ahmad et al. outline three main approaches taken when optimizing placement of EV chargers: that of the distribution network operator (DNO), the EV user, and the charging station owner (CSO) (2022). Each approach is unique in its perspective, and as such, requires varying information for its problem formulation. The DNO approach is generally technical and expansive with considerations towards entire transportation networks and grid power balance [@Ahmad2022]. The EV user approach is focused more specifically on driver needs by minimizing travelling times, waiting times, charging times, and all associated costs [@Ahmad2022]. Finally, the CSO approach is focused on maximizing profits and return on investments from EV charging by minimizing installation, operation, and maintenance costs [@Ahmad2022]. Considering the available data and the goals of PSU\u2019s Transportation and Parking Services, this project takes a CSO type approach, seeking to optimize build-out and investment from the perspective of PSU. From the CSO perspective, costs are generally the driving functions in EV charger placement optimization. One-time costs typically included in EV optimization models were building, construction, installation, and equipment costs [@Ahmad2022]. Recurring costs included electricity costs, equipment service costs, and maintenance [@Ahmad2022]. Within these models, costs are constrained by limits on investments and revenues from use-fees [@Ahmad2022]. Research also suggests station owners may receive revenue by way of vehicle to grid storage in off-peak times or through incentive-based demand response programs, although these incentives are determined at a network level (@moradijoz2018 and @Simorgh2018). There are 3 types of EV chargers, each with varying electricity requirements, charging speeds, and resulting usage patterns. The charger types, in order of ascending charging speed, are as follows: Level 1, Level 2, and Direct Current Fast Charging (DCFC) [@USDoT]. Level 1 chargers can charge a full electric vehicle from 0% to 80% in an average of 40-50 hours, Level 2 chargers in 4-10 hours, and DCFC in just 20 minutes to an hour [@USDoT]. Level 1 chargers are the simplest and most affordable type fo charging as they operate through standard 120V outlets, such as those found in homes, and most EVs come with a Level 1 cord requiring no additional equipment [@USDoE]. Level 2 chargers operate on 240V residential or 208V commercial services and require a dedicated 40-Amp circuit, therefore may require additional infrastructure changes to install [@USDoE]. DC Fast Chargers operate at a much higher 400V to 1000V direct current with outputs ranging from 50 to 350kW, but many electric vehicles may not be able to handle outputs greater than 150 kW (@USDoT and @HOWELL2017). Although the quick charging time associated with DCFC is convenient, research suggests this type of charging is typically reserved for long distance trips, with Level 1 or Level 2 charging preferred for use at home or work [@LEE2020]. In fact, 50-80% of charging has been found to occur at home with another 15-20% occurring at work, and just 5% occurring at public locations such as parking lots [@LEE2020]. As such, it appears that Level 1 or 2 charging is much more important for a location like Portland State University where dwell times are higher, especially for faculty, but ultimately demand for charging may not be all that high with patterns of charging at home in the forefront.","title":"Literature Review:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#interview-insights","text":"An interview with representatives from PSU Transportation and Parking Services provided anecdotal feedback that current charging infrastructure at PSU seems to exceed current demand. Using upgraded Blink and PlugsIO features, parking managers can track that only a handful of individuals are consistent users, and furthermore, employee survey results indicate that more than half of EV driving faculty do not need to charge on campus. Conversely, the demand for standard parking is very high with parking structures 1 and 2 consistently full throughout the week. PSU is still planning on building out EV charging capabilities, however the focus will be on Level 1 chargers due to lower costs to build and maintain.","title":"Interview Insights:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#objectives","text":"The following objectives were developed for this project after a review of the literature, the interview with PSU Transportation and Parking Services, and the available data: Determine the demand for electric vehicle charging on campus through an analysis of the Blink charging data provided by PSU Transportation and Parking Services. Develop an optimized build-out plan for the relocation of current demand to parking structure 3 over the course of one term. As demand for standard parking in structures 1 and 2 is high, the team will focus on an optimized plan to meet existing demand in the underutilized parking structure 3.","title":"Objectives:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#methodology","text":"At a high level, the project methodology included a literature review, interview, data analysis, and finally model implementation using demand and key trends from the data. Relevant EV optimization research was reviewed during the literature review process for key project considerations. Data and PSU specific insights were extracted from the interview process and used to inform model direction.","title":"Methodology"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#data-analysis","text":"","title":"Data Analysis"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#data-background","text":"The data used in this project was collected via Blink during the months of 08/31/2023 through 10/31/2023 across for 8 different EV chargers. These chargers are located in a single parking structure and are used as a representative sample of greater parking and EV parking trends on campus.","title":"Data Background:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#summary","text":"The sample data provided to us by Transportation and Parking Service provided 237 instances across a span of approximately 9 weeks (62 days). In this sample, Saturday was the weekday with the most occurrences across the data (50) and Sunday was the lowest occurrences (19). The average plug duration was 4 hours 5 minutes and total energy duration for those 8 chargers was 1138 hours (a total of 2637 kW\u2019s). The median occurrences on any given day was 4, the maximum occurrence on any given day was Saturday (10) and lowest occurrences across Sunday, Monday, Thursday, Tuesday and Friday (1).","title":"Summary:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#demand","text":"From the data sample, a minimum conservative projection could be estimated. Knowing that the sample data provided 8 chargers out of 24, we could take the sample average (4) and multiply it by remaining chargers not accounted for. Despite not understanding the demand of the other chargers, the average projection multiplied by 3 (= 84 weekly connections) estimates lower than a theoretical maximum given the observed sample (141 weekly). Therefore at a minimum, the estimated value might live within it\u2019s normal distribution.","title":"Demand:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#lots-available","text":"Given an estimated weekly demand, we could look at the percentages of lots used given the demand and the actual data observed. This percentage of demand calculated were used to estimate lots available.","title":"Lots Available:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#data-processing","text":"# Read in Blink data Blink <- Read(\"data/EV-blink.csv\", quiet = TRUE) # Convert relevant columns to duration measurements Blink$Total.Time <- as.duration(hms(Blink$Total.Time)) Blink$Occupancy.Billable.Time <- as.duration(hms(Blink$Occupancy.Billable.Time)) Blink$Total.Occupancy.Time <- as.duration(hms(Blink$Total.Occupancy.Time)) Blink$Billable.Time <- as.duration(hms(Blink$Billable.Time)) Blink$Plug.Duration <- as.duration(hms(Blink$Plug.Duration)) Blink$Energy.Duration <- as.duration(hms(Blink$Energy.Duration)) # Removing Outliers quartiles <- quantile(Blink$Total.Time.hrs, probs = c(0.25, 0.75), na.rm = FALSE) IQR <- IQR(c(Blink$Total.Time.hrs, Blink$Energy.Duration.hrs)) Lower <- quartiles[1] - 1.5 * IQR Upper <- quartiles[2] + 1.5 * IQR Blink2 <- subset(Blink, Blink$Total.Time.hrs > Lower & Blink$Total.Time.hrs < Upper) Blink2 <- subset(Blink, Blink$Energy.Duration.hrs > Lower & Blink$Energy.Duration.hrs < Upper) Summary of Blink Data by Parking Location Location.Name Count of Charging Instances Total Time (hrs) Total Energy Duration (hrs) Total Energy Use (kW) Portland State University 148 586.53 351.64 1549.79 Portland State University - Parking One 25 140.06 56.08 120.32 Portland State University - RMNC 64 235.10 154.60 765.17 # Maximum and Minimum values of numerical columns max_total_time <- max(Blink2$Total.Time.hrs, na.rm = TRUE) min_total_time <- min(Blink2$Total.Time.hrs, na.rm = TRUE) max_plug_duration <- max(Blink2$Plug.Duration, na.rm = TRUE) min_plug_duration <- min(Blink2$Plug.Duration, na.rm = TRUE) max_energy_duration <- max(Blink2$Energy.Duration.hrs, na.rm = TRUE) min_energy_duration <- min(Blink2$Energy.Duration.hrs, na.rm = TRUE) # Print the maximum and minimum values cat(\"Maximum Total.Time.hrs:\", max_total_time, \"\\n\") ## Maximum Total.Time.hrs: 22.36639 cat(\"Minimum Total.Time.hrs:\", min_total_time, \"\\n\") ## Minimum Total.Time.hrs: 0.007777778 cat(\"Maximum Plug.Duration (seconds...divide by 3600 for hrs):\", max_plug_duration, \"\\n\") ## Maximum Plug.Duration (seconds...divide by 3600 for hrs): 80519 cat(\"Minimum Plug.Duration (seconds...-divide by 3600 for hrs):\", min_plug_duration, \"\\n\") ## Minimum Plug.Duration (seconds...-divide by 3600 for hrs): 28 cat(\"Maximum Energy.Duration.hrs:\", max_energy_duration, \"\\n\") ## Maximum Energy.Duration.hrs: 9.451944 cat(\"Minimum Energy.Duration.hrs:\", min_energy_duration, \"\\n\") ## Minimum Energy.Duration.hrs: 0 # column to a date format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # day of the week Blink2$Day_of_Week <- weekdays(Blink2$Connection) # count occurrences of each day of the week day_counts <- table(Blink2$Day_of_Week) # display print(day_counts) ## < table of extent 0 > # converting to date format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # extract the day of the week Blink2$Day_of_Week <- weekdays(Blink2$Connection) # occurrences of each day of the week day_counts <- table(Blink2$Day_of_Week) # converting day_counts to a df day_counts_df <- as.data.frame(day_counts) names(day_counts_df) <- c(\"Day_of_Week\", \"Count\") # bar plot ggplot(day_counts_df, aes(x = Day_of_Week, y = Count, fill = Day_of_Week)) + geom_bar(stat = \"identity\") + labs(title = \"Occurrences on each day of the week\", x = \"DOW\", y = \"Count\") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_fill_discrete(name = \"Day of the Week\") # correct format Blink2$Connection <- as.Date(Blink2$Connection, format = \"%A, %B %d, %Y\") # group data by date and count occurrences per date date_occurrences <- Blink2 %>% group_by(Connection) %>% summarise(occurrences = n()) # display print(date_occurrences) ## # A tibble: 1 \u00d7 2 ## Connection occurrences ## <date> <int> ## 1 NA 237 # maximum, median, and minimum occurrences occurrences_stats <- date_occurrences %>% summarise(max_occurrences = max(occurrences), median_occurrences = median(occurrences), min_occurrences = min(occurrences)) # statistics print(occurrences_stats) ## # A tibble: 1 \u00d7 3 ## max_occurrences median_occurrences min_occurrences ## <int> <int> <int> ## 1 237 237 237 # correct format Blink2$CTime <- as.POSIXct(Blink2$CTime, format = \"%I:%M:%S %p\") # plot ggplot(Blink2, aes(x = CTime, fill = Day_of_Week)) + geom_histogram(binwidth = 3600, #(1 hour = 3600 seconds) color = \"black\", alpha = 0.7) + facet_wrap(~Day_of_Week, ncol = 3) + labs(title = \"Occurrences by DOW and Time\", x = \"Time\", y = \"Count\") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # correct format: character format, convert it to POSIXct Blink2$CTime <- as.POSIXct(Blink2$CTime, format = \"%I:%M:%S %p\") # group data by connection column and calculate mmm time_summary <- Blink2 %>% group_by(Day_of_Week) %>% summarise(greatest_time = max(CTime), median_time = median(CTime, na.rm = TRUE), least_time = min(CTime)) print(time_summary)","title":"Data Processing:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#data-analysis-summary","text":"From this analysis we conclude that the optimal number of chargers is 12 because it would sustain current demand and accomdate fluctuational demand. Based on this analysis, the estimated general trend of demand over 10 weeks is around 840 connections. Using Blink charging data, our main objective was to identify demand for electric vehicle charging. The blink charge data was collected from PSU Transportation and parking Services. We analyzed data through August, September and October. Three different PSU charging sites are listed in our data report. Portland State University, Portland State University-Parking One, and Portland State University RMNC are the three locations. We were able to determine the duration of the car\u2019s connection to the charging port based on the provided data, which also included the day of the week, date, and time of the connection as well as information about the disconnection. As a result, we could observe the entire amount of time spent charging each car. The Data Report also included additional invoicing information and the maximum power used by each car. Every record in the data had a serial number that corresponded to the car\u2019s connection to the charging port. Based on the data given the car takes an average of four hours and five minutes to charge. In our data, \u201coccurrences\u201d refers to the total number of times the cars have used the charger connection. The resulting list of occurrences showed that there were 20 on Monday, 42 on Tuesday, 37 on Wednesday, 33 on Thursday, 36 on Friday, 50 on Saturday, and 19 on Sunday.The total number of occurrences was 237, and when compared to the other days of the week, we observed that Saturday has the highest number of occurrences.","title":"Data Analysis Summary:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#installation-optimization-model","text":"To ensure we meet business needs as identified by our interview with PSU\u2019s Transportation and Parking department, our initial optimization model focuses on centralizing PSU\u2019s current charging demand in Parking Structure 3. The model is heavily simplified to meet our capacity within this project, and so does not take into account every factor that would apply to the installation and costing process. Instead, we are focusing on the following elements: Purchase costs and charger storage Installation labor costs Timeline requirements Available space for construction Additionally, we have severe limitations on our data availability. The Blink charging dataset is a representative subset of PSU\u2019s full charging needs and parking demand. However, we did not have access to additional details on PSU\u2019s parking demand and charging needs. As such, we used the Blink data to extrapolate assumptions about the charging and parking demand on campus, but additional details would be needed to complete the picture. Along with that limitation, we did not have information on authentic labor costs for installation when using PSU\u2019s construciton services, and so generated estimates based on our understanding of construction expenses and from conversations with individuals who have had other work completed by PSU in the past.","title":"Installation Optimization Model"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#model-formulation","text":"Our model\u2019s objective is to minimize costs, which are the sum of installation and purchase costs. Our first constraint is that we must install our required number of chargers (the demand) within our required time limit. Outside of the constraints that set limitations on variables (eg. binary vs. integer), the remaining constraints can be grouped as: Purchasing : The total purchase price each week is the number of chargers purchased multiplied by the cost of each charger. Additionally, we are adding in a bulk-purchase discount, where purchasing 5 chargers provides a discount of 20% (i.e. the price of a charger). The total purchase cost per week subtracts the applicable discount from the cost of the purchased chargers for the week. The cost of the chargers, $300, was provided from our interview with Transportation and Parking, however the discount was an added feature included due to research into various charging companies. Storage : Inventory management is an important element of any installation process, and so we wanted to be sure to include it in this project. However, this was an area where we had limitations on data. As such, we set an estimated limit of the number of chargers available to store at one time as a means of controlling how many could be purchased and installed each week. We also estimated that there were already a certain number of chargers in storage, available for install on week 1. We then established constraints around charger inventory at the end of each week, based on the number of chargers at the start of the week, how many were installed, and how many were purchased. Installation : Our installation costs were another area of limited data. However, it is important to take into account the cost of labor in a model such as this, as skilled installation is often quite a bit more than the cost of the product itself. Using our familiarity with general construction practices, we established a base installation fee which is charged no matter the number of chargers installed (so long as it\u2019s more than 0), to account for the work that must be done whether 1 or 20 chargers are installed in a given week. Added to that is a per-charger installation cost to account for the increase in work. We also set a limit on the number of chargers that could be installed each week, as well as limitations on how much space could be used for construction, identified as a count of parking spaces (assuming a certain number of surrounding spaced would be used for each install) with the limitation based on an estimated percent of available space in the parking garage for a given week during the term. Variables Reference: Charger Tracking: D = Total chargers to install B S = Starting inventory of chargers (week 0) x t = Chargers installed on week t y t = Chargers purchased in week t z t = Stored chargers at the end of week t Costs: B I = Base cost for install C I = Per-charger install cost C Ti t = Total install cost for week t C P = Per-charger purchase cost C Tp t = Total purchase costs for week t Limits: L I = Weekly install limit L S = Charger storage limit L A t = Limit available area for construction for week t Weights: W I t = Install binary for week t W P t = Discount binary for week t W A = Weight of spaces to be used","title":"Model Formulation:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#model-implementation","text":"For our implementation, we are setting external variables based on calculated (from the Blink), provided (from the PSU interview), or estimated values as previously discussed. The initial model uses Parking Structure 3 as the location for installation, chosen due to our conversation with Transportation and Parking. Our limits on how much of the parking structure is available for construction, as well as our demand, are extrapolations from the Blink charging data while the per-charger purchase cost was provided by Transportation and Parking. All other inputs are estimates. lot <- 1300 #total spaces in parking structure 3 D <- 12 #total chargers to install BS <- 2 #starting inventory of chargers LI <- 3 #max chargers to install each week LS <- 5 #max inventory capacity for chargers CI <- 2000 #per charger install labor cost CP <- 300 #per charger purchase cost WA <- 5 #average area used during install (parking spaces) BI <- 5000 #base cost of install #percent of lot available for construction, capped at 50% LA <- c(lot*.5, #pre-term lot*0, lot*.0, lot*.19, lot*.29, lot*.40, #weeks 1-5 of term lot*.37, lot*.35, lot*.39, lot*.31, lot*.32, #weeks 6-10 of term lot*.0) #finals week weeks <- length(LA) #completion time (weeks) model <- MIPModel() |> add_variable(Vx[t], t=1:weeks, type=\"integer\", lb = 0, ub = LI) |> #var: install/week add_variable(Vy[t], t=1:weeks, type=\"integer\", lb = 0) |> #var: purchases/week add_variable(Vz[t], t=1:weeks, type=\"integer\", lb = 0, ub = LS) |> #var: chargers stored/week add_variable(VCTi[t], t=1:weeks, type=\"continuous\", lb = 0) |> #var: total install/week add_variable(VCTp[t], t=1:weeks, type=\"continuous\", lb = 0) |> #var: total purchase/week add_variable(VWp[t], t=1:weeks, type=\"binary\") |> #var: bulk discount activator add_variable(VWi[t], t=1:weeks, type=\"binary\") |> #var: base install binary set_objective(sum_expr(VCTi[t]+VCTp[t], t=1:weeks), \"min\") |> #obj: min costs add_constraint(sum_expr(Vx[t], t=1:weeks)==D) |> #con: install total demand add_constraint(Vz[t-1]+Vy[t]-Vx[t] == Vz[t], t = 2:weeks) |> #con: weeks 2+ stored chargers add_constraint(BS+Vy[t]-Vx[t] == Vz[t], t = 1) |> #con: week 1 stored chargers add_constraint(Vx[t] <= Vz[t-1], t = 2:weeks) |> #con: weeks 2+ install limit add_constraint(Vx[t] <= BS, t = 1) |> #con: week 1 install limit add_constraint(Vx[t]*WA <= LA[t], t = 1:weeks) |> #con: construction area limit add_constraint(Vx[t]-D*VWi[t] <= 0, t = 1:weeks) |> #con: base charge if installing add_constraint(Vx[t]*CI+VWi[t]*BI == VCTi[t], t = 1:weeks) |> #con: installation cost/week add_constraint(Vy[t]-5*VWp[t]>=0, t = 1:weeks) |> #con: bulk discount add_constraint(Vy[t]*CP - CP*VWp[t] == VCTp[t], t = 1:weeks) #con: purchase cost/week model_res <- solve_model(model, with_ROI(solver = \"glpk\"))","title":"Model Implementation:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#model-solution","text":"The solution for this model has four installation weeks, with each week using the full limit of possible installations (3). This minimizes the number of base installation charges incurred, which are one of the primary cost drivers for the installation costs. For purchasing chargers, the model only purchases enough to meet the discount threshold once, indicating that the purchase costs are not an overall cost driver to the installation plan. The total expense for installation using this model would be $46,700, including both installation and purchase costs, with a completion date of the last week of term. One thing to note with this plan is that the total space avaiable for construction, excluding the weeks when we set availability to 0, did not appear to be a limiting factor. By capping installations at 3 per week, we are at maximum using 15 spaces for a given week\u2019s installation. Outside of the weeks when availability was set to 0, the week with the least available space was week 3 of term, with 19% of the lot available for construction. In a lot with 1,300 spaces, that\u2019s 247 spaces available for construction, well above the threshold established by our installation limits. One Term Charger Install Plan, Parking Structure 3 No Classes Week 1 Week 2 Week 3 Week 4 Mid Terms Week 6 Week 7 Week 8 Week 9 Week 10 Finals TOTAL Install Count 0 0 0 3 3 3 0 0 0 0 3 0 12 Purchased Chargers 1 0 0 3 5 1 0 0 0 0 0 0 10 Stored Chargers 3 3 3 3 5 3 3 3 3 3 0 0 - Labor Costs 0 0 0 11000 11000 11000 0 0 0 0 11000 0 44000 Purchase Costs 300 0 0 900 1200 300 0 0 0 0 0 0 2700 TOTAL Costs 300 0 0 11900 12200 11300 0 0 0 0 11000 0 46700","title":"Model Solution:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#alternative-solution","text":"We wanted to posit an alternative solution to moving all charging demand to Parking Structure 3, though. That parking structure has severe limitations related to location and open hours which could present barriers for commuters to use that as their primary parking location. There is, however, another parking structure closer to the center of campus, the underground Fourth Avenue Building (FAB) parking lot. This lot is smaller in capacity, with a total of 430 spaces, but also contains both hourly and monthly parking options and (based on our personal observances) is rarely filled to capacity. As such, this could be an excellent alternative location for centralized EV charging. The formulation for the model remains as listed above, as this is a generalized model. As such, for the sake of space, we will not repeat the model formulation here.","title":"Alternative Solution:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#alternative-location-model","text":"With the implementation, there are also no adjustments to the actual model build itself. Rather, all adjustments come from the external variables. As such, again for the sake of space, we will omit displaying the implementation for the alternative model, and instead focus on the changes to the external variables. When working with the FAB parking structure, there are two things we need to take into account. The first is that is it a much smaller structure, with 430 spaces compared to Parking Structure 3\u2019s 1300 spaces. The other is that this parking structure has much higher employee commuter usage and reduced public usage, due to it\u2019s location and lack of publicity. To support both of these concerns, we drastically reduced the amount of available space for construction during the term to be 2% of the total lot size, and adjusted the off-term availability to be 5% of the total lot size. This is because, while there are student commuters that use this lot, we wanted to ensure enough standard parking was available for employees who continue to commute to campus even when school is not in session. lot <- 430 #total spaces in parking structure 3 D <- 12 #total chargers to install BS <- 2 #starting inventory of chargers LI <- 3 #max chargers to install each week LS <- 5 #max inventory capacity for chargers CI <- 2000 #per charger install labor cost CP <- 300 #per charger purchase cost WA <- 5 #average area used during install (parking spaces) BI <- 5000 #base cost of install #percent of lot available for construction, capped at 50% LA <- c(lot*.05, #pre-term lot*.02, lot*.02, lot*.02, lot*.02, lot*.02, #weeks 1-5 of term lot*.02, lot*.02, lot*.02, lot*.02, lot*.02, #weeks 6-10 of term lot*.02) #finals week weeks <- length(LA) #completion time (weeks)","title":"Alternative Location Model:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#alternative-location-solution","text":"By forcing the single term install in the FAB parking structure, as a reflection of our original modeling for Parking Structure 3, our total costs increased by approximately $25,000. This is due to the cap on available space throughout the term, which forced the model to have more frequent installs of just a single charger, thereby increasing the total count of base install fees. To minimize costs as best as possible the model moved to two bulk purchases of charges (rather than more frequent smaller purchases). However, that is a minimal savings ($600) when compared to the total cost of the installation project. One Term Charger Install Plan, FAB Parking No Classes Week 1 Week 2 Week 3 Week 4 Mid Terms Week 6 Week 7 Week 8 Week 9 Week 10 Finals TOTAL Install Count 2 1 1 1 1 1 1 1 0 1 1 1 12 Purchased Chargers 5 0 0 0 0 5 0 0 0 0 0 0 10 Stored Chargers 5 4 3 2 1 5 4 3 3 2 1 0 - Labor Costs 9000 7000 7000 7000 7000 7000 7000 7000 0 7000 7000 7000 79000 Purchase Costs 1200 0 0 0 0 1200 0 0 0 0 0 0 2400 TOTAL Costs 10200 7000 7000 7000 7000 8200 7000 7000 0 7000 7000 7000 81400","title":"Alternative Location Solution:"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#discussion","text":"At first blush, the alternative location appears to be a worse choice due to the added limitations on construction space. After all, based on our two implementation options, we can determine that a fast, single-term, roll-out of charger installations in an under-utilized parking structure is feasible given our assumptions of charging demand. And when given the option between two installations that vary by about $25,00, it seems like the optimal choice is obvious: centralize EV charging locations in Parking Structure 3. However, the initial numbers are only part of the story. With that understanding, we reached out to various EV drivers who commute to campus on a regular basis to understand their barriers to charging on campus, and their impression of the current infrastructure. From those conversations, we have garnered the impression that the biggest barrier to EV charging on campus for commuters is lack of availability for monthly permit holders. According to one individual who holds a monthly permit, all EV spaces are in hourly parking and when parking with their permit in those spaces they were ticketed for doing so, since they hadn\u2019t paid for the hourly parking. It does appear that the bulk, if not all, of PSU\u2019s EV charging spaces are relegated to hourly parking locations. Regular campus commuters are more likely to use monthly or term-based parking permits, rather than using hourly parking. If they are already paying for a space, there is no incentive to then pay a second time for charging at a Level 1 charger. As indicated in our literary reviews, EV ownership is on the rise, particularly in the Portland Metro Area. We as a university need to determine a charging solution that fits the authentic needs of campus commuters, rather than focusing on data collected from hourly parking locations as that is less likely to demonstrate the needs of our daily commuters. Our recommendation would be to spend additional time learning about the driving habits of the true campus commuters, ones who hold monthly or term-based parking permits, to determine and work to meet their EV charging needs. From that, we would recommend creating a charger location and installation plan that supports those commuters, ensuring that monthly or term-based permit holders have equitable access to EV charging. While considering these options, we would also encourage the university to conduct a deeper analysis into options related to some of our smaller garages, rather than the main parking structures, as those lots would allow the university to continue to focus on general parking in the main lots and potentially re-purpose some of the smaller lots to support the growing base of EV drivers.","title":"Discussion"},{"location":"EV_Charging_Optimization/EV_Charging_Optimization/#references","text":"\u201c2023 Biennial Zero-Emission Vehicle Report.\u201d 2023, September. https://www.oregon.gov/energy/Dataand-Reports/Documents/2023-Biennial-Zero-Emission-Vehicle-Report.pdf . Ahmad, Fareed, Atif Iqbal, Imtiaz Ashraf, Mousa Marzband, and Irfan khan. 2022. \u201cOptimal Location of Electric Vehicle Charging Station and Its Impact on Distribution Network: A Review.\u201d Energy Reports 8: 2314\u201333. https://doi.org/https://doi.org/10.1016/j.egyr.2022.01.180 . \u201cCAMPUS BUILDINGS.\u201d n.d. Portland State University. https://www.pdx.edu/buildings/?facility_features%5B701%5D=701&page=0 . \u201cCharger Types and Speeds.\u201d 2023. US Department of Transportation. https://www.transportation.gov/rural/ev/toolkit/ev-basics/charging-speeds . David Howell, Brian Cunningham, Steven Boyd. 2017. \u201cEnabling Fast Charging: A Technology Gap Assessment.\u201d Office of Energy Efficiency & Renewable Energy, September. https://www.energy.gov/sites/prod/files/2017/10/f38/XFC%20Technology%20Gap%20Assessment%20Report_FINAL_10202017.pdf . \u201cDeveloping Infrastructure to Charge Electric Vehicles.\u201d n.d. US Department of Energy. https://afdc.energy.gov/fuels/electricity_infrastructure.html . \u201cElectric Vehicles.\u201d n.d. International Energy Agency. https://www.iea.org/energy-system/transport/electric-vehicles . \u201cFACTS : PSU BY THE NUMBERS.\u201d n.d. Portland State University. https://www.pdx.edu/portlandstate-university-facts.12 \u201cHOURLY & VISITOR PARKING.\u201d n.d. Portland State University. https://www.pdx.edu/transportation/hourly-visitor-parking . Lee, Jae Hyun, Debapriya Chakraborty, Scott J. Hardman, and Gil Tal. 2020. \u201cExploring Electric Vehicle Charging Patterns: Mixed Usage of Charging Infrastructure.\u201d Transportation Research Part D: Transport and Environment 79: 102249. https://doi.org/https://doi.org/10.1016/j.trd.2020.102249 . Moradijoz, M., F. Moazzen, S. Allahmoradi, M. Parsa Moghaddam, and M. R Haghifam. 2018. \u201cA Two Stage Model for Optimum Allocation of Electric Vehicle Parking Lots in Smart Grids.\u201d In 2018 Smart Grid Conference (SGC), 1\u20135. https://doi.org/10.1109/SGC.2018.8777877 . \u201cPSU Employee Headcount.\u201d 2023. Portland State University. https://public.tableau.com/app/profile/portland.state/viz/PSUEmployeeHeadcount/About . Simorgh, Hamid, Hasan Doagou-Mojarrad, Hadi Razmi, and Gevork B. Gharehpetian. 2018. \u201cCostBased Optimal Siting and Sizing of Electric Vehicle Charging Stations Considering Demand Response Programmes.\u201d IET Generation, Transmission & Distribution 12 (8): 1712\u201320. https://doi.org/https://doi.org/10.1049/iet-gtd.2017.1663 . Zhou, Min, Piao Long, Nan Kong, Lindu Zhao, Fu Jia, and Kathryn S Campy. 2021. \u201cCharacterizing theMotivational Mechanism Behind Taxi Driver\u2019s Adoption of Electric Vehicles for Living: Insights from China.\u201d Transportation Research Part A: Policy and Practice 144: 134\u201352.","title":"References"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/","text":"Fraudulent Account Detection The file users.csv contains data on a subset of fictional banking users. The abbreviation 'KYC' stands for 'Know Your Customer' - a process of identifying and verifying the client's identity when opening an account and periodically over time. The variable IS_FRAUDSTER from this dataset is your target variable. The file transactions.csv contains details of fictional transactions of these users. Project and data procured from collected from StrataScratch . # import packages import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, precision_score from sklearn.preprocessing import MinMaxScaler from sklearn import metrics, preprocessing from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsClassifier from datetime import datetime #date time now = datetime.now() print('Analysis on', now.strftime('%Y-%m-%d'), 'at', now.strftime('%H:%M')) Analysis on 2024-07-27 at 09:09 Analysis on 2024-07-27 at 09:09 Data Exploration As shown through our initial exploration, below, we are starting with two dataframes derived from provided CSV files. I'm also including a third dataframe which provides details on currencies vs. country of origin. This was generated from the UPS Country/Territory and Currency Codes document . By doing so, we will have three starting dataframes: Dataframe Source Description transactions transactions.csv Each row represents an individual transation with various details, including user, currency type, amount, and other relevant information. users users.csv Each row represents a single user with various details related to the user's account. included is the 'is_fraudster' identifier, which is our target. ccy ups_currencies.csv Each row represents a single currency type used in different countries The goal of this section and the next is to understand the relevant features in each dataframe, and figure out a way to combine the relevant details into a singular resource for analysis. # load data transactions = pd.read_csv('data/FRAUD-transactions.csv') users = pd.read_csv('data/FRAUD-users.csv') ccy = pd.read_csv('data/FRAUD-ups_currencies.csv') # set names for dataframes for reference transactions.name = 'Transactions' users.name = 'Users' ccy.name = 'CCY' # create a list of dataframes for iteration dataframes = [transactions, users, ccy] # get info for each dataframe for df in dataframes: print(df.info(),'\\n') <class 'pandas.core.frame.DataFrame'> RangeIndex: 688651 entries, 0 to 688650 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 CURRENCY 688651 non-null object 1 AMOUNT 688651 non-null int64 2 STATE 688651 non-null object 3 CREATED_DATE 688651 non-null object 4 MERCHANT_CATEGORY 223065 non-null object 5 MERCHANT_COUNTRY 483055 non-null object 6 ENTRY_METHOD 688651 non-null object 7 USER_ID 688651 non-null object 8 TYPE 688651 non-null object 9 SOURCE 688651 non-null object 10 ID 688651 non-null object 11 AMOUNT_USD 635328 non-null float64 dtypes: float64(1), int64(1), object(10) memory usage: 63.0+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 9944 entries, 0 to 9943 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 9944 non-null object 1 HAS_EMAIL 9944 non-null int64 2 PHONE_COUNTRY 9944 non-null object 3 IS_FRAUDSTER 9944 non-null bool 4 TERMS_VERSION 8417 non-null object 5 CREATED_DATE 9944 non-null object 6 STATE 9944 non-null object 7 COUNTRY 9944 non-null object 8 BIRTH_YEAR 9944 non-null int64 9 KYC 9944 non-null object 10 FAILED_SIGN_IN_ATTEMPTS 9944 non-null int64 dtypes: bool(1), int64(3), object(7) memory usage: 786.7+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 250 entries, 0 to 249 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 COUNTRY 250 non-null object 1 UPS_Code 249 non-null object 2 IATA_Code 249 non-null object 3 CCY 250 non-null object dtypes: object(4) memory usage: 7.9+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 688651 entries, 0 to 688650 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 CURRENCY 688651 non-null object 1 AMOUNT 688651 non-null int64 2 STATE 688651 non-null object 3 CREATED_DATE 688651 non-null object 4 MERCHANT_CATEGORY 223065 non-null object 5 MERCHANT_COUNTRY 483055 non-null object 6 ENTRY_METHOD 688651 non-null object 7 USER_ID 688651 non-null object 8 TYPE 688651 non-null object 9 SOURCE 688651 non-null object 10 ID 688651 non-null object 11 AMOUNT_USD 635328 non-null float64 dtypes: float64(1), int64(1), object(10) memory usage: 63.0+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 9944 entries, 0 to 9943 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 9944 non-null object 1 HAS_EMAIL 9944 non-null int64 2 PHONE_COUNTRY 9944 non-null object 3 IS_FRAUDSTER 9944 non-null bool 4 TERMS_VERSION 8417 non-null object 5 CREATED_DATE 9944 non-null object 6 STATE 9944 non-null object 7 COUNTRY 9944 non-null object 8 BIRTH_YEAR 9944 non-null int64 9 KYC 9944 non-null object 10 FAILED_SIGN_IN_ATTEMPTS 9944 non-null int64 dtypes: bool(1), int64(3), object(7) memory usage: 786.7+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 250 entries, 0 to 249 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 COUNTRY 250 non-null object 1 UPS_Code 249 non-null object 2 IATA_Code 249 non-null object 3 CCY 250 non-null object dtypes: object(4) memory usage: 7.9+ KB None # function to get unique and null values for each column in a dataframe def matrix(dataframe): temp = [[dataframe[column].nunique() for column in dataframe.columns], [dataframe[column].isnull().sum() for column in dataframe.columns]] print(dataframe.name,'has',len(dataframe),'total values; column details:\\n') print(pd.DataFrame(temp, columns=dataframe.columns, index=['Unique Values', 'Null Values']).transpose()) # iterate through dataframes to get unique and null values for each column for df in dataframes: print(matrix(df),'\\n--------------------------\\n') Transactions has 688651 total values; column details: Unique Values Null Values CURRENCY 29 0 AMOUNT 32257 0 STATE 7 0 CREATED_DATE 688640 0 MERCHANT_CATEGORY 115 465586 MERCHANT_COUNTRY 344 205596 ENTRY_METHOD 6 0 USER_ID 8021 0 TYPE 5 0 SOURCE 11 0 ID 688651 0 AMOUNT_USD 37402 53323 None -------------------------- Users has 9944 total values; column details: Unique Values Null Values ID 9944 0 HAS_EMAIL 2 0 PHONE_COUNTRY 83 0 IS_FRAUDSTER 2 0 TERMS_VERSION 7 1527 CREATED_DATE 9944 0 STATE 2 0 COUNTRY 46 0 BIRTH_YEAR 69 0 KYC 4 0 FAILED_SIGN_IN_ATTEMPTS 5 0 None -------------------------- CCY has 250 total values; column details: Unique Values Null Values COUNTRY 250 0 UPS_Code 249 1 IATA_Code 224 1 CCY 139 0 None -------------------------- Transactions has 688651 total values; column details: Unique Values Null Values CURRENCY 29 0 AMOUNT 32257 0 STATE 7 0 CREATED_DATE 688640 0 MERCHANT_CATEGORY 115 465586 MERCHANT_COUNTRY 344 205596 ENTRY_METHOD 6 0 USER_ID 8021 0 TYPE 5 0 SOURCE 11 0 ID 688651 0 AMOUNT_USD 37402 53323 None -------------------------- Users has 9944 total values; column details: Unique Values Null Values ID 9944 0 HAS_EMAIL 2 0 PHONE_COUNTRY 83 0 IS_FRAUDSTER 2 0 TERMS_VERSION 7 1527 CREATED_DATE 9944 0 STATE 2 0 COUNTRY 46 0 BIRTH_YEAR 69 0 KYC 4 0 FAILED_SIGN_IN_ATTEMPTS 5 0 None -------------------------- CCY has 250 total values; column details: Unique Values Null Values COUNTRY 250 0 UPS_Code 249 1 IATA_Code 224 1 CCY 139 0 None -------------------------- transactions.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 CURRENCY GBP PLN GBP AMOUNT 4420 1500 191 STATE COMPLETED COMPLETED COMPLETED CREATED_DATE 2017-12-10 16:38:55.577 2017-12-10 16:37:24.792 2017-12-10 16:37:16.234 MERCHANT_CATEGORY NaN point_of_interest airport MERCHANT_COUNTRY NLD POL PRT ENTRY_METHOD chip manu chip USER_ID 3ff52b92-d416-4e22-8cad-018f500d4bbc 76cbaad3-4721-4a3b-92b9-3eb9e9319565 7bcaa34e-b889-4582-9c29-0b3bab34fb8c TYPE ATM CARD_PAYMENT CARD_PAYMENT SOURCE GAIA GAIA GAIA ID 367bf5f9-7cce-4683-90b9-d3c011bf4c87 ff6802b9-360d-4efe-b09b-f99c6cac3383 ddb4a930-7d8a-4f38-9079-ddc4b0db980e AMOUNT_USD 3268.0 NaN 141.0 users.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb HAS_EMAIL 1 1 1 PHONE_COUNTRY GB||JE||IM||GG GB||JE||IM||GG ES IS_FRAUDSTER False False False TERMS_VERSION 2018-05-25 2018-01-01 2018-09-20 CREATED_DATE 2017-08-06 07:33:33.341000 2017-03-07 10:18:59.427000 2018-05-31 04:41:24.672000 STATE ACTIVE ACTIVE ACTIVE COUNTRY GB GB ES BIRTH_YEAR 1971 1982 1973 KYC PASSED PASSED PASSED FAILED_SIGN_IN_ATTEMPTS 0 0 0 ccy.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 COUNTRY Afghanistan Aland Islands Albania UPS_Code AF AX AL IATA_Code AF AX AL CCY USD EUR EUR Before getting too much further, I'm going to check the correlation between the users's state, number of failed sign-in attempts, and KYC status as compared to if the user is a fraudster. The goal is to identify if there is a 1:1 ratio, in which case the variable would be meaningless. As shown below, there is a 1:1 correlation for if the account is locked, so we won't use that variable. However, the other two (KYC and failed sign in attempts) are still potential options. # check for connection between STATE and IS_FRAUDSTER fraudsters = users[users['IS_FRAUDSTER'] == True] not_fraudsters = users[users['IS_FRAUDSTER'] == False] print('Account states for fraud accounts:',fraudsters['STATE'].unique(), '\\nAccount states for non-fraud accounts:', not_fraudsters['STATE'].unique(), '\\n\\nKYC for fraud accounts:',fraudsters['KYC'].unique(), '\\nKYC for non-fraud accounts:', not_fraudsters['KYC'].unique(), '\\n\\nFailed Sign In for fraud accounts:',fraudsters['FAILED_SIGN_IN_ATTEMPTS'].unique(), '\\nFailed Sign In for non-fraud accounts:', not_fraudsters['FAILED_SIGN_IN_ATTEMPTS'].unique()) Account states for fraud accounts: ['LOCKED'] Account states for non-fraud accounts: ['ACTIVE'] KYC for fraud accounts: ['NONE' 'PASSED' 'PENDING' 'FAILED'] KYC for non-fraud accounts: ['PASSED' 'NONE' 'FAILED' 'PENDING'] Failed Sign In for fraud accounts: [0 1 2] Failed Sign In for non-fraud accounts: [0 1 3 2 6] Account states for fraud accounts: ['LOCKED'] Account states for non-fraud accounts: ['ACTIVE'] KYC for fraud accounts: ['NONE' 'PASSED' 'PENDING' 'FAILED'] KYC for non-fraud accounts: ['PASSED' 'NONE' 'FAILED' 'PENDING'] Failed Sign In for fraud accounts: [0 1 2] Failed Sign In for non-fraud accounts: [0 1 3 2 6] From here we'll begin feature engineering using the three remaining dataframes, and continue unifying all relevant data into a single dataframe. Feature Engineering For the analysis, I'm proposing the following initial features as potential predictors: | Column| Type | Description| |------------- |------------- |-------------| | IS_FRAUDSTER | bool | user IDed as fraudster | | HAS_EMAIL | bool | user has registered email | | FAILED_SIGN_IN_ATTEMPTS | cont | count of failed sign in attempts | | KYC_[STATUS] | dummy/bool | user's stage in identification verification | | PHONE_COUNTRY_MATCH | bool | user's listed country matches their phone's country code | | PHONE_MULTIPLE_COUNTRIES | bool | user has phone numbers in multiple countries | | RECENT_TERMS | bool | user has accepted the most recent TOC| | CREATED_TO_FIRST | cont | minutes between account creation to first transaction| | TOTAL_TRANSACTION_COUNT | int | count of user's total transactions | | TOTAL_TRANSACTION_SUM | cont | sum of user's total transaction amounts | | MEDIAN_TRANSACTION_AMOUNT | cont | median transaction amount | | AVG_DAILY_TRANSACTION_COUNT | cont | average transactions made per day by user | | INTL_TRANSACTION_PCT | cont | percent of user's transactions that don't match their listed country's currency | | TYPE_[TYPE] | cont | percent of transactions made of each transaction type | | METHOD_[METHOD] | cont | percent of transactions attributed to different entry methods | Note: Each sample accounts for a single user. Additionally, all these features will not be used in the final analysis. We are simply generating them as a starting point to identify the most relevant features. This first section takes care of the low-hanging fruit: * Updating data types and renaming columns as needed * Creating the analysis dataframe with features that are already ready for use * Generating new features according to the following: * If the user's country and the country code for their phone match * If the user has more than one phone country listed * If the user has accepted the most recent terms of service # update date types to datetime for calculating users['TERMS_VERSION'] = pd.to_datetime(users['TERMS_VERSION']) users['CREATED_DATE'] = pd.to_datetime(users['CREATED_DATE']) transactions['CREATED_DATE'] = pd.to_datetime(transactions['CREATED_DATE'], format='mixed') # begin dataframe with columns that don't need modification df = pd.DataFrame(users.loc[:,['ID', 'IS_FRAUDSTER', 'HAS_EMAIL', 'KYC', 'FAILED_SIGN_IN_ATTEMPTS']]) # rename ID to simplify later merging df.rename(columns={'ID':'USER_ID'}, inplace=True) # update data types & create dummy variables df['HAS_EMAIL'] = df['HAS_EMAIL'].astype(bool) df = pd.get_dummies(df, columns=['KYC'], prefix='KYC', drop_first=False) # get details for columns that need simple adjustments df['PHONE_COUNTRY_MATCH'] = users['COUNTRY'] == users['PHONE_COUNTRY'] df['PHONE_MULTIPLE_COUNTRIES'] = users['PHONE_COUNTRY'].str.len() > 2 df['RECENT_TERMS'] = users['TERMS_VERSION'] == users['TERMS_VERSION'].max() df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True This next section deals with slightly more complex, yet still straightforward, feature engineering, either used directly or to assist with other feature engineering: * Calculating the time from account creation to the first transaction * Getting the total number of transactions for each user * Getting the total sum (in USD) of each user's transactions (note: we're using USD to avoid unit discrepencies by using multiple currencies) * Identifying the median transaction amount for the user (note: we're avoiding the mean transaction amount since that can be determined by the sum and count of transactions, and thereby will be overly influential on the analysis) * Getting the average number of transactions made per day by the user # created to first transaction create_to_first = transactions.groupby('USER_ID')['CREATED_DATE'].min().reset_index() create_to_first['USER_CREATED'] = create_to_first['USER_ID'].map(users.set_index('ID')['CREATED_DATE']) create_to_first['CREATED_TO_FIRST'] = \\ ((create_to_first['CREATED_DATE'] - create_to_first['USER_CREATED']).dt.total_seconds())/60 create_to_first = create_to_first.drop(columns=['CREATED_DATE', 'USER_CREATED']) df = pd.merge(df, create_to_first, on='USER_ID', how='left') # total transaction count total_transaction_count = transactions.groupby('USER_ID')['ID'].count().reset_index() total_transaction_count.rename(columns={'ID': 'TOTAL_TRANSACTION_COUNT'}, inplace=True) df = pd.merge(df, total_transaction_count, on='USER_ID', how='left') # sum of transaction amounts total_transaction__sum = transactions.groupby('USER_ID')['AMOUNT_USD'].sum().reset_index() total_transaction__sum.rename(columns={'AMOUNT_USD': 'TOTAL_TRANSACTION_SUM'}, inplace=True) df = pd.merge(df, total_transaction__sum, on='USER_ID', how='left') # median transaction amount median_transaction_amount = transactions.groupby('USER_ID')['AMOUNT_USD'].median().reset_index() median_transaction_amount.rename(columns={'AMOUNT_USD': 'MEDIAN_TRANSACTION_AMOUNT'}, inplace=True) df = pd.merge(df, median_transaction_amount, on='USER_ID', how='left') # avg daily transaction count avg_daily_transaction_count = transactions.groupby(['USER_ID', 'CREATED_DATE'])['ID'].count().reset_index() avg_daily_transaction_count = avg_daily_transaction_count.groupby('USER_ID')['ID'].mean().reset_index() avg_daily_transaction_count.rename(columns={'ID': 'AVG_DAILY_TRANSACTION_COUNT'}, inplace=True) df = pd.merge(df, avg_daily_transaction_count, on='USER_ID', how='left') # review results of all the merges df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 Now we move into the more in-depth feature engineering, starting with identifying the percent of international transactions per user. To do so, we first need to get counts of the purchases made per user, divided by unique currency codes. # group transactions by user and currency international_transactions = transactions.groupby(['USER_ID', 'CURRENCY'])['ID'].nunique().reset_index() international_transactions.rename(columns={'ID': 'TRANSACTION_COUNT', 'CURRENCY': 'CCY'}, inplace=True) # merge with relevant columns from users dataframe international_transactions = pd.merge(international_transactions, users[['ID', 'COUNTRY']], left_on='USER_ID', right_on='ID', how='left') international_transactions.rename(columns={'COUNTRY': 'USER_COUNTRY'}, inplace=True) # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 581 USER_COUNTRY 581 dtype: int64 # get unique user_ids for grouped transactions with null values no_id = international_transactions[international_transactions['ID'].isnull()] no_id = no_id['USER_ID'].unique() # compare to users dataframe users[users['ID'].isin(no_id)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID HAS_EMAIL PHONE_COUNTRY IS_FRAUDSTER TERMS_VERSION CREATED_DATE STATE COUNTRY BIRTH_YEAR KYC FAILED_SIGN_IN_ATTEMPTS Based on the above information, there are 581 grouped transactions with listed User IDs that don't match the provided list of users. As such, we would not have identifiers for if these are fraudulent accounts, and so we're dropping those rows. # drop null values; transactions without registered user information international_transactions.dropna(inplace=True) # merge with ccy data for country information, and update column name international_transactions = pd.merge(international_transactions, ccy[['CCY', 'UPS_Code']], on='CCY', how='left') international_transactions.rename(columns={'UPS_Code': 'CCY_COUNTRY'}, inplace=True) # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 0 USER_COUNTRY 0 CCY_COUNTRY 341 dtype: int64 # identify CCY for which CCY_COUNTRY is null ccy_fix = list(international_transactions[international_transactions['CCY_COUNTRY'].isnull()]['CCY'].unique()) ccy_fix ['RON', 'BTC', 'ETH', 'LTC', 'XRP'] A quick Google Search identified the RON as being the Romanian new leu, and the other four as being cryptocurrency. # update country codes for cryptocurrencies crypto = ['BTC', 'ETH', 'LTC', 'XRP'] international_transactions.loc[international_transactions['CCY'].isin(crypto), 'CCY_COUNTRY'] = 'Crypto' # update country codes for RON international_transactions.loc[international_transactions['CCY'] == 'RON', 'CCY_COUNTRY'] = 'RO' # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 0 USER_COUNTRY 0 CCY_COUNTRY 0 dtype: int64 Now that the transactions are grouped and all needed information is connected, we can figure out the percent of international transactions per user. # check for matching country codes, group international_transactions['MATCHES_USER_COUNTRY'] = \\ international_transactions['USER_COUNTRY'] == international_transactions['CCY_COUNTRY'] # group by user and country match bool international_transactions = \\ international_transactions.groupby(['USER_ID', 'MATCHES_USER_COUNTRY'])['ID'].count().reset_index() international_transactions.rename(columns={'ID': 'INTL_TRANSACTIONS'}, inplace=True) # limit to international transactions international_transactions = \\ international_transactions[international_transactions['MATCHES_USER_COUNTRY'] != True] # integrate with total transaction count international_transactions = \\ pd.merge(international_transactions, total_transaction_count, on='USER_ID', how='left') # calculate international transaction percent international_transactions['INTL_TRANSACTION_PCT'] = \\ international_transactions['INTL_TRANSACTIONS'] / international_transactions['TOTAL_TRANSACTION_COUNT'] international_transactions = \\ international_transactions[['USER_ID', 'INTL_TRANSACTION_PCT']] # merge with main dataframe df = pd.merge(df, international_transactions, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 This next section deals with getting the percent of transactions per user of each transaction type. # group by user and transaction type transaction_types = transactions.groupby(['USER_ID', 'TYPE'])['ID'].count().reset_index() # pivot table to get transaction type counts transaction_types = transaction_types.pivot(index='USER_ID', columns='TYPE', values='ID') transaction_types.columns = [f'TYPE_{col}' for col in transaction_types.columns] transaction_types.fillna(0, inplace=True) # in case no transactions of a certain type # calculate the sum of all types for each user row_totals = transaction_types.sum(axis=1) # calculate the percentage for each type per user transaction_types = transaction_types.div(row_totals, axis=0).reset_index() # merge with main dataframe df = pd.merge(df, transaction_types, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.0 0.0 TYPE_BANK_TRANSFER 0.0 0.0 0.0 TYPE_CARD_PAYMENT 0.333333 0.454545 0.5 TYPE_P2P 0.0 0.181818 0.383333 TYPE_TOPUP 0.5 0.363636 0.116667 Similar to how we generated the transaction type values, we're doing the same thing with entry methods # group by user and entry method transaction_method = transactions.groupby(['USER_ID', 'ENTRY_METHOD'])['ID'].count().reset_index() # pivot table to get transaction method counts transaction_method = transaction_method.pivot(index='USER_ID', columns='ENTRY_METHOD', values='ID') transaction_method.columns = [f'METHOD_{col}' for col in transaction_method.columns] transaction_method.fillna(0, inplace=True) # in case no transactions of a certain entry method # calculate the sum of all types for each user row_totals = transaction_method.sum(axis=1) # calculate the percentage for each type per user transaction_method = transaction_method.div(row_totals, axis=0).reset_index() # merge with main dataframe df = pd.merge(df, transaction_method, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.0 0.0 TYPE_BANK_TRANSFER 0.0 0.0 0.0 TYPE_CARD_PAYMENT 0.333333 0.454545 0.5 TYPE_P2P 0.0 0.181818 0.383333 TYPE_TOPUP 0.5 0.363636 0.116667 METHOD_chip 0.333333 0.181818 0.15 METHOD_cont 0.166667 0.272727 0.033333 METHOD_mags 0.0 0.0 0.016667 METHOD_manu 0.0 0.0 0.3 METHOD_mcon 0.0 0.0 0.0 METHOD_misc 0.5 0.545455 0.5 The last steps in this portion are to: * Remove USER_ID, which up to now was used for merging purposes but is now superfluous. We are also replacing na with 0, as those would have beeen generated from a lack of values to calculate. * Convert bool values to int to ensure we can calculate appropriately # identify bool columns bool_columns = df.select_dtypes(include=bool).columns # convert bool columns to integers df[bool_columns] = df[bool_columns].astype(int) # clean up dataframe for next steps df.drop('USER_ID', axis=1, inplace=True) df = df.dropna(how='any', axis=0) # drop rows with inf values df.fillna(0, inplace=True) df.isna().sum() IS_FRAUDSTER 0 HAS_EMAIL 0 FAILED_SIGN_IN_ATTEMPTS 0 KYC_FAILED 0 KYC_NONE 0 KYC_PASSED 0 KYC_PENDING 0 PHONE_COUNTRY_MATCH 0 PHONE_MULTIPLE_COUNTRIES 0 RECENT_TERMS 0 CREATED_TO_FIRST 0 TOTAL_TRANSACTION_COUNT 0 TOTAL_TRANSACTION_SUM 0 MEDIAN_TRANSACTION_AMOUNT 0 AVG_DAILY_TRANSACTION_COUNT 0 INTL_TRANSACTION_PCT 0 TYPE_ATM 0 TYPE_BANK_TRANSFER 0 TYPE_CARD_PAYMENT 0 TYPE_P2P 0 TYPE_TOPUP 0 METHOD_chip 0 METHOD_cont 0 METHOD_mags 0 METHOD_manu 0 METHOD_mcon 0 METHOD_misc 0 dtype: int64 # check head as a final precaution df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 IS_FRAUDSTER 0.000000 0.000000 0.000000 HAS_EMAIL 1.000000 1.000000 1.000000 FAILED_SIGN_IN_ATTEMPTS 0.000000 0.000000 0.000000 KYC_FAILED 0.000000 0.000000 0.000000 KYC_NONE 0.000000 0.000000 0.000000 KYC_PASSED 1.000000 1.000000 1.000000 KYC_PENDING 0.000000 0.000000 0.000000 PHONE_COUNTRY_MATCH 0.000000 0.000000 1.000000 PHONE_MULTIPLE_COUNTRIES 1.000000 1.000000 0.000000 RECENT_TERMS 0.000000 0.000000 1.000000 CREATED_TO_FIRST 8923.971100 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.000000 11.000000 60.000000 TOTAL_TRANSACTION_SUM 61876.000000 43922.000000 114328.000000 MEDIAN_TRANSACTION_AMOUNT 1458.000000 750.000000 156.000000 AVG_DAILY_TRANSACTION_COUNT 1.000000 1.000000 1.000000 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.000000 0.000000 TYPE_BANK_TRANSFER 0.000000 0.000000 0.000000 TYPE_CARD_PAYMENT 0.333333 0.454545 0.500000 TYPE_P2P 0.000000 0.181818 0.383333 TYPE_TOPUP 0.500000 0.363636 0.116667 METHOD_chip 0.333333 0.181818 0.150000 METHOD_cont 0.166667 0.272727 0.033333 METHOD_mags 0.000000 0.000000 0.016667 METHOD_manu 0.000000 0.000000 0.300000 METHOD_mcon 0.000000 0.000000 0.000000 METHOD_misc 0.500000 0.545455 0.500000 Feature Selection # establish features and target all_features = list(df.columns[1:]) target = df.columns[0] # set X and y y = df[target] X = df[all_features] # get variance inflation factor for each feature vif = pd.DataFrame() vif['Predictor'] = X.columns vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] cr = df.corr()[target].round(3) vif['Relevance'] = [cr.iloc[i] for i in range(X.shape[1])] round(vif, 2).sort_values(by='Relevance', ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictor VIF Relevance 0 HAS_EMAIL 1.40 1.00 17 TYPE_CARD_PAYMENT inf 0.17 16 TYPE_BANK_TRANSFER inf 0.16 8 RECENT_TERMS 1.13 0.16 6 PHONE_COUNTRY_MATCH 8.35 0.11 13 AVG_DAILY_TRANSACTION_COUNT 1.00 0.11 3 KYC_NONE inf 0.05 24 METHOD_mcon inf 0.05 1 FAILED_SIGN_IN_ATTEMPTS 1.01 0.04 14 INTL_TRANSACTION_PCT 1.67 0.03 12 MEDIAN_TRANSACTION_AMOUNT 1.08 0.02 2 KYC_FAILED inf 0.01 20 METHOD_chip inf 0.00 5 KYC_PENDING inf -0.00 10 TOTAL_TRANSACTION_COUNT 1.33 -0.01 21 METHOD_cont inf -0.01 25 METHOD_misc inf -0.02 11 TOTAL_TRANSACTION_SUM 1.06 -0.05 19 TYPE_TOPUP inf -0.05 4 KYC_PASSED inf -0.06 23 METHOD_manu inf -0.07 15 TYPE_ATM inf -0.08 22 METHOD_mags inf -0.08 18 TYPE_P2P inf -0.10 9 CREATED_TO_FIRST 1.02 -0.12 7 PHONE_MULTIPLE_COUNTRIES 8.38 -0.15 #multivariate feature selection estimator = DecisionTreeClassifier() selector = RFE(estimator, n_features_to_select=10, step=1).fit(X,y) rnk = pd.DataFrame() rnk['Feature'] = X.columns rnk['Rank']= selector.ranking_ rnk.sort_values('Rank') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feature Rank 12 MEDIAN_TRANSACTION_AMOUNT 1 23 METHOD_manu 1 20 METHOD_chip 1 19 TYPE_TOPUP 1 16 TYPE_BANK_TRANSFER 1 15 TYPE_ATM 1 9 CREATED_TO_FIRST 1 10 TOTAL_TRANSACTION_COUNT 1 11 TOTAL_TRANSACTION_SUM 1 14 INTL_TRANSACTION_PCT 1 17 TYPE_CARD_PAYMENT 2 21 METHOD_cont 3 2 KYC_FAILED 4 25 METHOD_misc 5 18 TYPE_P2P 6 5 KYC_PENDING 7 6 PHONE_COUNTRY_MATCH 8 7 PHONE_MULTIPLE_COUNTRIES 9 8 RECENT_TERMS 10 3 KYC_NONE 11 1 FAILED_SIGN_IN_ATTEMPTS 12 24 METHOD_mcon 13 4 KYC_PASSED 14 13 AVG_DAILY_TRANSACTION_COUNT 15 0 HAS_EMAIL 16 22 METHOD_mags 17 For this next section, I manually adjusted the number of features until we removed all the extremely high VIF scores, to maintain the maximum number of possible variables prior to hyperparameter tuning. # isolate top features selected_features = [feature for feature in all_features if feature in list(rnk.sort_values('Rank').head(8)['Feature'])] # reset X X = df[selected_features] # rerun vf vif = pd.DataFrame() vif['Predictor'] = X.columns vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] cr = df.corr()[target].round(3) vif['Relevance'] = [cr.iloc[i] for i in range(X.shape[1])] round(vif, 2).sort_values(by='Relevance', ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictor VIF Relevance 0 CREATED_TO_FIRST 1.08 1.00 6 METHOD_chip 2.21 0.11 3 TYPE_ATM 1.84 0.05 1 TOTAL_TRANSACTION_COUNT 1.28 0.04 2 MEDIAN_TRANSACTION_AMOUNT 1.11 0.01 5 TYPE_TOPUP 1.27 -0.00 4 TYPE_BANK_TRANSFER 1.18 -0.06 7 METHOD_manu 1.13 -0.15 # establish correlation matrix for intercorrelation corr_matrix = X.corr().abs() # absolute values to make visualizing easier # visualize a simple correlation matrix plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.2f') plt.title('Feature Correlation Matrix') plt.show() # review correlation between target and features df[selected_features + [target]].corr()[target].sort_values().round(2) TOTAL_TRANSACTION_COUNT -0.05 METHOD_chip -0.01 CREATED_TO_FIRST -0.01 TYPE_TOPUP 0.00 METHOD_manu 0.05 MEDIAN_TRANSACTION_AMOUNT 0.11 TYPE_ATM 0.16 TYPE_BANK_TRANSFER 0.17 IS_FRAUDSTER 1.00 Name: IS_FRAUDSTER, dtype: float64 There are a few variables that appear to have intercorrelations: * The groupings of TYPE and METHOD of transaction: Since TYPE provides more specificity, and has higher correlations with the target, we'll maintain that for the analysis. * KYC_PASSED and KYC_NONE: We didn't drop the first dummy for KYC, to figure out which would be most useful. Since KYC_NONE has the stronger relationship with our target, we're going to remove KYC_PASSED. * PHONE_COUNTRY_MATCH and PHONE_MULTIPLE_COUNTRIES: These strongly correlate with each other, and also have decent correlations with the target. However, PHONE_MULTIPLE_COUNTRIES has a slightly stronger correlation, so we'll keep that variable. # update features to remove multicollinear features selected_features = [feature for feature in selected_features if feature not in ['METHOD_chip', 'METHOD_cont', 'METHOD_mags', 'METHOD_manu', 'METHOD_mcon', 'METHOD_misc', 'KYC_PASSED', 'PHONE_COUNTRY_MATCH']] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.4, random_state=17) # determine hyperparameters param_grid = { 'max_depth': list(range(5,20)), 'min_samples_leaf': [10], 'max_features': list(range(2,10)) } gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=17), param_grid, cv=5, n_jobs=-1) gridSearch.fit(X_train, y_train) print('Score: ', gridSearch.best_score_) best_params = gridSearch.best_params_ print('Parameters: ', best_params) Score: 0.9596387177949902 Parameters: {'max_depth': 13, 'max_features': 2, 'min_samples_leaf': 10} Score: 0.9596387177949902 Parameters: {'max_depth': 13, 'max_features': 2, 'min_samples_leaf': 10} # train on tree model using best parameters tree_model = DecisionTreeClassifier(random_state=17, max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'], max_features=best_params['max_features']) tree_model.fit(X_train, y_train) # get feature importances & list of features used feature_importances = tree_model.feature_importances_ # generate a list of features used features_used = X_train.columns[feature_importances > 0] list(features_used) ['CREATED_TO_FIRST', 'TOTAL_TRANSACTION_COUNT', 'MEDIAN_TRANSACTION_AMOUNT', 'TYPE_ATM', 'TYPE_BANK_TRANSFER', 'TYPE_TOPUP'] # isolate top features selected_features = [feature for feature in all_features if feature in list(features_used)] # reset X with top features X = df[selected_features] # resplit the data X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.4, random_state=17) # scale the data scaler = preprocessing.MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # show distribution print('Training samples:',len(X_train),'\\n', y_train.value_counts()) print('\\n') print('Testing samples:',len(X_test),'\\n', y_test.value_counts()) Training samples: 4311 IS_FRAUDSTER 0 4128 1 183 Name: count, dtype: int64 Testing samples: 2875 IS_FRAUDSTER 0 2761 1 114 Name: count, dtype: int64 Training samples: 4311 IS_FRAUDSTER 0 4128 1 183 Name: count, dtype: int64 Testing samples: 2875 IS_FRAUDSTER 0 2761 1 114 Name: count, dtype: int64 Modeling When doing the modeling, I began with Random Forest Regression, Logistic Regression, the Decision Tree Classifier, a KNN classifier, and a stacked ensemble combining a few of these together. Each model worked fairly well, however the best performer was the KNN classifier. The stacked ensemble performed similarily well, however it didn't improve on teh KNN classifier so to minimize computational efforts I decided to limit modeling to KNN classification. This is all assuming that we want to focus on precision to minimize false positives in our target, for the best customer experience. # Define the parameter grid to search param_grid = { 'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'leaf_size': [10, 30, 50] } # Initialize the k-NN classifier knn_classifier = KNeighborsClassifier() # Initialize GridSearchCV with k-NN classifier and parameter grid grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy') # Perform GridSearchCV to find the best combination of hyperparameters grid_search.fit(X_train, y_train) # Get the best k-NN classifier model best_knn_classifier = grid_search.best_estimator_ # Use the best model to make predictions train_predictions = best_knn_classifier.predict(X_train) test_predictions = best_knn_classifier.predict(X_test) # Accuracy scores train_accuracy = accuracy_score(y_train, train_predictions) test_accuracy = accuracy_score(y_test, test_predictions) train_precision = round(precision_score(y_train, train_predictions), 3) test_precision = round(precision_score(y_test, test_predictions), 3) print('Training Scores:') print(' Accuracy: %.3f' % train_accuracy) print(' Precision: %.3f' % train_precision) print('\\nTesting Scores:') print(' Accuracy: %.3f' % test_accuracy) print(' Precision: %.3f' % test_precision) Training Scores: Accuracy: 0.960 Precision: 0.647 Testing Scores: Accuracy: 0.959 Precision: 0.381 Training Scores: Accuracy: 0.960 Precision: 0.647 Testing Scores: Accuracy: 0.959 Precision: 0.381 I want to also compare this to the null model as that will determine the overall benefit of this model. # compare to null model target_mean = y.mean() max_target_mean = np.max([y.mean(), 1-y.mean()]) # establish accuracy of null model print('Proportion of 0\\'s (Not Fraud Acct): %.3f' % (1-target_mean)) print('Proportion of 1\\'s (Fraud Acct): %.3f' % target_mean) print('Null model accuracy: %.3f' % max_target_mean) print('Null model precision: %.3f' % target_mean) Proportion of 0's (Not Fraud Acct): 0.959 Proportion of 1's (Fraud Acct): 0.041 Null model accuracy: 0.959 Null model precision: 0.041 Proportion of 0's (Not Fraud Acct): 0.959 Proportion of 1's (Fraud Acct): 0.041 Null model accuracy: 0.959 Null model precision: 0.041 Conclusion While the overall accuracy of the null model is comporable to that of the KNN Classifier, the KNN Classifier has much better precision. This would ensure we reduce the occurance of false positives, thereby supporting the customer experience. However, there is a noted disadvantage to using this classifier as it is not as transparent in decision making as, say, a decision tree classifier would be. As such, while it would be beneficial for implementation, it may not support business needs if there's a desire to develop a better understanding of the factors that help us identify fraudulent accounts.","title":"Fraudulent Account Detection"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#fraudulent-account-detection","text":"The file users.csv contains data on a subset of fictional banking users. The abbreviation 'KYC' stands for 'Know Your Customer' - a process of identifying and verifying the client's identity when opening an account and periodically over time. The variable IS_FRAUDSTER from this dataset is your target variable. The file transactions.csv contains details of fictional transactions of these users. Project and data procured from collected from StrataScratch . # import packages import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, precision_score from sklearn.preprocessing import MinMaxScaler from sklearn import metrics, preprocessing from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.neighbors import KNeighborsClassifier from datetime import datetime #date time now = datetime.now() print('Analysis on', now.strftime('%Y-%m-%d'), 'at', now.strftime('%H:%M')) Analysis on 2024-07-27 at 09:09 Analysis on 2024-07-27 at 09:09","title":"Fraudulent Account Detection"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#data-exploration","text":"As shown through our initial exploration, below, we are starting with two dataframes derived from provided CSV files. I'm also including a third dataframe which provides details on currencies vs. country of origin. This was generated from the UPS Country/Territory and Currency Codes document . By doing so, we will have three starting dataframes: Dataframe Source Description transactions transactions.csv Each row represents an individual transation with various details, including user, currency type, amount, and other relevant information. users users.csv Each row represents a single user with various details related to the user's account. included is the 'is_fraudster' identifier, which is our target. ccy ups_currencies.csv Each row represents a single currency type used in different countries The goal of this section and the next is to understand the relevant features in each dataframe, and figure out a way to combine the relevant details into a singular resource for analysis. # load data transactions = pd.read_csv('data/FRAUD-transactions.csv') users = pd.read_csv('data/FRAUD-users.csv') ccy = pd.read_csv('data/FRAUD-ups_currencies.csv') # set names for dataframes for reference transactions.name = 'Transactions' users.name = 'Users' ccy.name = 'CCY' # create a list of dataframes for iteration dataframes = [transactions, users, ccy] # get info for each dataframe for df in dataframes: print(df.info(),'\\n') <class 'pandas.core.frame.DataFrame'> RangeIndex: 688651 entries, 0 to 688650 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 CURRENCY 688651 non-null object 1 AMOUNT 688651 non-null int64 2 STATE 688651 non-null object 3 CREATED_DATE 688651 non-null object 4 MERCHANT_CATEGORY 223065 non-null object 5 MERCHANT_COUNTRY 483055 non-null object 6 ENTRY_METHOD 688651 non-null object 7 USER_ID 688651 non-null object 8 TYPE 688651 non-null object 9 SOURCE 688651 non-null object 10 ID 688651 non-null object 11 AMOUNT_USD 635328 non-null float64 dtypes: float64(1), int64(1), object(10) memory usage: 63.0+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 9944 entries, 0 to 9943 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 9944 non-null object 1 HAS_EMAIL 9944 non-null int64 2 PHONE_COUNTRY 9944 non-null object 3 IS_FRAUDSTER 9944 non-null bool 4 TERMS_VERSION 8417 non-null object 5 CREATED_DATE 9944 non-null object 6 STATE 9944 non-null object 7 COUNTRY 9944 non-null object 8 BIRTH_YEAR 9944 non-null int64 9 KYC 9944 non-null object 10 FAILED_SIGN_IN_ATTEMPTS 9944 non-null int64 dtypes: bool(1), int64(3), object(7) memory usage: 786.7+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 250 entries, 0 to 249 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 COUNTRY 250 non-null object 1 UPS_Code 249 non-null object 2 IATA_Code 249 non-null object 3 CCY 250 non-null object dtypes: object(4) memory usage: 7.9+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 688651 entries, 0 to 688650 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 CURRENCY 688651 non-null object 1 AMOUNT 688651 non-null int64 2 STATE 688651 non-null object 3 CREATED_DATE 688651 non-null object 4 MERCHANT_CATEGORY 223065 non-null object 5 MERCHANT_COUNTRY 483055 non-null object 6 ENTRY_METHOD 688651 non-null object 7 USER_ID 688651 non-null object 8 TYPE 688651 non-null object 9 SOURCE 688651 non-null object 10 ID 688651 non-null object 11 AMOUNT_USD 635328 non-null float64 dtypes: float64(1), int64(1), object(10) memory usage: 63.0+ MB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 9944 entries, 0 to 9943 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 9944 non-null object 1 HAS_EMAIL 9944 non-null int64 2 PHONE_COUNTRY 9944 non-null object 3 IS_FRAUDSTER 9944 non-null bool 4 TERMS_VERSION 8417 non-null object 5 CREATED_DATE 9944 non-null object 6 STATE 9944 non-null object 7 COUNTRY 9944 non-null object 8 BIRTH_YEAR 9944 non-null int64 9 KYC 9944 non-null object 10 FAILED_SIGN_IN_ATTEMPTS 9944 non-null int64 dtypes: bool(1), int64(3), object(7) memory usage: 786.7+ KB None <class 'pandas.core.frame.DataFrame'> RangeIndex: 250 entries, 0 to 249 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 COUNTRY 250 non-null object 1 UPS_Code 249 non-null object 2 IATA_Code 249 non-null object 3 CCY 250 non-null object dtypes: object(4) memory usage: 7.9+ KB None # function to get unique and null values for each column in a dataframe def matrix(dataframe): temp = [[dataframe[column].nunique() for column in dataframe.columns], [dataframe[column].isnull().sum() for column in dataframe.columns]] print(dataframe.name,'has',len(dataframe),'total values; column details:\\n') print(pd.DataFrame(temp, columns=dataframe.columns, index=['Unique Values', 'Null Values']).transpose()) # iterate through dataframes to get unique and null values for each column for df in dataframes: print(matrix(df),'\\n--------------------------\\n') Transactions has 688651 total values; column details: Unique Values Null Values CURRENCY 29 0 AMOUNT 32257 0 STATE 7 0 CREATED_DATE 688640 0 MERCHANT_CATEGORY 115 465586 MERCHANT_COUNTRY 344 205596 ENTRY_METHOD 6 0 USER_ID 8021 0 TYPE 5 0 SOURCE 11 0 ID 688651 0 AMOUNT_USD 37402 53323 None -------------------------- Users has 9944 total values; column details: Unique Values Null Values ID 9944 0 HAS_EMAIL 2 0 PHONE_COUNTRY 83 0 IS_FRAUDSTER 2 0 TERMS_VERSION 7 1527 CREATED_DATE 9944 0 STATE 2 0 COUNTRY 46 0 BIRTH_YEAR 69 0 KYC 4 0 FAILED_SIGN_IN_ATTEMPTS 5 0 None -------------------------- CCY has 250 total values; column details: Unique Values Null Values COUNTRY 250 0 UPS_Code 249 1 IATA_Code 224 1 CCY 139 0 None -------------------------- Transactions has 688651 total values; column details: Unique Values Null Values CURRENCY 29 0 AMOUNT 32257 0 STATE 7 0 CREATED_DATE 688640 0 MERCHANT_CATEGORY 115 465586 MERCHANT_COUNTRY 344 205596 ENTRY_METHOD 6 0 USER_ID 8021 0 TYPE 5 0 SOURCE 11 0 ID 688651 0 AMOUNT_USD 37402 53323 None -------------------------- Users has 9944 total values; column details: Unique Values Null Values ID 9944 0 HAS_EMAIL 2 0 PHONE_COUNTRY 83 0 IS_FRAUDSTER 2 0 TERMS_VERSION 7 1527 CREATED_DATE 9944 0 STATE 2 0 COUNTRY 46 0 BIRTH_YEAR 69 0 KYC 4 0 FAILED_SIGN_IN_ATTEMPTS 5 0 None -------------------------- CCY has 250 total values; column details: Unique Values Null Values COUNTRY 250 0 UPS_Code 249 1 IATA_Code 224 1 CCY 139 0 None -------------------------- transactions.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 CURRENCY GBP PLN GBP AMOUNT 4420 1500 191 STATE COMPLETED COMPLETED COMPLETED CREATED_DATE 2017-12-10 16:38:55.577 2017-12-10 16:37:24.792 2017-12-10 16:37:16.234 MERCHANT_CATEGORY NaN point_of_interest airport MERCHANT_COUNTRY NLD POL PRT ENTRY_METHOD chip manu chip USER_ID 3ff52b92-d416-4e22-8cad-018f500d4bbc 76cbaad3-4721-4a3b-92b9-3eb9e9319565 7bcaa34e-b889-4582-9c29-0b3bab34fb8c TYPE ATM CARD_PAYMENT CARD_PAYMENT SOURCE GAIA GAIA GAIA ID 367bf5f9-7cce-4683-90b9-d3c011bf4c87 ff6802b9-360d-4efe-b09b-f99c6cac3383 ddb4a930-7d8a-4f38-9079-ddc4b0db980e AMOUNT_USD 3268.0 NaN 141.0 users.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb HAS_EMAIL 1 1 1 PHONE_COUNTRY GB||JE||IM||GG GB||JE||IM||GG ES IS_FRAUDSTER False False False TERMS_VERSION 2018-05-25 2018-01-01 2018-09-20 CREATED_DATE 2017-08-06 07:33:33.341000 2017-03-07 10:18:59.427000 2018-05-31 04:41:24.672000 STATE ACTIVE ACTIVE ACTIVE COUNTRY GB GB ES BIRTH_YEAR 1971 1982 1973 KYC PASSED PASSED PASSED FAILED_SIGN_IN_ATTEMPTS 0 0 0 ccy.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 COUNTRY Afghanistan Aland Islands Albania UPS_Code AF AX AL IATA_Code AF AX AL CCY USD EUR EUR Before getting too much further, I'm going to check the correlation between the users's state, number of failed sign-in attempts, and KYC status as compared to if the user is a fraudster. The goal is to identify if there is a 1:1 ratio, in which case the variable would be meaningless. As shown below, there is a 1:1 correlation for if the account is locked, so we won't use that variable. However, the other two (KYC and failed sign in attempts) are still potential options. # check for connection between STATE and IS_FRAUDSTER fraudsters = users[users['IS_FRAUDSTER'] == True] not_fraudsters = users[users['IS_FRAUDSTER'] == False] print('Account states for fraud accounts:',fraudsters['STATE'].unique(), '\\nAccount states for non-fraud accounts:', not_fraudsters['STATE'].unique(), '\\n\\nKYC for fraud accounts:',fraudsters['KYC'].unique(), '\\nKYC for non-fraud accounts:', not_fraudsters['KYC'].unique(), '\\n\\nFailed Sign In for fraud accounts:',fraudsters['FAILED_SIGN_IN_ATTEMPTS'].unique(), '\\nFailed Sign In for non-fraud accounts:', not_fraudsters['FAILED_SIGN_IN_ATTEMPTS'].unique()) Account states for fraud accounts: ['LOCKED'] Account states for non-fraud accounts: ['ACTIVE'] KYC for fraud accounts: ['NONE' 'PASSED' 'PENDING' 'FAILED'] KYC for non-fraud accounts: ['PASSED' 'NONE' 'FAILED' 'PENDING'] Failed Sign In for fraud accounts: [0 1 2] Failed Sign In for non-fraud accounts: [0 1 3 2 6] Account states for fraud accounts: ['LOCKED'] Account states for non-fraud accounts: ['ACTIVE'] KYC for fraud accounts: ['NONE' 'PASSED' 'PENDING' 'FAILED'] KYC for non-fraud accounts: ['PASSED' 'NONE' 'FAILED' 'PENDING'] Failed Sign In for fraud accounts: [0 1 2] Failed Sign In for non-fraud accounts: [0 1 3 2 6] From here we'll begin feature engineering using the three remaining dataframes, and continue unifying all relevant data into a single dataframe.","title":"Data Exploration"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#feature-engineering","text":"For the analysis, I'm proposing the following initial features as potential predictors: | Column| Type | Description| |------------- |------------- |-------------| | IS_FRAUDSTER | bool | user IDed as fraudster | | HAS_EMAIL | bool | user has registered email | | FAILED_SIGN_IN_ATTEMPTS | cont | count of failed sign in attempts | | KYC_[STATUS] | dummy/bool | user's stage in identification verification | | PHONE_COUNTRY_MATCH | bool | user's listed country matches their phone's country code | | PHONE_MULTIPLE_COUNTRIES | bool | user has phone numbers in multiple countries | | RECENT_TERMS | bool | user has accepted the most recent TOC| | CREATED_TO_FIRST | cont | minutes between account creation to first transaction| | TOTAL_TRANSACTION_COUNT | int | count of user's total transactions | | TOTAL_TRANSACTION_SUM | cont | sum of user's total transaction amounts | | MEDIAN_TRANSACTION_AMOUNT | cont | median transaction amount | | AVG_DAILY_TRANSACTION_COUNT | cont | average transactions made per day by user | | INTL_TRANSACTION_PCT | cont | percent of user's transactions that don't match their listed country's currency | | TYPE_[TYPE] | cont | percent of transactions made of each transaction type | | METHOD_[METHOD] | cont | percent of transactions attributed to different entry methods | Note: Each sample accounts for a single user. Additionally, all these features will not be used in the final analysis. We are simply generating them as a starting point to identify the most relevant features. This first section takes care of the low-hanging fruit: * Updating data types and renaming columns as needed * Creating the analysis dataframe with features that are already ready for use * Generating new features according to the following: * If the user's country and the country code for their phone match * If the user has more than one phone country listed * If the user has accepted the most recent terms of service # update date types to datetime for calculating users['TERMS_VERSION'] = pd.to_datetime(users['TERMS_VERSION']) users['CREATED_DATE'] = pd.to_datetime(users['CREATED_DATE']) transactions['CREATED_DATE'] = pd.to_datetime(transactions['CREATED_DATE'], format='mixed') # begin dataframe with columns that don't need modification df = pd.DataFrame(users.loc[:,['ID', 'IS_FRAUDSTER', 'HAS_EMAIL', 'KYC', 'FAILED_SIGN_IN_ATTEMPTS']]) # rename ID to simplify later merging df.rename(columns={'ID':'USER_ID'}, inplace=True) # update data types & create dummy variables df['HAS_EMAIL'] = df['HAS_EMAIL'].astype(bool) df = pd.get_dummies(df, columns=['KYC'], prefix='KYC', drop_first=False) # get details for columns that need simple adjustments df['PHONE_COUNTRY_MATCH'] = users['COUNTRY'] == users['PHONE_COUNTRY'] df['PHONE_MULTIPLE_COUNTRIES'] = users['PHONE_COUNTRY'].str.len() > 2 df['RECENT_TERMS'] = users['TERMS_VERSION'] == users['TERMS_VERSION'].max() df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True This next section deals with slightly more complex, yet still straightforward, feature engineering, either used directly or to assist with other feature engineering: * Calculating the time from account creation to the first transaction * Getting the total number of transactions for each user * Getting the total sum (in USD) of each user's transactions (note: we're using USD to avoid unit discrepencies by using multiple currencies) * Identifying the median transaction amount for the user (note: we're avoiding the mean transaction amount since that can be determined by the sum and count of transactions, and thereby will be overly influential on the analysis) * Getting the average number of transactions made per day by the user # created to first transaction create_to_first = transactions.groupby('USER_ID')['CREATED_DATE'].min().reset_index() create_to_first['USER_CREATED'] = create_to_first['USER_ID'].map(users.set_index('ID')['CREATED_DATE']) create_to_first['CREATED_TO_FIRST'] = \\ ((create_to_first['CREATED_DATE'] - create_to_first['USER_CREATED']).dt.total_seconds())/60 create_to_first = create_to_first.drop(columns=['CREATED_DATE', 'USER_CREATED']) df = pd.merge(df, create_to_first, on='USER_ID', how='left') # total transaction count total_transaction_count = transactions.groupby('USER_ID')['ID'].count().reset_index() total_transaction_count.rename(columns={'ID': 'TOTAL_TRANSACTION_COUNT'}, inplace=True) df = pd.merge(df, total_transaction_count, on='USER_ID', how='left') # sum of transaction amounts total_transaction__sum = transactions.groupby('USER_ID')['AMOUNT_USD'].sum().reset_index() total_transaction__sum.rename(columns={'AMOUNT_USD': 'TOTAL_TRANSACTION_SUM'}, inplace=True) df = pd.merge(df, total_transaction__sum, on='USER_ID', how='left') # median transaction amount median_transaction_amount = transactions.groupby('USER_ID')['AMOUNT_USD'].median().reset_index() median_transaction_amount.rename(columns={'AMOUNT_USD': 'MEDIAN_TRANSACTION_AMOUNT'}, inplace=True) df = pd.merge(df, median_transaction_amount, on='USER_ID', how='left') # avg daily transaction count avg_daily_transaction_count = transactions.groupby(['USER_ID', 'CREATED_DATE'])['ID'].count().reset_index() avg_daily_transaction_count = avg_daily_transaction_count.groupby('USER_ID')['ID'].mean().reset_index() avg_daily_transaction_count.rename(columns={'ID': 'AVG_DAILY_TRANSACTION_COUNT'}, inplace=True) df = pd.merge(df, avg_daily_transaction_count, on='USER_ID', how='left') # review results of all the merges df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 Now we move into the more in-depth feature engineering, starting with identifying the percent of international transactions per user. To do so, we first need to get counts of the purchases made per user, divided by unique currency codes. # group transactions by user and currency international_transactions = transactions.groupby(['USER_ID', 'CURRENCY'])['ID'].nunique().reset_index() international_transactions.rename(columns={'ID': 'TRANSACTION_COUNT', 'CURRENCY': 'CCY'}, inplace=True) # merge with relevant columns from users dataframe international_transactions = pd.merge(international_transactions, users[['ID', 'COUNTRY']], left_on='USER_ID', right_on='ID', how='left') international_transactions.rename(columns={'COUNTRY': 'USER_COUNTRY'}, inplace=True) # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 581 USER_COUNTRY 581 dtype: int64 # get unique user_ids for grouped transactions with null values no_id = international_transactions[international_transactions['ID'].isnull()] no_id = no_id['USER_ID'].unique() # compare to users dataframe users[users['ID'].isin(no_id)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID HAS_EMAIL PHONE_COUNTRY IS_FRAUDSTER TERMS_VERSION CREATED_DATE STATE COUNTRY BIRTH_YEAR KYC FAILED_SIGN_IN_ATTEMPTS Based on the above information, there are 581 grouped transactions with listed User IDs that don't match the provided list of users. As such, we would not have identifiers for if these are fraudulent accounts, and so we're dropping those rows. # drop null values; transactions without registered user information international_transactions.dropna(inplace=True) # merge with ccy data for country information, and update column name international_transactions = pd.merge(international_transactions, ccy[['CCY', 'UPS_Code']], on='CCY', how='left') international_transactions.rename(columns={'UPS_Code': 'CCY_COUNTRY'}, inplace=True) # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 0 USER_COUNTRY 0 CCY_COUNTRY 341 dtype: int64 # identify CCY for which CCY_COUNTRY is null ccy_fix = list(international_transactions[international_transactions['CCY_COUNTRY'].isnull()]['CCY'].unique()) ccy_fix ['RON', 'BTC', 'ETH', 'LTC', 'XRP'] A quick Google Search identified the RON as being the Romanian new leu, and the other four as being cryptocurrency. # update country codes for cryptocurrencies crypto = ['BTC', 'ETH', 'LTC', 'XRP'] international_transactions.loc[international_transactions['CCY'].isin(crypto), 'CCY_COUNTRY'] = 'Crypto' # update country codes for RON international_transactions.loc[international_transactions['CCY'] == 'RON', 'CCY_COUNTRY'] = 'RO' # check for null values international_transactions.isnull().sum() USER_ID 0 CCY 0 TRANSACTION_COUNT 0 ID 0 USER_COUNTRY 0 CCY_COUNTRY 0 dtype: int64 Now that the transactions are grouped and all needed information is connected, we can figure out the percent of international transactions per user. # check for matching country codes, group international_transactions['MATCHES_USER_COUNTRY'] = \\ international_transactions['USER_COUNTRY'] == international_transactions['CCY_COUNTRY'] # group by user and country match bool international_transactions = \\ international_transactions.groupby(['USER_ID', 'MATCHES_USER_COUNTRY'])['ID'].count().reset_index() international_transactions.rename(columns={'ID': 'INTL_TRANSACTIONS'}, inplace=True) # limit to international transactions international_transactions = \\ international_transactions[international_transactions['MATCHES_USER_COUNTRY'] != True] # integrate with total transaction count international_transactions = \\ pd.merge(international_transactions, total_transaction_count, on='USER_ID', how='left') # calculate international transaction percent international_transactions['INTL_TRANSACTION_PCT'] = \\ international_transactions['INTL_TRANSACTIONS'] / international_transactions['TOTAL_TRANSACTION_COUNT'] international_transactions = \\ international_transactions[['USER_ID', 'INTL_TRANSACTION_PCT']] # merge with main dataframe df = pd.merge(df, international_transactions, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 This next section deals with getting the percent of transactions per user of each transaction type. # group by user and transaction type transaction_types = transactions.groupby(['USER_ID', 'TYPE'])['ID'].count().reset_index() # pivot table to get transaction type counts transaction_types = transaction_types.pivot(index='USER_ID', columns='TYPE', values='ID') transaction_types.columns = [f'TYPE_{col}' for col in transaction_types.columns] transaction_types.fillna(0, inplace=True) # in case no transactions of a certain type # calculate the sum of all types for each user row_totals = transaction_types.sum(axis=1) # calculate the percentage for each type per user transaction_types = transaction_types.div(row_totals, axis=0).reset_index() # merge with main dataframe df = pd.merge(df, transaction_types, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.0 0.0 TYPE_BANK_TRANSFER 0.0 0.0 0.0 TYPE_CARD_PAYMENT 0.333333 0.454545 0.5 TYPE_P2P 0.0 0.181818 0.383333 TYPE_TOPUP 0.5 0.363636 0.116667 Similar to how we generated the transaction type values, we're doing the same thing with entry methods # group by user and entry method transaction_method = transactions.groupby(['USER_ID', 'ENTRY_METHOD'])['ID'].count().reset_index() # pivot table to get transaction method counts transaction_method = transaction_method.pivot(index='USER_ID', columns='ENTRY_METHOD', values='ID') transaction_method.columns = [f'METHOD_{col}' for col in transaction_method.columns] transaction_method.fillna(0, inplace=True) # in case no transactions of a certain entry method # calculate the sum of all types for each user row_totals = transaction_method.sum(axis=1) # calculate the percentage for each type per user transaction_method = transaction_method.div(row_totals, axis=0).reset_index() # merge with main dataframe df = pd.merge(df, transaction_method, on='USER_ID', how='left') df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 USER_ID 1872820f-e3ac-4c02-bdc7-727897b60043 545ff94d-66f8-4bea-b398-84425fb2301e 10376f1a-a28a-4885-8daa-c8ca496026bb IS_FRAUDSTER False False False HAS_EMAIL True True True FAILED_SIGN_IN_ATTEMPTS 0 0 0 KYC_FAILED False False False KYC_NONE False False False KYC_PASSED True True True KYC_PENDING False False False PHONE_COUNTRY_MATCH False False True PHONE_MULTIPLE_COUNTRIES True True False RECENT_TERMS False False True CREATED_TO_FIRST 8923.9711 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.0 11.0 60.0 TOTAL_TRANSACTION_SUM 61876.0 43922.0 114328.0 MEDIAN_TRANSACTION_AMOUNT 1458.0 750.0 156.0 AVG_DAILY_TRANSACTION_COUNT 1.0 1.0 1.0 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.0 0.0 TYPE_BANK_TRANSFER 0.0 0.0 0.0 TYPE_CARD_PAYMENT 0.333333 0.454545 0.5 TYPE_P2P 0.0 0.181818 0.383333 TYPE_TOPUP 0.5 0.363636 0.116667 METHOD_chip 0.333333 0.181818 0.15 METHOD_cont 0.166667 0.272727 0.033333 METHOD_mags 0.0 0.0 0.016667 METHOD_manu 0.0 0.0 0.3 METHOD_mcon 0.0 0.0 0.0 METHOD_misc 0.5 0.545455 0.5 The last steps in this portion are to: * Remove USER_ID, which up to now was used for merging purposes but is now superfluous. We are also replacing na with 0, as those would have beeen generated from a lack of values to calculate. * Convert bool values to int to ensure we can calculate appropriately # identify bool columns bool_columns = df.select_dtypes(include=bool).columns # convert bool columns to integers df[bool_columns] = df[bool_columns].astype(int) # clean up dataframe for next steps df.drop('USER_ID', axis=1, inplace=True) df = df.dropna(how='any', axis=0) # drop rows with inf values df.fillna(0, inplace=True) df.isna().sum() IS_FRAUDSTER 0 HAS_EMAIL 0 FAILED_SIGN_IN_ATTEMPTS 0 KYC_FAILED 0 KYC_NONE 0 KYC_PASSED 0 KYC_PENDING 0 PHONE_COUNTRY_MATCH 0 PHONE_MULTIPLE_COUNTRIES 0 RECENT_TERMS 0 CREATED_TO_FIRST 0 TOTAL_TRANSACTION_COUNT 0 TOTAL_TRANSACTION_SUM 0 MEDIAN_TRANSACTION_AMOUNT 0 AVG_DAILY_TRANSACTION_COUNT 0 INTL_TRANSACTION_PCT 0 TYPE_ATM 0 TYPE_BANK_TRANSFER 0 TYPE_CARD_PAYMENT 0 TYPE_P2P 0 TYPE_TOPUP 0 METHOD_chip 0 METHOD_cont 0 METHOD_mags 0 METHOD_manu 0 METHOD_mcon 0 METHOD_misc 0 dtype: int64 # check head as a final precaution df.head(3).transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 IS_FRAUDSTER 0.000000 0.000000 0.000000 HAS_EMAIL 1.000000 1.000000 1.000000 FAILED_SIGN_IN_ATTEMPTS 0.000000 0.000000 0.000000 KYC_FAILED 0.000000 0.000000 0.000000 KYC_NONE 0.000000 0.000000 0.000000 KYC_PASSED 1.000000 1.000000 1.000000 KYC_PENDING 0.000000 0.000000 0.000000 PHONE_COUNTRY_MATCH 0.000000 0.000000 1.000000 PHONE_MULTIPLE_COUNTRIES 1.000000 1.000000 0.000000 RECENT_TERMS 0.000000 0.000000 1.000000 CREATED_TO_FIRST 8923.971100 1.894117 15479.922883 TOTAL_TRANSACTION_COUNT 6.000000 11.000000 60.000000 TOTAL_TRANSACTION_SUM 61876.000000 43922.000000 114328.000000 MEDIAN_TRANSACTION_AMOUNT 1458.000000 750.000000 156.000000 AVG_DAILY_TRANSACTION_COUNT 1.000000 1.000000 1.000000 INTL_TRANSACTION_PCT 8.333333 0.545455 0.716667 TYPE_ATM 0.166667 0.000000 0.000000 TYPE_BANK_TRANSFER 0.000000 0.000000 0.000000 TYPE_CARD_PAYMENT 0.333333 0.454545 0.500000 TYPE_P2P 0.000000 0.181818 0.383333 TYPE_TOPUP 0.500000 0.363636 0.116667 METHOD_chip 0.333333 0.181818 0.150000 METHOD_cont 0.166667 0.272727 0.033333 METHOD_mags 0.000000 0.000000 0.016667 METHOD_manu 0.000000 0.000000 0.300000 METHOD_mcon 0.000000 0.000000 0.000000 METHOD_misc 0.500000 0.545455 0.500000","title":"Feature Engineering"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#feature-selection","text":"# establish features and target all_features = list(df.columns[1:]) target = df.columns[0] # set X and y y = df[target] X = df[all_features] # get variance inflation factor for each feature vif = pd.DataFrame() vif['Predictor'] = X.columns vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] cr = df.corr()[target].round(3) vif['Relevance'] = [cr.iloc[i] for i in range(X.shape[1])] round(vif, 2).sort_values(by='Relevance', ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictor VIF Relevance 0 HAS_EMAIL 1.40 1.00 17 TYPE_CARD_PAYMENT inf 0.17 16 TYPE_BANK_TRANSFER inf 0.16 8 RECENT_TERMS 1.13 0.16 6 PHONE_COUNTRY_MATCH 8.35 0.11 13 AVG_DAILY_TRANSACTION_COUNT 1.00 0.11 3 KYC_NONE inf 0.05 24 METHOD_mcon inf 0.05 1 FAILED_SIGN_IN_ATTEMPTS 1.01 0.04 14 INTL_TRANSACTION_PCT 1.67 0.03 12 MEDIAN_TRANSACTION_AMOUNT 1.08 0.02 2 KYC_FAILED inf 0.01 20 METHOD_chip inf 0.00 5 KYC_PENDING inf -0.00 10 TOTAL_TRANSACTION_COUNT 1.33 -0.01 21 METHOD_cont inf -0.01 25 METHOD_misc inf -0.02 11 TOTAL_TRANSACTION_SUM 1.06 -0.05 19 TYPE_TOPUP inf -0.05 4 KYC_PASSED inf -0.06 23 METHOD_manu inf -0.07 15 TYPE_ATM inf -0.08 22 METHOD_mags inf -0.08 18 TYPE_P2P inf -0.10 9 CREATED_TO_FIRST 1.02 -0.12 7 PHONE_MULTIPLE_COUNTRIES 8.38 -0.15 #multivariate feature selection estimator = DecisionTreeClassifier() selector = RFE(estimator, n_features_to_select=10, step=1).fit(X,y) rnk = pd.DataFrame() rnk['Feature'] = X.columns rnk['Rank']= selector.ranking_ rnk.sort_values('Rank') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feature Rank 12 MEDIAN_TRANSACTION_AMOUNT 1 23 METHOD_manu 1 20 METHOD_chip 1 19 TYPE_TOPUP 1 16 TYPE_BANK_TRANSFER 1 15 TYPE_ATM 1 9 CREATED_TO_FIRST 1 10 TOTAL_TRANSACTION_COUNT 1 11 TOTAL_TRANSACTION_SUM 1 14 INTL_TRANSACTION_PCT 1 17 TYPE_CARD_PAYMENT 2 21 METHOD_cont 3 2 KYC_FAILED 4 25 METHOD_misc 5 18 TYPE_P2P 6 5 KYC_PENDING 7 6 PHONE_COUNTRY_MATCH 8 7 PHONE_MULTIPLE_COUNTRIES 9 8 RECENT_TERMS 10 3 KYC_NONE 11 1 FAILED_SIGN_IN_ATTEMPTS 12 24 METHOD_mcon 13 4 KYC_PASSED 14 13 AVG_DAILY_TRANSACTION_COUNT 15 0 HAS_EMAIL 16 22 METHOD_mags 17 For this next section, I manually adjusted the number of features until we removed all the extremely high VIF scores, to maintain the maximum number of possible variables prior to hyperparameter tuning. # isolate top features selected_features = [feature for feature in all_features if feature in list(rnk.sort_values('Rank').head(8)['Feature'])] # reset X X = df[selected_features] # rerun vf vif = pd.DataFrame() vif['Predictor'] = X.columns vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] cr = df.corr()[target].round(3) vif['Relevance'] = [cr.iloc[i] for i in range(X.shape[1])] round(vif, 2).sort_values(by='Relevance', ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictor VIF Relevance 0 CREATED_TO_FIRST 1.08 1.00 6 METHOD_chip 2.21 0.11 3 TYPE_ATM 1.84 0.05 1 TOTAL_TRANSACTION_COUNT 1.28 0.04 2 MEDIAN_TRANSACTION_AMOUNT 1.11 0.01 5 TYPE_TOPUP 1.27 -0.00 4 TYPE_BANK_TRANSFER 1.18 -0.06 7 METHOD_manu 1.13 -0.15 # establish correlation matrix for intercorrelation corr_matrix = X.corr().abs() # absolute values to make visualizing easier # visualize a simple correlation matrix plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.2f') plt.title('Feature Correlation Matrix') plt.show() # review correlation between target and features df[selected_features + [target]].corr()[target].sort_values().round(2) TOTAL_TRANSACTION_COUNT -0.05 METHOD_chip -0.01 CREATED_TO_FIRST -0.01 TYPE_TOPUP 0.00 METHOD_manu 0.05 MEDIAN_TRANSACTION_AMOUNT 0.11 TYPE_ATM 0.16 TYPE_BANK_TRANSFER 0.17 IS_FRAUDSTER 1.00 Name: IS_FRAUDSTER, dtype: float64 There are a few variables that appear to have intercorrelations: * The groupings of TYPE and METHOD of transaction: Since TYPE provides more specificity, and has higher correlations with the target, we'll maintain that for the analysis. * KYC_PASSED and KYC_NONE: We didn't drop the first dummy for KYC, to figure out which would be most useful. Since KYC_NONE has the stronger relationship with our target, we're going to remove KYC_PASSED. * PHONE_COUNTRY_MATCH and PHONE_MULTIPLE_COUNTRIES: These strongly correlate with each other, and also have decent correlations with the target. However, PHONE_MULTIPLE_COUNTRIES has a slightly stronger correlation, so we'll keep that variable. # update features to remove multicollinear features selected_features = [feature for feature in selected_features if feature not in ['METHOD_chip', 'METHOD_cont', 'METHOD_mags', 'METHOD_manu', 'METHOD_mcon', 'METHOD_misc', 'KYC_PASSED', 'PHONE_COUNTRY_MATCH']] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.4, random_state=17) # determine hyperparameters param_grid = { 'max_depth': list(range(5,20)), 'min_samples_leaf': [10], 'max_features': list(range(2,10)) } gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=17), param_grid, cv=5, n_jobs=-1) gridSearch.fit(X_train, y_train) print('Score: ', gridSearch.best_score_) best_params = gridSearch.best_params_ print('Parameters: ', best_params) Score: 0.9596387177949902 Parameters: {'max_depth': 13, 'max_features': 2, 'min_samples_leaf': 10} Score: 0.9596387177949902 Parameters: {'max_depth': 13, 'max_features': 2, 'min_samples_leaf': 10} # train on tree model using best parameters tree_model = DecisionTreeClassifier(random_state=17, max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'], max_features=best_params['max_features']) tree_model.fit(X_train, y_train) # get feature importances & list of features used feature_importances = tree_model.feature_importances_ # generate a list of features used features_used = X_train.columns[feature_importances > 0] list(features_used) ['CREATED_TO_FIRST', 'TOTAL_TRANSACTION_COUNT', 'MEDIAN_TRANSACTION_AMOUNT', 'TYPE_ATM', 'TYPE_BANK_TRANSFER', 'TYPE_TOPUP'] # isolate top features selected_features = [feature for feature in all_features if feature in list(features_used)] # reset X with top features X = df[selected_features] # resplit the data X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.4, random_state=17) # scale the data scaler = preprocessing.MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # show distribution print('Training samples:',len(X_train),'\\n', y_train.value_counts()) print('\\n') print('Testing samples:',len(X_test),'\\n', y_test.value_counts()) Training samples: 4311 IS_FRAUDSTER 0 4128 1 183 Name: count, dtype: int64 Testing samples: 2875 IS_FRAUDSTER 0 2761 1 114 Name: count, dtype: int64 Training samples: 4311 IS_FRAUDSTER 0 4128 1 183 Name: count, dtype: int64 Testing samples: 2875 IS_FRAUDSTER 0 2761 1 114 Name: count, dtype: int64","title":"Feature Selection"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#modeling","text":"When doing the modeling, I began with Random Forest Regression, Logistic Regression, the Decision Tree Classifier, a KNN classifier, and a stacked ensemble combining a few of these together. Each model worked fairly well, however the best performer was the KNN classifier. The stacked ensemble performed similarily well, however it didn't improve on teh KNN classifier so to minimize computational efforts I decided to limit modeling to KNN classification. This is all assuming that we want to focus on precision to minimize false positives in our target, for the best customer experience. # Define the parameter grid to search param_grid = { 'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'leaf_size': [10, 30, 50] } # Initialize the k-NN classifier knn_classifier = KNeighborsClassifier() # Initialize GridSearchCV with k-NN classifier and parameter grid grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy') # Perform GridSearchCV to find the best combination of hyperparameters grid_search.fit(X_train, y_train) # Get the best k-NN classifier model best_knn_classifier = grid_search.best_estimator_ # Use the best model to make predictions train_predictions = best_knn_classifier.predict(X_train) test_predictions = best_knn_classifier.predict(X_test) # Accuracy scores train_accuracy = accuracy_score(y_train, train_predictions) test_accuracy = accuracy_score(y_test, test_predictions) train_precision = round(precision_score(y_train, train_predictions), 3) test_precision = round(precision_score(y_test, test_predictions), 3) print('Training Scores:') print(' Accuracy: %.3f' % train_accuracy) print(' Precision: %.3f' % train_precision) print('\\nTesting Scores:') print(' Accuracy: %.3f' % test_accuracy) print(' Precision: %.3f' % test_precision) Training Scores: Accuracy: 0.960 Precision: 0.647 Testing Scores: Accuracy: 0.959 Precision: 0.381 Training Scores: Accuracy: 0.960 Precision: 0.647 Testing Scores: Accuracy: 0.959 Precision: 0.381 I want to also compare this to the null model as that will determine the overall benefit of this model. # compare to null model target_mean = y.mean() max_target_mean = np.max([y.mean(), 1-y.mean()]) # establish accuracy of null model print('Proportion of 0\\'s (Not Fraud Acct): %.3f' % (1-target_mean)) print('Proportion of 1\\'s (Fraud Acct): %.3f' % target_mean) print('Null model accuracy: %.3f' % max_target_mean) print('Null model precision: %.3f' % target_mean) Proportion of 0's (Not Fraud Acct): 0.959 Proportion of 1's (Fraud Acct): 0.041 Null model accuracy: 0.959 Null model precision: 0.041 Proportion of 0's (Not Fraud Acct): 0.959 Proportion of 1's (Fraud Acct): 0.041 Null model accuracy: 0.959 Null model precision: 0.041","title":"Modeling"},{"location":"Fraudulent_Acct_Detection/Fraudulent_Acct_Detection/#conclusion","text":"While the overall accuracy of the null model is comporable to that of the KNN Classifier, the KNN Classifier has much better precision. This would ensure we reduce the occurance of false positives, thereby supporting the customer experience. However, there is a noted disadvantage to using this classifier as it is not as transparent in decision making as, say, a decision tree classifier would be. As such, while it would be beneficial for implementation, it may not support business needs if there's a desire to develop a better understanding of the factors that help us identify fraudulent accounts.","title":"Conclusion"},{"location":"Game_AB_Testing/Game_AB_Testing/","text":"AB Game Testing This project is an analysis on the impact of player retention when a key component to an online game is altered. The available data When a player installed the game, they were randomly assigned to either gate_30 or gate_40, identifying at which level the first game gate is presented. Context The data we have is from 90,189 players that installed the game while the AB-test was running. The variables are: userid: A unique number that identifies each player. version: Whether the player was put in the control group (gate_30 - a gate at level 30) or the group with the moved gate (gate_40 - a gate at level 40). sum_gamerounds: the number of game rounds played by the player during the first 14 days after install. retention_1: Did the player come back and play 1 day after installing? retention_7: Did the player come back and play 7 days after installing? Data Exploration import pandas as pd import numpy as np from scipy.stats import ttest_ind import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px d = pd.read_csv('data/GAME-stats.csv') d.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userid version sum_gamerounds retention_1 retention_7 0 116 gate_30 3 False False 1 337 gate_30 38 True False 2 377 gate_40 165 True False 3 483 gate_40 1 False False 4 488 gate_40 179 True True d['sum_gamerounds'].describe() count 90189.000000 mean 51.872457 std 195.050858 min 0.000000 25% 5.000000 50% 16.000000 75% 51.000000 max 49854.000000 Name: sum_gamerounds, dtype: float64 d['sum_gamerounds'].nlargest(10) 57702 49854 7912 2961 29417 2640 43671 2438 48188 2294 46344 2251 87007 2156 36933 2124 88328 2063 6536 2015 Name: sum_gamerounds, dtype: int64 above_500 = d[d['sum_gamerounds'] > 500]['sum_gamerounds'].count() below_500 = d[d['sum_gamerounds'] < 500]['sum_gamerounds'].count() above_500, below_500 (856, 89329) Data Wrangling d.drop(d[d['sum_gamerounds'] > 500].index, inplace=True) d['sum_gamerounds'].describe() count 89333.000000 mean 44.740801 std 72.476563 min 0.000000 25% 5.000000 50% 16.000000 75% 49.000000 max 500.000000 Name: sum_gamerounds, dtype: float64 plt.figure(figsize=(5, 2)) sns.boxplot(x=d['sum_gamerounds'], color='darkorange').set( xlabel='Per-User Gamerounds Played', title='Boxplot of Summed Gamerounds') plt.show() Gate30_Day1 = d[d['version'] == 'gate_30']['retention_1'].tolist() Gate40_Day1 = d[d['version'] == 'gate_40']['retention_1'].tolist() Gate30_Day7 = d[d['version'] == 'gate_30']['retention_7'].tolist() Gate40_Day7 = d[d['version'] == 'gate_40']['retention_7'].tolist() Hypoithesis Testing # Performing the t-test t, p = ttest_ind(Gate30_Day1, Gate40_Day1) # Printing the result print('Statistical Test for Day 1 Retention') print(f\"t = {t:.3f}\") print(f\"p = {p:.3f}\") Statistical Test for Day 1 Retention t = 1.789 p = 0.074 Statistical Test for Day 1 Retention t = 1.789 p = 0.074 # Performing the t-test t, p = ttest_ind(Gate30_Day7, Gate40_Day7) # Printing the result print('Statistical Test for Day 7 Retention') print(f\"t = {t:.3f}\") print(f\"p = {p:.3f}\") Statistical Test for Day 7 Retention t = 3.246 p = 0.001 Statistical Test for Day 7 Retention t = 3.246 p = 0.001 Visualizing the Results Gate30 = d[d['version'] == 'gate_30'] Gate40 = d[d['version'] == 'gate_40'] Gate30['retention_1'].value_counts()[True] Gate30['retention_7'].value_counts()[True] Gate40['retention_1'].value_counts()[True] Gate40['retention_7'].value_counts()[True] stages = [\"Downloaded Game\", \"1 Day Retention\", \"7 Day Retention\"] g_30 = pd.DataFrame(dict(number=[len(Gate30), Gate30['retention_1'].value_counts()[True], Gate30['retention_7'].value_counts()[True]], stage=stages)) g_30['version'] = 'Gate 30' g_40 = pd.DataFrame(dict(number=[len(Gate40), Gate40['retention_1'].value_counts()[True], Gate40['retention_7'].value_counts()[True]], stage=stages)) g_40['version'] = 'Gate 40' df = pd.concat([g_30, g_40], axis=0) fig = px.funnel(df, x='number', y='stage', color='version', color_discrete_sequence=px.colors.qualitative.Safe, title='Funnel Chart of Game Retention by Version', labels={'number':'Number of Players','version':'Game Version','stage':'Retention Point'}) fig.update_layout(title_x=0.5, font=dict( size=14, ) ) fig.show() # further segment to bulk of players Gate30_7True = Gate30[(Gate30['retention_7'] == True) & (Gate30['sum_gamerounds'] <= 100)] Gate40_7True = Gate40[(Gate40['retention_7'] == True) & (Gate40['sum_gamerounds'] <= 100)] # Plotting the data plt.figure(figsize=(8, 4)) plt.hist(Gate30_7True['sum_gamerounds'], alpha=1, label='Gate 30', color='#88CCEE') plt.hist(Gate40_7True['sum_gamerounds'], alpha=0.5, label='Gate 40', color='#CC6677') plt.legend(loc='upper right') plt.title('Users Retained After 7 Days', fontsize=18, pad=10) plt.xlabel('Total Game Rounds Played', fontsize=14, labelpad=10) plt.ylabel('Number of Users', fontsize=14, labelpad=10) plt.show()","title":"AB Game Testing"},{"location":"Game_AB_Testing/Game_AB_Testing/#ab-game-testing","text":"This project is an analysis on the impact of player retention when a key component to an online game is altered. The available data When a player installed the game, they were randomly assigned to either gate_30 or gate_40, identifying at which level the first game gate is presented.","title":"AB Game Testing"},{"location":"Game_AB_Testing/Game_AB_Testing/#context","text":"The data we have is from 90,189 players that installed the game while the AB-test was running. The variables are: userid: A unique number that identifies each player. version: Whether the player was put in the control group (gate_30 - a gate at level 30) or the group with the moved gate (gate_40 - a gate at level 40). sum_gamerounds: the number of game rounds played by the player during the first 14 days after install. retention_1: Did the player come back and play 1 day after installing? retention_7: Did the player come back and play 7 days after installing?","title":"Context"},{"location":"Game_AB_Testing/Game_AB_Testing/#data-exploration","text":"import pandas as pd import numpy as np from scipy.stats import ttest_ind import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px d = pd.read_csv('data/GAME-stats.csv') d.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } userid version sum_gamerounds retention_1 retention_7 0 116 gate_30 3 False False 1 337 gate_30 38 True False 2 377 gate_40 165 True False 3 483 gate_40 1 False False 4 488 gate_40 179 True True d['sum_gamerounds'].describe() count 90189.000000 mean 51.872457 std 195.050858 min 0.000000 25% 5.000000 50% 16.000000 75% 51.000000 max 49854.000000 Name: sum_gamerounds, dtype: float64 d['sum_gamerounds'].nlargest(10) 57702 49854 7912 2961 29417 2640 43671 2438 48188 2294 46344 2251 87007 2156 36933 2124 88328 2063 6536 2015 Name: sum_gamerounds, dtype: int64 above_500 = d[d['sum_gamerounds'] > 500]['sum_gamerounds'].count() below_500 = d[d['sum_gamerounds'] < 500]['sum_gamerounds'].count() above_500, below_500 (856, 89329)","title":"Data Exploration"},{"location":"Game_AB_Testing/Game_AB_Testing/#data-wrangling","text":"d.drop(d[d['sum_gamerounds'] > 500].index, inplace=True) d['sum_gamerounds'].describe() count 89333.000000 mean 44.740801 std 72.476563 min 0.000000 25% 5.000000 50% 16.000000 75% 49.000000 max 500.000000 Name: sum_gamerounds, dtype: float64 plt.figure(figsize=(5, 2)) sns.boxplot(x=d['sum_gamerounds'], color='darkorange').set( xlabel='Per-User Gamerounds Played', title='Boxplot of Summed Gamerounds') plt.show() Gate30_Day1 = d[d['version'] == 'gate_30']['retention_1'].tolist() Gate40_Day1 = d[d['version'] == 'gate_40']['retention_1'].tolist() Gate30_Day7 = d[d['version'] == 'gate_30']['retention_7'].tolist() Gate40_Day7 = d[d['version'] == 'gate_40']['retention_7'].tolist()","title":"Data Wrangling"},{"location":"Game_AB_Testing/Game_AB_Testing/#hypoithesis-testing","text":"# Performing the t-test t, p = ttest_ind(Gate30_Day1, Gate40_Day1) # Printing the result print('Statistical Test for Day 1 Retention') print(f\"t = {t:.3f}\") print(f\"p = {p:.3f}\") Statistical Test for Day 1 Retention t = 1.789 p = 0.074 Statistical Test for Day 1 Retention t = 1.789 p = 0.074 # Performing the t-test t, p = ttest_ind(Gate30_Day7, Gate40_Day7) # Printing the result print('Statistical Test for Day 7 Retention') print(f\"t = {t:.3f}\") print(f\"p = {p:.3f}\") Statistical Test for Day 7 Retention t = 3.246 p = 0.001 Statistical Test for Day 7 Retention t = 3.246 p = 0.001","title":"Hypoithesis Testing"},{"location":"Game_AB_Testing/Game_AB_Testing/#visualizing-the-results","text":"Gate30 = d[d['version'] == 'gate_30'] Gate40 = d[d['version'] == 'gate_40'] Gate30['retention_1'].value_counts()[True] Gate30['retention_7'].value_counts()[True] Gate40['retention_1'].value_counts()[True] Gate40['retention_7'].value_counts()[True] stages = [\"Downloaded Game\", \"1 Day Retention\", \"7 Day Retention\"] g_30 = pd.DataFrame(dict(number=[len(Gate30), Gate30['retention_1'].value_counts()[True], Gate30['retention_7'].value_counts()[True]], stage=stages)) g_30['version'] = 'Gate 30' g_40 = pd.DataFrame(dict(number=[len(Gate40), Gate40['retention_1'].value_counts()[True], Gate40['retention_7'].value_counts()[True]], stage=stages)) g_40['version'] = 'Gate 40' df = pd.concat([g_30, g_40], axis=0) fig = px.funnel(df, x='number', y='stage', color='version', color_discrete_sequence=px.colors.qualitative.Safe, title='Funnel Chart of Game Retention by Version', labels={'number':'Number of Players','version':'Game Version','stage':'Retention Point'}) fig.update_layout(title_x=0.5, font=dict( size=14, ) ) fig.show() # further segment to bulk of players Gate30_7True = Gate30[(Gate30['retention_7'] == True) & (Gate30['sum_gamerounds'] <= 100)] Gate40_7True = Gate40[(Gate40['retention_7'] == True) & (Gate40['sum_gamerounds'] <= 100)] # Plotting the data plt.figure(figsize=(8, 4)) plt.hist(Gate30_7True['sum_gamerounds'], alpha=1, label='Gate 30', color='#88CCEE') plt.hist(Gate40_7True['sum_gamerounds'], alpha=0.5, label='Gate 40', color='#CC6677') plt.legend(loc='upper right') plt.title('Users Retained After 7 Days', fontsize=18, pad=10) plt.xlabel('Total Game Rounds Played', fontsize=14, labelpad=10) plt.ylabel('Number of Users', fontsize=14, labelpad=10) plt.show()","title":"Visualizing the Results"},{"location":"HBR_Reports/Asante/","text":"Asante Teaching Hospital Challenges Problems for Asante Hospital Asante Teaching Hospital is a not-for-profit hospital in an area where for-profit hospitals thrive. As a not-for-profit hospital, Asante does not have the opportunity to seek out investors or invest surplus funds as a means of managing unexpected expenses. Rather, the hospital is reliant on insurance payouts, patient out of pocket payments, and financial support from a private foundation. Along with the challenges associated with bringing in funding, Asante is a premier hospital that provides a superior level of care. To ensure that level of care can be maintained, the hospital uses a meticulous billing process that is challenging for the hospital to compile, problematic for insurance companies who prefer bundled pricing, and stressful for patients. Problems with patient billing People like to have a feeling of control and know what to expect. This is particularly true when being discharged from a hospital after a stressful medical procedure, such as giving birth. As such, having to wait for hours after discharge to get and pay for a lengthy and detailed hospital bill, while not knowing the whole time what will be contained on it, is both overwhelming and frustrating. However, that is exactly what patients of Asante Teaching Hospital experience. Rather than using a normal costing method, where a patient could both be provided with anticipated expenses before admission and be discharged quickly, Asante uses actual costing and creates a thorough bill containing every line item and expense unique to a particular patient after their stay is complete. This process is not sustainable or helpful to either patients or the hospital itself. Recommendations Pooling cost drivers As there are standard expected annual expenses for the maternity ward, those expenses can be broken up into two main pools: Building-Based Expenses and Patient-Based Expenses (exhibit 2.1). The Building-Based expenses will not adjust according to the number of patients using the space (eg. building insurance is the same month by month), however Patient-Based Expenses may change depending on how many patients are in the hospital (eg. more laundry is washed the more patients there are). In the selected hospital information in Exhibit 1, there are two potential cost drivers associated with each of those expenses: the number of patients using the ward and the number of square feet in the ward. Regarding the number of patients, we also want to take into account their stay length, which averages to 3 days for this hospital. From this, we have two activity rates as shown in exhibit 2.2. The first is the rate of R1,042.04 per square foot used by the patient (so patients with larger, private rooms can be billed accordingly). The second is the base cost per patient per day of their stay, which is R52.28. These expenses do not change depending on the type of labor and delivery, so can remain static no matter the medical interventions needed; the only things needed to calculate these costs are the size of the room/part of the room used and the number of days a patient stays. Staff salaries calculated using ABC Regarding staff salaries, we need to breakdown how many minutes each staff member is actually available to provide care, which is shown in exhibit 3.1. This shows the anticipated number of days in their standard workweek, broken out into the total number of minutes they are paid for (including personal time, sick time, holiday, and training). We then remove those expenses to determine a final set of available care minutes for both residents and all other staff. It is important to note that personal, sick, holiday, and training time has an addition of 23% to the time, to account for taxes and benefit expenses. Using that information, we can compare staff care availability to their annual salary in exhibit 3.2 to see the cost of each employee's time on a per-minute basis, based on employee type. With that information, we can compare employee's minute-by-minute expenses to how many minutes each employee type spends working on different delivery types (levels 1-3). When totaled, this then provides the anticipated employee expense for each of the three delivery types. This does not change whether the patient has more or less space, or stays more or fewer days. As such, this can be a direct line item depending on the delivery type. Recommendation summary With the pooled overhead expenses and ABC costing for employee salaries per delivery type, billing will be simpler since the accounting department will only need to determine the delivery type, number of stays stayed, and square footage used. However, these are all baseline anticipated expenses and do not account for an employee that needs additional sick days, hallway and office space, when a delivery requires more employee time than needed, or other unanticipated expenses. As such, we recommend using this as a base costing method with an additional 20% addition to account for these unanticipated expenses in order to assure the maternity ward remains self-sufficient. Exhibits Exhibit 1: Selected hospital information Statistic Level 1 Level 2 Level 3 Average days in maternity ward 3 3 4 Total natural birth patients 4,160 240 390 Unit Amount Total maternity ward patients 11,975 Total maternity ward sqft 30,294 Total hospital sqft 455,000 Exhibit 2.1: Pooled annual maternity ward overhead Overhead Item Total Building-based Costs Equipment Depreciation R 363,672 Insurance R 233,991 Utilities R 7,454,026 Rent R 16,195,458 Groundskeeping R 898,940 Security R 302,076 Information Tech R 6,119,349 Subtotal - Building R 31,567,512 Patient-based Costs Marketing R 105,412 General/Admin R 314,622 Housekeeping R 206,241 Laundry R 395,295 Dining Hall R 856,684 Subtotal - Patient R 1,878,254 Total R 33,445,766 Exhibit 2.2: Cost drivers (overhead) Cost Driver Activity Rate Unit Building-Based Costs R 1,042.04 sqft used Patient-based Costs R 52.28 patient per day Exhibit 3.1: Employee availability (in minutes) Resident All others Daily worked time 960.00 504.00 Annual payout of time 249,600.00 131,040.00 Annual personal leave* 9,446.40 4,959.36 Annual sick leave* 7,084.80 3,719.52 Annual holiday time* 14,169.60 7,439.04 Total annual training* 7,675.20 7,675.20 Total available time 211,224.00 107,246.88 with 23% addition for benefits/taxes Exhibit 3.2: Employee annual salaries Position Salary Note OBGYN R 35,403,451 Total for team of 3 Paediatrician R 23,477,139 Total for team of 4 Midwife/Nurse R 114,557 Resident R 231,841 Registration Clerk R 9,092 Practical Nurse R 7,122 Exhibit 3.3: Cost drivers (employee salaries) Cost Driver Activity Rate Unit OBGYN R 110.04 minute worked Paediatrician R 54.73 minute worked Midwife/Nurse R 1.07 minute worked Resident R 1.10 minute worked Registration Clerk R 0.08 minute worked Practical Nurse R 0.07 minute worked Exhibit 4.1: Maternity staff time per delivery (in minutes) Position Level 1 Level 2 Level 3 OBGYN 30 37 80 Paediatrician 43 55 71 Midwife/Nurse 1,422 1,422 1,600 Resident 225 240 412 Registration Clerk 43 43 60 Practical Nurse 99 99 110 Exhibit 4.2: Maternity staff costs per delivery Position Level 1 Level 2 Level 3 OBGYN R 3,301.12 R 4,071.38 R 8,802.98 Paediatrician R 2,353.25 R 3,009.98 R 3,885.61 Midwife/Nurse R 1,518.93 R 1,518.93 R 1,709.06 Resident R 246.96 R 263.43 R 452.21 Registration Clerk R 3.65 R 3.65 R 5.09 Practical Nurse R 6.57 R 6.57 R 7.30 Total per delivery type R 7,430.48 R 8,873.93 R 14,862.25","title":"Asante Teaching Hospital Billing"},{"location":"HBR_Reports/Asante/#asante-teaching-hospital","text":"","title":"Asante Teaching Hospital"},{"location":"HBR_Reports/Asante/#challenges","text":"","title":"Challenges"},{"location":"HBR_Reports/Asante/#problems-for-asante-hospital","text":"Asante Teaching Hospital is a not-for-profit hospital in an area where for-profit hospitals thrive. As a not-for-profit hospital, Asante does not have the opportunity to seek out investors or invest surplus funds as a means of managing unexpected expenses. Rather, the hospital is reliant on insurance payouts, patient out of pocket payments, and financial support from a private foundation. Along with the challenges associated with bringing in funding, Asante is a premier hospital that provides a superior level of care. To ensure that level of care can be maintained, the hospital uses a meticulous billing process that is challenging for the hospital to compile, problematic for insurance companies who prefer bundled pricing, and stressful for patients.","title":"Problems for Asante Hospital"},{"location":"HBR_Reports/Asante/#problems-with-patient-billing","text":"People like to have a feeling of control and know what to expect. This is particularly true when being discharged from a hospital after a stressful medical procedure, such as giving birth. As such, having to wait for hours after discharge to get and pay for a lengthy and detailed hospital bill, while not knowing the whole time what will be contained on it, is both overwhelming and frustrating. However, that is exactly what patients of Asante Teaching Hospital experience. Rather than using a normal costing method, where a patient could both be provided with anticipated expenses before admission and be discharged quickly, Asante uses actual costing and creates a thorough bill containing every line item and expense unique to a particular patient after their stay is complete. This process is not sustainable or helpful to either patients or the hospital itself.","title":"Problems with patient billing"},{"location":"HBR_Reports/Asante/#recommendations","text":"","title":"Recommendations"},{"location":"HBR_Reports/Asante/#pooling-cost-drivers","text":"As there are standard expected annual expenses for the maternity ward, those expenses can be broken up into two main pools: Building-Based Expenses and Patient-Based Expenses (exhibit 2.1). The Building-Based expenses will not adjust according to the number of patients using the space (eg. building insurance is the same month by month), however Patient-Based Expenses may change depending on how many patients are in the hospital (eg. more laundry is washed the more patients there are). In the selected hospital information in Exhibit 1, there are two potential cost drivers associated with each of those expenses: the number of patients using the ward and the number of square feet in the ward. Regarding the number of patients, we also want to take into account their stay length, which averages to 3 days for this hospital. From this, we have two activity rates as shown in exhibit 2.2. The first is the rate of R1,042.04 per square foot used by the patient (so patients with larger, private rooms can be billed accordingly). The second is the base cost per patient per day of their stay, which is R52.28. These expenses do not change depending on the type of labor and delivery, so can remain static no matter the medical interventions needed; the only things needed to calculate these costs are the size of the room/part of the room used and the number of days a patient stays.","title":"Pooling cost drivers"},{"location":"HBR_Reports/Asante/#staff-salaries-calculated-using-abc","text":"Regarding staff salaries, we need to breakdown how many minutes each staff member is actually available to provide care, which is shown in exhibit 3.1. This shows the anticipated number of days in their standard workweek, broken out into the total number of minutes they are paid for (including personal time, sick time, holiday, and training). We then remove those expenses to determine a final set of available care minutes for both residents and all other staff. It is important to note that personal, sick, holiday, and training time has an addition of 23% to the time, to account for taxes and benefit expenses. Using that information, we can compare staff care availability to their annual salary in exhibit 3.2 to see the cost of each employee's time on a per-minute basis, based on employee type. With that information, we can compare employee's minute-by-minute expenses to how many minutes each employee type spends working on different delivery types (levels 1-3). When totaled, this then provides the anticipated employee expense for each of the three delivery types. This does not change whether the patient has more or less space, or stays more or fewer days. As such, this can be a direct line item depending on the delivery type.","title":"Staff salaries calculated using ABC"},{"location":"HBR_Reports/Asante/#recommendation-summary","text":"With the pooled overhead expenses and ABC costing for employee salaries per delivery type, billing will be simpler since the accounting department will only need to determine the delivery type, number of stays stayed, and square footage used. However, these are all baseline anticipated expenses and do not account for an employee that needs additional sick days, hallway and office space, when a delivery requires more employee time than needed, or other unanticipated expenses. As such, we recommend using this as a base costing method with an additional 20% addition to account for these unanticipated expenses in order to assure the maternity ward remains self-sufficient.","title":"Recommendation summary"},{"location":"HBR_Reports/Asante/#exhibits","text":"","title":"Exhibits"},{"location":"HBR_Reports/Asante/#exhibit-1-selected-hospital-information","text":"Statistic Level 1 Level 2 Level 3 Average days in maternity ward 3 3 4 Total natural birth patients 4,160 240 390 Unit Amount Total maternity ward patients 11,975 Total maternity ward sqft 30,294 Total hospital sqft 455,000","title":"Exhibit 1: Selected hospital information"},{"location":"HBR_Reports/Asante/#exhibit-21-pooled-annual-maternity-ward-overhead","text":"Overhead Item Total Building-based Costs Equipment Depreciation R 363,672 Insurance R 233,991 Utilities R 7,454,026 Rent R 16,195,458 Groundskeeping R 898,940 Security R 302,076 Information Tech R 6,119,349 Subtotal - Building R 31,567,512 Patient-based Costs Marketing R 105,412 General/Admin R 314,622 Housekeeping R 206,241 Laundry R 395,295 Dining Hall R 856,684 Subtotal - Patient R 1,878,254 Total R 33,445,766","title":"Exhibit 2.1: Pooled annual maternity ward overhead"},{"location":"HBR_Reports/Asante/#exhibit-22-cost-drivers-overhead","text":"Cost Driver Activity Rate Unit Building-Based Costs R 1,042.04 sqft used Patient-based Costs R 52.28 patient per day","title":"Exhibit 2.2: Cost drivers (overhead)"},{"location":"HBR_Reports/Asante/#exhibit-31-employee-availability-in-minutes","text":"Resident All others Daily worked time 960.00 504.00 Annual payout of time 249,600.00 131,040.00 Annual personal leave* 9,446.40 4,959.36 Annual sick leave* 7,084.80 3,719.52 Annual holiday time* 14,169.60 7,439.04 Total annual training* 7,675.20 7,675.20 Total available time 211,224.00 107,246.88 with 23% addition for benefits/taxes","title":"Exhibit 3.1: Employee availability (in minutes)"},{"location":"HBR_Reports/Asante/#exhibit-32-employee-annual-salaries","text":"Position Salary Note OBGYN R 35,403,451 Total for team of 3 Paediatrician R 23,477,139 Total for team of 4 Midwife/Nurse R 114,557 Resident R 231,841 Registration Clerk R 9,092 Practical Nurse R 7,122","title":"Exhibit 3.2: Employee annual salaries"},{"location":"HBR_Reports/Asante/#exhibit-33-cost-drivers-employee-salaries","text":"Cost Driver Activity Rate Unit OBGYN R 110.04 minute worked Paediatrician R 54.73 minute worked Midwife/Nurse R 1.07 minute worked Resident R 1.10 minute worked Registration Clerk R 0.08 minute worked Practical Nurse R 0.07 minute worked","title":"Exhibit 3.3: Cost drivers (employee salaries)"},{"location":"HBR_Reports/Asante/#exhibit-41-maternity-staff-time-per-delivery-in-minutes","text":"Position Level 1 Level 2 Level 3 OBGYN 30 37 80 Paediatrician 43 55 71 Midwife/Nurse 1,422 1,422 1,600 Resident 225 240 412 Registration Clerk 43 43 60 Practical Nurse 99 99 110","title":"Exhibit 4.1: Maternity staff time per delivery (in minutes)"},{"location":"HBR_Reports/Asante/#exhibit-42-maternity-staff-costs-per-delivery","text":"Position Level 1 Level 2 Level 3 OBGYN R 3,301.12 R 4,071.38 R 8,802.98 Paediatrician R 2,353.25 R 3,009.98 R 3,885.61 Midwife/Nurse R 1,518.93 R 1,518.93 R 1,709.06 Resident R 246.96 R 263.43 R 452.21 Registration Clerk R 3.65 R 3.65 R 5.09 Practical Nurse R 6.57 R 6.57 R 7.30 Total per delivery type R 7,430.48 R 8,873.93 R 14,862.25","title":"Exhibit 4.2: Maternity staff costs per delivery"},{"location":"HBR_Reports/Danshui/","text":"Danshui Plant 2 Problem at Hand The Danushi Plant 2 is being used to manufacture and ship the new iPhone 4. However, this plant has never manufactured phones before -- it was setup originally for computer manufacturing and so the factory workers do not have a strong background in the manufacturing processes for phones. Due to that, along with some manufacturing issues, the plant is producing and shipping on average about 20k fewer phones per month than budgeted. Alongside the reduction in production, some expected costs were increased -- specifically labor expenses, supervision, and one of the required chips. Additionally, the anticipated revenue transfer was decreased. Exhibit 1, the preliminary report for August 2010, shows these issues and has the factory running at a monthly deficit of just over 600k versus the anticipated profit of 100k. Leadership at this factory needs to evaluate the budget to understand the authenticity of the deficit, and to determine what can be done to ensure continued profitability. Analysis Summary Though exhibit 1 is technically accurate, it is much like comparing apples to oranges. Many of the budgeted expenses, and the budgeted revenue, have changed. As such, we need to take a deeper look into the actual impact of these changes in expenses. Most of the fixed costs (rent, utilities, etc.) remain static between the anticipated and actual budget, with the exception of supervision. However, Exhibit 2 shows the budgeted per-unit production expenses of each phone as compared to the actual per-unit costs (derived from the details the actual expenses of exhibit 1). This exhibit demonstrated that even with the increase in labor and chip costs, other decreases more than made up the difference and the actual per-unit production rate is almost $0.50 less than budgeted. With that adjustment, we can then use Exhibit 3 to see the breakeven point on the original budget with a 200k production versus the actual costs and revenue with 180k monthly production. This exhibit shows that when using actual costs, the number of units required to break even is almost 60k fewer than required using the budgeted numbers, and the needed revenue to break even is almost $13mil less than required for the original budget. Overall, the original report, which tried to compare a budget for 200k unit production to an actual budget for a plant that's producing 180k units monthly, skews the expense report in such a way that the plant appears less profitable than it is in reality. This is extra apparent when we review exhibit 4 which compares the budgeted per-unit and fixed costs to the actual per-unit and fixed costs on an expected 180k production. Had Danushi planned for a 180k unit production vs a 200k production, the total costs would have been immediately apparent. Recommended Next Steps The first step this plant needs to take is to accept the fact that due to the unfamiliar production process, Danushi Plant 2 will set its budget on a 180k monthly unit production, rather than 200k monthly production. Provided that sets the standard, the plant will simply by default get a more accurate picture of its profitability. One other recommendation would be to consider moving labor expenses into an ABC costing method. Since each unit goes through over 100 steps in the labor process and is touched by over 300 people, that is a lot of nuanced costing information being missed in the bulk \"assembly and packaging\" line item. It is likely that supervision should also be included in the ABC costing method for labor, since those two tasks are intrinsically tied together. With these changes, Danushi Plant 2 should have a better picture of its profitability. Exhibit 1: August 2010 Preliminary Report on Operations Monthly Budget Actual Units 200,000 180,000 Revenue (transfer from Shenzhen) $ 41,240,000 $ 37,476,000 Variable Costs Materials Flash memory $ 5,400,000 $ 5,249,000 Application Process $ 2,150,000 $ 1,935,000 Chips - Phone $ 2,810,000 $ 2,529,000 Gyroscope $ 520,000 $ 468,000 8 Other Chips $ 14,190,000 $ 12,643,000 Total: chips $ 25,070,000 $22,824,000 Variable Supplies & Tools $ 12,507,000 $ 11,305,000 Materials Subtotal $ 37,577,000 $34,129,000 Labor: Assembly and Packaging $ 2,622,000 $ 3,092,000 Shipping $ 212,000 $ 191,000 Total Variable Costs $ 40,411,000 $37,412,000 Fixed Costs Factory Rent $ 400,000 $ 400,000 Machine Depreciation $ 150,000 $ 150,000 Utility Fee & Taxes $ 52,000 $ 52,000 Supervision $ 127,000 $ 134,000 Total Fixed Costs $ 729,000 $ 736,000 Total Costs $ 41,140,000 *$ Net Income $ 100,000 $(672,000) Exhibit 2-- Per-Unit Costs for iPhone 4 Budgeted Actual Purchased Chips Flash Memory $ 27.00 $ 29.16 Application Processor $ 10.75 $ 10.75 Chip for Phone Calls $ 14.05 $ 14.05 Gyroscope $ 2.60 $ 2.60 8 other purchased chips $ 70.95 $ 70.24 Total purchased chips $ 125.35 $ 126.80 Labor: Assembly & Packaging $ 13.11 $ 17.18 Shipping $ 1.06 $ 1.06 Total Fixed Costs $ 139.52 $ 145.04 Variable Tools & Supplies $ 62.54 $ 56.53 Total cost per unit $ 202.06 $ 201.57 Exhibit 3: Contribution Margin & Break-Even Point Budgeted Actual Monthly Revenue $ 41,240,000 $ 37,476,000 Monthly Unit Production 200,000 180,000 Expected Income Per Unit $ 206.20 $ 208.20 Per-Unit Production Costs $ 202.06 $ 201.57 Contribution Margin per Unit $ 4.14 $ 6.63 Fixed Monthly Costs $ 729,000 $ 736,000 Breakeven point by Unit 176,087 110,973 Breakeven point in $ $ 36,309,130 $ 23,104,654 Exhibit 4: Production Costs Based on 180k Production Monthly Budget Actual Units Produced 180,000 180,000 Variable Costs Materials Flash memory $ 4,860,000 $ 5,249,000 Application Process $ 1,935,000 $ 1,935,000 Chips - Phone $ 2,529,000 $ 2,529,000 Gyroscope $ 468,000 $ 468,000 8 Other Chips $ 12,771,000 $ 12,643,000 Total: chips $ 22,563,000 $ 22,824,000 Variable Supplies & Tools $ 11,257,200 $10,175,400 Materials Subtotal $ 33,820,200 $32,999,400 Labor: Assembly and Packaging $ 2,359,800 $ 3,092,000 Shipping $ 190,800 $ 190,800 Total Per-Unit Costs $ 36,370,800 $36,282,200 Fixed Costs Factory Rent $ 400,000 $ 400,000 Machine Depreciation $ 150,000 $ 150,000 Utility Fee & Taxes $ 52,000 $ 52,000 Supervision $ 127,000 $ 134,000 Total Fixed Costs $ 729,000 $ 736,000 Total Costs $ 37,099,800 $37,018,200","title":"Danshui Plant Manufacturing"},{"location":"HBR_Reports/Danshui/#danshui-plant-2","text":"","title":"Danshui Plant 2"},{"location":"HBR_Reports/Danshui/#problem-at-hand","text":"The Danushi Plant 2 is being used to manufacture and ship the new iPhone 4. However, this plant has never manufactured phones before -- it was setup originally for computer manufacturing and so the factory workers do not have a strong background in the manufacturing processes for phones. Due to that, along with some manufacturing issues, the plant is producing and shipping on average about 20k fewer phones per month than budgeted. Alongside the reduction in production, some expected costs were increased -- specifically labor expenses, supervision, and one of the required chips. Additionally, the anticipated revenue transfer was decreased. Exhibit 1, the preliminary report for August 2010, shows these issues and has the factory running at a monthly deficit of just over 600k versus the anticipated profit of 100k. Leadership at this factory needs to evaluate the budget to understand the authenticity of the deficit, and to determine what can be done to ensure continued profitability.","title":"Problem at Hand"},{"location":"HBR_Reports/Danshui/#analysis-summary","text":"Though exhibit 1 is technically accurate, it is much like comparing apples to oranges. Many of the budgeted expenses, and the budgeted revenue, have changed. As such, we need to take a deeper look into the actual impact of these changes in expenses. Most of the fixed costs (rent, utilities, etc.) remain static between the anticipated and actual budget, with the exception of supervision. However, Exhibit 2 shows the budgeted per-unit production expenses of each phone as compared to the actual per-unit costs (derived from the details the actual expenses of exhibit 1). This exhibit demonstrated that even with the increase in labor and chip costs, other decreases more than made up the difference and the actual per-unit production rate is almost $0.50 less than budgeted. With that adjustment, we can then use Exhibit 3 to see the breakeven point on the original budget with a 200k production versus the actual costs and revenue with 180k monthly production. This exhibit shows that when using actual costs, the number of units required to break even is almost 60k fewer than required using the budgeted numbers, and the needed revenue to break even is almost $13mil less than required for the original budget. Overall, the original report, which tried to compare a budget for 200k unit production to an actual budget for a plant that's producing 180k units monthly, skews the expense report in such a way that the plant appears less profitable than it is in reality. This is extra apparent when we review exhibit 4 which compares the budgeted per-unit and fixed costs to the actual per-unit and fixed costs on an expected 180k production. Had Danushi planned for a 180k unit production vs a 200k production, the total costs would have been immediately apparent.","title":"Analysis Summary"},{"location":"HBR_Reports/Danshui/#recommended-next-steps","text":"The first step this plant needs to take is to accept the fact that due to the unfamiliar production process, Danushi Plant 2 will set its budget on a 180k monthly unit production, rather than 200k monthly production. Provided that sets the standard, the plant will simply by default get a more accurate picture of its profitability. One other recommendation would be to consider moving labor expenses into an ABC costing method. Since each unit goes through over 100 steps in the labor process and is touched by over 300 people, that is a lot of nuanced costing information being missed in the bulk \"assembly and packaging\" line item. It is likely that supervision should also be included in the ABC costing method for labor, since those two tasks are intrinsically tied together. With these changes, Danushi Plant 2 should have a better picture of its profitability.","title":"Recommended Next Steps"},{"location":"HBR_Reports/Danshui/#exhibit-1-august-2010-preliminary-report-on-operations","text":"Monthly Budget Actual Units 200,000 180,000 Revenue (transfer from Shenzhen) $ 41,240,000 $ 37,476,000 Variable Costs Materials Flash memory $ 5,400,000 $ 5,249,000 Application Process $ 2,150,000 $ 1,935,000 Chips - Phone $ 2,810,000 $ 2,529,000 Gyroscope $ 520,000 $ 468,000 8 Other Chips $ 14,190,000 $ 12,643,000 Total: chips $ 25,070,000 $22,824,000 Variable Supplies & Tools $ 12,507,000 $ 11,305,000 Materials Subtotal $ 37,577,000 $34,129,000 Labor: Assembly and Packaging $ 2,622,000 $ 3,092,000 Shipping $ 212,000 $ 191,000 Total Variable Costs $ 40,411,000 $37,412,000 Fixed Costs Factory Rent $ 400,000 $ 400,000 Machine Depreciation $ 150,000 $ 150,000 Utility Fee & Taxes $ 52,000 $ 52,000 Supervision $ 127,000 $ 134,000 Total Fixed Costs $ 729,000 $ 736,000 Total Costs $ 41,140,000 *$ Net Income $ 100,000 $(672,000)","title":"Exhibit 1: August 2010 Preliminary Report on Operations"},{"location":"HBR_Reports/Danshui/#exhibit-2-per-unit-costs-for-iphone-4","text":"Budgeted Actual Purchased Chips Flash Memory $ 27.00 $ 29.16 Application Processor $ 10.75 $ 10.75 Chip for Phone Calls $ 14.05 $ 14.05 Gyroscope $ 2.60 $ 2.60 8 other purchased chips $ 70.95 $ 70.24 Total purchased chips $ 125.35 $ 126.80 Labor: Assembly & Packaging $ 13.11 $ 17.18 Shipping $ 1.06 $ 1.06 Total Fixed Costs $ 139.52 $ 145.04 Variable Tools & Supplies $ 62.54 $ 56.53 Total cost per unit $ 202.06 $ 201.57","title":"Exhibit 2-- Per-Unit Costs for iPhone 4"},{"location":"HBR_Reports/Danshui/#exhibit-3-contribution-margin-break-even-point","text":"Budgeted Actual Monthly Revenue $ 41,240,000 $ 37,476,000 Monthly Unit Production 200,000 180,000 Expected Income Per Unit $ 206.20 $ 208.20 Per-Unit Production Costs $ 202.06 $ 201.57 Contribution Margin per Unit $ 4.14 $ 6.63 Fixed Monthly Costs $ 729,000 $ 736,000 Breakeven point by Unit 176,087 110,973 Breakeven point in $ $ 36,309,130 $ 23,104,654","title":"Exhibit 3: Contribution Margin &amp; Break-Even Point"},{"location":"HBR_Reports/Danshui/#exhibit-4-production-costs-based-on-180k-production","text":"Monthly Budget Actual Units Produced 180,000 180,000 Variable Costs Materials Flash memory $ 4,860,000 $ 5,249,000 Application Process $ 1,935,000 $ 1,935,000 Chips - Phone $ 2,529,000 $ 2,529,000 Gyroscope $ 468,000 $ 468,000 8 Other Chips $ 12,771,000 $ 12,643,000 Total: chips $ 22,563,000 $ 22,824,000 Variable Supplies & Tools $ 11,257,200 $10,175,400 Materials Subtotal $ 33,820,200 $32,999,400 Labor: Assembly and Packaging $ 2,359,800 $ 3,092,000 Shipping $ 190,800 $ 190,800 Total Per-Unit Costs $ 36,370,800 $36,282,200 Fixed Costs Factory Rent $ 400,000 $ 400,000 Machine Depreciation $ 150,000 $ 150,000 Utility Fee & Taxes $ 52,000 $ 52,000 Supervision $ 127,000 $ 134,000 Total Fixed Costs $ 729,000 $ 736,000 Total Costs $ 37,099,800 $37,018,200","title":"Exhibit 4: Production Costs Based on 180k Production"},{"location":"HBR_Reports/LondonWater/","text":"London Water Problem at Hand The water management system for London, Ontario has gone through multiple administrative and regulatory changes over the last few decades, causing increases in expenses for purely bureaucratic purposes -- though in the case of the regulatory changes these expenses are welcomed for public safety. These changes, though, are in addition to the natural expenses that arise with a city that has seen a drastic increase in growth over the years and has aging pipelines that need to be planned for replacement in the coming decades. Add that to a pricing structure that hasn't been reviewed in some time and London Water's billing method is due for some adjustments. The business is not sustainable as is; the pricing structure needs to be adjusted to ensure future financial stability that accounts for needed upgrades and expansions on the water system, while also encouraging reduced water usage both for environmental reasons and to help reduce the strain on the water system. However, it is not as easy as simply increasing rates across the board. There are three main categories of water use: water intake, wastewater processing, and stormwater management. The first, and least expensive to manage, is easy to account for using water meters. However, there is nuance to the last two -- which also happen to be the most expensive to manage. Wastewater processing is not as easy as \"water in equals water out\". Rather, there are multiple businesses, providing economic value to the city, that use water in their product. These include companies like breweries, so water intake is vastly higher than their wastewater production. Additionally, there are concerns around the equitability of the stormwater billing structure due to the limitations of the current system. Analysis Summary The existing water rate structure, shown in Exhibit 1, differentiates rates between residential customers (R1-R3) and business customers (industrial, commercial, and institutional -- ICI1-ICI3). Using this payment structure, with the exception of ICI1 which is an extreme outlier, businesses that are using the same amount of water as residential customers are paying less per cubic meter of water non-business customers. While the likely intent of reduced rates for businesses is to support industries that rely on water for their final product, such as breweries, a business using the same amount of water as a residential customer is most likely not using water in their final product. As such, residential customers are directly subsidizing the water expenses for businesses in the area. Exhibit 2, though, shows the proposed new water rate which does not differentiate between business and residential customers. Rather, the new rate breaks metered water usage into eight groups with unique rates intended to meet particular goals based on the amount of water being used. The first two groups, Block 1 and 2, are for customers using the least amount of water and are intended to provide consumers with a built-in discount for reducing their water consumption. From there, Block 3 and 4 increase rates to account for the added strain on the water system by residential and business customers for whom water intake is assumed to be similar to wastewater output. Starting with group 5 and beyond, the cubic meter rate reduces and continues to reduce to both account for and encourage the development of businesses that use water for their sales product. For the residential side, customers who are on the higher end of water usage will see an increase in their water expenses. However, that is intentional and meant to encourage a reduction in water use over time. As seen in Exhibit 3, the prospective of metered residential water usage rates, under the new billing system the average residential customer would be included in Block 3 for 2011-2013, while the former system had the average customer in RS1 for 2011 and 2012, then RS2 for 2013. With the new billing system in place, the total metered fees for residential customers would increase, bringing in an additional 6-8 million in revenue provided water usage doesn't change. Recommended Next Steps By adjusting the metered water rates to no longer differentiate between residential and business customers, London Water will be able to both bring in additional revenue and provide a more equitable experience to its customer base while still adjusting for business enterprises that use water as part of their final product. However, while a new customer billing layout is being introduced alongside the new metered fee system, the flat rate fees are as yet unchanged. The new billing system for metered charges directly addresses water intake and wastewater processing, however the inequity in expenses related to stormwater processing still needs to be addressed. This is the next step that must be taken in the review of London Water's billing system. Under the current system, as seen in exhibit 4, the smallest cutoff for stormwater charges is 0.4 hectares, or 1 acre. As London, Ontario is a fairly dense urban area, the average residential lot is well below that cutoff. For example, exhibit 5 demonstrates how a single institutional business customer uses over 60 times the land mass of 8 residential customers (shown in the circled oval), yet the group of residential customers pay more the same amount for stormwater runoff than the institutional customer. In full transparency, the business customers are underpaying for their stormwater fees. By adding nuance to the billing structure to account for variable lot sizes, London Water will be able to build more equity into their billing structure while also recouping lost expenses for processing stormwater runoff for businesses that use larger areas of land. This is the next London Water needs to take in building a sustainable billing system. Exhibits Exhibit 1: London Water's Current Water Rate Structure Exhibit 2: London Water's New Water Rate Structure Exhibit 3: Prospective of Metered Residential Water Usage Rates 2011 2012 2013 Total Residential Water Usage for the Year 18,527,265 m 3 18,488,769 m 3 23,584,012 m 3 Total Number of Residential Customers 99,641 100,801 102,421 Average Monthly Usage per Residential Customer 15.50 m 3 15.28 m 3 19.19 m 3 Average Water Rate: New $ 2.06 $ 2.06 $ 2.06 Average Water Rate: Old $ 1.61 $ 1.61 $ 1.78 Revenue: New Rate $ 38,166,166 $ 38,086,864 $ 48,583,065 Revenue: Old Rate $ 29,828,897 $ 29,766,918 $ 41,979,541 Revenue Increase $ 8,337,269 $ 8,319,946 $ 6,603,523 Exhibit 4: Stormwater Processing Fees Customer Type Rate Unit Residential < 0.4 hectares $ 13.78 monthly Institutional < 0.4 hectares $ 13.66 monthly Commercial/Industrial < 0.4 hectares $ 14.78 monthly Commercial/Institutional/Residential > 0.4 hectares $ 75.12 per hectare Industrial > 0.4 hectares $ 112.68 per hectare 0.4 hectares ~ 1 acre Exhibit 5: Institutional vs Residential Land Area Use","title":"London Water Pricing"},{"location":"HBR_Reports/LondonWater/#london-water","text":"","title":"London Water"},{"location":"HBR_Reports/LondonWater/#problem-at-hand","text":"The water management system for London, Ontario has gone through multiple administrative and regulatory changes over the last few decades, causing increases in expenses for purely bureaucratic purposes -- though in the case of the regulatory changes these expenses are welcomed for public safety. These changes, though, are in addition to the natural expenses that arise with a city that has seen a drastic increase in growth over the years and has aging pipelines that need to be planned for replacement in the coming decades. Add that to a pricing structure that hasn't been reviewed in some time and London Water's billing method is due for some adjustments. The business is not sustainable as is; the pricing structure needs to be adjusted to ensure future financial stability that accounts for needed upgrades and expansions on the water system, while also encouraging reduced water usage both for environmental reasons and to help reduce the strain on the water system. However, it is not as easy as simply increasing rates across the board. There are three main categories of water use: water intake, wastewater processing, and stormwater management. The first, and least expensive to manage, is easy to account for using water meters. However, there is nuance to the last two -- which also happen to be the most expensive to manage. Wastewater processing is not as easy as \"water in equals water out\". Rather, there are multiple businesses, providing economic value to the city, that use water in their product. These include companies like breweries, so water intake is vastly higher than their wastewater production. Additionally, there are concerns around the equitability of the stormwater billing structure due to the limitations of the current system.","title":"Problem at Hand"},{"location":"HBR_Reports/LondonWater/#analysis-summary","text":"The existing water rate structure, shown in Exhibit 1, differentiates rates between residential customers (R1-R3) and business customers (industrial, commercial, and institutional -- ICI1-ICI3). Using this payment structure, with the exception of ICI1 which is an extreme outlier, businesses that are using the same amount of water as residential customers are paying less per cubic meter of water non-business customers. While the likely intent of reduced rates for businesses is to support industries that rely on water for their final product, such as breweries, a business using the same amount of water as a residential customer is most likely not using water in their final product. As such, residential customers are directly subsidizing the water expenses for businesses in the area. Exhibit 2, though, shows the proposed new water rate which does not differentiate between business and residential customers. Rather, the new rate breaks metered water usage into eight groups with unique rates intended to meet particular goals based on the amount of water being used. The first two groups, Block 1 and 2, are for customers using the least amount of water and are intended to provide consumers with a built-in discount for reducing their water consumption. From there, Block 3 and 4 increase rates to account for the added strain on the water system by residential and business customers for whom water intake is assumed to be similar to wastewater output. Starting with group 5 and beyond, the cubic meter rate reduces and continues to reduce to both account for and encourage the development of businesses that use water for their sales product. For the residential side, customers who are on the higher end of water usage will see an increase in their water expenses. However, that is intentional and meant to encourage a reduction in water use over time. As seen in Exhibit 3, the prospective of metered residential water usage rates, under the new billing system the average residential customer would be included in Block 3 for 2011-2013, while the former system had the average customer in RS1 for 2011 and 2012, then RS2 for 2013. With the new billing system in place, the total metered fees for residential customers would increase, bringing in an additional 6-8 million in revenue provided water usage doesn't change.","title":"Analysis Summary"},{"location":"HBR_Reports/LondonWater/#recommended-next-steps","text":"By adjusting the metered water rates to no longer differentiate between residential and business customers, London Water will be able to both bring in additional revenue and provide a more equitable experience to its customer base while still adjusting for business enterprises that use water as part of their final product. However, while a new customer billing layout is being introduced alongside the new metered fee system, the flat rate fees are as yet unchanged. The new billing system for metered charges directly addresses water intake and wastewater processing, however the inequity in expenses related to stormwater processing still needs to be addressed. This is the next step that must be taken in the review of London Water's billing system. Under the current system, as seen in exhibit 4, the smallest cutoff for stormwater charges is 0.4 hectares, or 1 acre. As London, Ontario is a fairly dense urban area, the average residential lot is well below that cutoff. For example, exhibit 5 demonstrates how a single institutional business customer uses over 60 times the land mass of 8 residential customers (shown in the circled oval), yet the group of residential customers pay more the same amount for stormwater runoff than the institutional customer. In full transparency, the business customers are underpaying for their stormwater fees. By adding nuance to the billing structure to account for variable lot sizes, London Water will be able to build more equity into their billing structure while also recouping lost expenses for processing stormwater runoff for businesses that use larger areas of land. This is the next London Water needs to take in building a sustainable billing system.","title":"Recommended Next Steps"},{"location":"HBR_Reports/LondonWater/#exhibits","text":"","title":"Exhibits"},{"location":"HBR_Reports/LondonWater/#exhibit-1-london-waters-current-water-rate-structure","text":"","title":"Exhibit 1: London Water's Current Water Rate Structure"},{"location":"HBR_Reports/LondonWater/#exhibit-2-london-waters-new-water-rate-structure","text":"","title":"Exhibit 2: London Water's New Water Rate Structure"},{"location":"HBR_Reports/LondonWater/#exhibit-3-prospective-of-metered-residential-water-usage-rates","text":"2011 2012 2013 Total Residential Water Usage for the Year 18,527,265 m 3 18,488,769 m 3 23,584,012 m 3 Total Number of Residential Customers 99,641 100,801 102,421 Average Monthly Usage per Residential Customer 15.50 m 3 15.28 m 3 19.19 m 3 Average Water Rate: New $ 2.06 $ 2.06 $ 2.06 Average Water Rate: Old $ 1.61 $ 1.61 $ 1.78 Revenue: New Rate $ 38,166,166 $ 38,086,864 $ 48,583,065 Revenue: Old Rate $ 29,828,897 $ 29,766,918 $ 41,979,541 Revenue Increase $ 8,337,269 $ 8,319,946 $ 6,603,523","title":"Exhibit 3: Prospective of Metered Residential Water Usage Rates"},{"location":"HBR_Reports/LondonWater/#exhibit-4-stormwater-processing-fees","text":"Customer Type Rate Unit Residential < 0.4 hectares $ 13.78 monthly Institutional < 0.4 hectares $ 13.66 monthly Commercial/Industrial < 0.4 hectares $ 14.78 monthly Commercial/Institutional/Residential > 0.4 hectares $ 75.12 per hectare Industrial > 0.4 hectares $ 112.68 per hectare 0.4 hectares ~ 1 acre","title":"Exhibit 4: Stormwater Processing Fees"},{"location":"HBR_Reports/LondonWater/#exhibit-5-institutional-vs-residential-land-area-use","text":"","title":"Exhibit 5: Institutional vs Residential Land Area Use"},{"location":"HBR_Reports/PolicePricing/","text":"Pricing Police Case New Billing Model Impact The new billing model uses two cost drivers: base policing services and the calls for service. As shown in exhibit 1, the base policing services are calculated at a rate of $195.96 per property within a given municipality, while the calls for service are calculated at $83.97 per hour of weighted time spent on calls for service over the preceding four years. These numbers are generated for 2015, and will be updated annually using the most recent calculations for calls for service and property counts. Using this model, the Alder, Balsam, and Cedar municipalities will all see a change in their billing rates, shown in exhibit 2. Alder and Cedar will both see an increase for 2015, with Cedar's fees increasing by almost 50% and Alder just shy of 20%. Balsam, on the other hand, will see a decrease of just over 25% in their billing rate. This is logical since Alder, Balsam, and Cedar were each using a billing rate from different years: 2011, 2013, and 2009 respectively, as shown in exhibit 3. Additionally, the former billing model used a single cost driver based on the number of properties in a given municipality for billing purposes. However, the number of properties do not correlate directly to the number of service call hours used. For example, in the preceding four years, the percentage of total weighed time for service calls that Alder used was nearly double the percentage of properties they have to the total service area, as shown in exhibits 3 and 4. Single Cost Driver Impact When considering proposed single cost driver options, we can use property counts, calls for service, population, household income, or property value as the cost drivers. The applicable rates for each cost driver is shown in exhibit 5. The cost drivers were determined by dividing the combined expense totals for base services and calls for service (exhibit 6) by the applicable municipality information provided (exhibit 3). The resulting calculations for the Adler, Balsam, and Cedar municipalities are available in exhibit 7, including the difference in cost for each between the new two driver model and the former model. It is important to note that the first option, calculating based on properties, is the same as the current model and so if no changes were made this would be the model used. For each of the proposed single cost driver options, there will be an impact on each municipalities expenses. For Alder, calculating by either population or calls for service would increase their expenses when compared to either the current model or the proposed new model, while all other options would be a reduction in expenses. Balsam would see a reduction in expenses for any of the single cost drivers when compared to the current plan, but would see a less than 10% change (either positive or negative) when comparing to the proposed new plan. Lastly, Cedar would see an increase in expenses for any of the options when compared to the current model, by as much as 67%. The new model, though, has variable differences to single cost driver options. Summary & Recommendation The large variance in how different billing models impact different municipalities lays in the fact that each municipality is unique. It's important to account not just for a single cost driver, such as population, but to also note how much time and effort each municipality requires of OPP. There are not direct correlations between a single factor and the needs of a municipality for OPP. As such, it is fairest to use the two-driver model where the base services are calculated using population while using historical calls for service to anticipate future call for service needs. While some municipalities may be up and coming and will require fewer calls for service or less time on each service call, using the preceding four years to calculate anticipated needs provides an ongoing method of updating the calculation on a year-by-year basis. If there is an extreme change in CFS needs one year, that will reflect in the following year's expenses. This is a flexible calculation that provides room for growth and changing municipality needs, which is not accounted for in a single cost driver model. Exhibits Exhibit 1: Cost Drivers for the New Billing Model Cost Driver Activity Rate Unit Base Policing Services $ 195.96 property Calls for Service $ 83.97 hour of weighted time Exhibit 2 -- Per-Municipality Service Costs for the New Billing Model Service Costs Alder Balsam Cedar Base Policing Services $927,262.28 $3,059,064.14 $147,358.67 Calls for Service $1,189,202.80 $1,810,373.07 $ 113,573.42 Total $2,116,465.08 $4,869,437.21 $260,932.09 Former Contract Expense $1,724,246.16 $6,121,541.43 $ 130,712.64 Difference from former contract $392,218.92 $(1,252,104.22) $130,219.45 Percentage change 18.53% -25.71% 49.91% Exhibit 3: Municipality Details & Former Contract Category Alder Balsam Cedar All Areas Region East Central North West n/a Population 14,220 27,221 2,370 2,146,789 Area (km 2 ) 17.24 22.37 12.90 1,035.57 Pop. Density (km 2 ) 824.83 1,216.85 183.72 2.07 Households 4,430 15,082 671 1,069,951 Commercial Prop. 302 529 81 58,847 Total Properties 4,732 15,611 752 1,128,798 Avg. Household Income $ 49,769 $ 82,179 $ 62,465 $ 78,160 Avg. Household Size 3.21 1.80 3.53 2.01 Total Property Assessment $891,836,576 $3,695,312,326 $280,010,747 $276,024,606,014 Average Property Assessment $ 188,469 $ 236,712 $ 372,355 $ 244,530 Year of Prev. Contract 2011 2013 2009 n/a Cost per Property per Prev. Contract $ 364.38 $ 392.13 $ 173.82 n/a Exhibit 4: 2015 Calls for Service Totals Calls for service totals Avg. Calls Total Weighted Time % of weighted time Alder 3,215 14,162 0.86% Balsam 5,348 21,559 1.31% Cedar 340 1,353 0.08% ALL SERVICE AREAS 399,243 1,644,880.28 Exhibit 5: Single Cost Driver Model Cost Driver Activity Rate Unit Properties $ 318.32 property CFS $ 900.00 call Population $ 167.37 resident Household Income $ 0.004297 dollar of household income Property Value $ 0.001302 dollar of property assessment Exhibit 6: Summary Base Service and Calls for Service Category FTE's Base Service Calls for Service Total (all areas) Total uniform salaries & benefits 2,237.47 $173,098,725 $107,444,219 $280,542,944 Total detachment civilian salaries & benefits 192.32 $8,932,506 $5,928,200 $14,860,706 Total support staff salaries & benefits costs $16,279,664 $10,285,817 $26,565,481 Total salaries & benefits $198,310,895 $123,658,236 $321,969,131 Total other direct operating expenses $22,883,486 $14,464,157 $37,347,643 Total municipal base & calls for service cost $221,194,381 $138,122,393 $359,316,774 Total OPP-policed municipal properties 1,128,798 Base service cost per property $195.96 Exhibit 7: Single Cost Driver Model Cost Driver Dollar Amounts Percentage Change Alder Balsam Cedar Alder Balsam Cedar Properties $1,506,280.99 $4,969,263.02 $239,375.17 Difference from new billing model $(610,184.09) $99,825.81 $(21,556.92) -40.51% 2.01% -9.01% Difference from former contract $(217,965.17) $(1,152,278.41) $108,662.53 -14.47% -23.19% 45.39% Calls for Service $2,893,484.49 $4,813,174.20 $305,998.36 Difference from new billing model* $777,019.41 $(56,263.01) $45,066.27 26.85% -1.17% 14.73% Difference from former contract $1,169,238.33 $(1,308,367.23) $175,285.72 40.41% -27.18% 57.28% Population $2,380,059.02 $4,556,089.07 $396,676.50 Difference from new billing model $263,593.94 $(313,348.14) $135,744.41 11.08% -6.88% 34.22% Difference from former contract $655,812.86 $(1,565,452.36) $265,963.86 27.55% -34.36% 67.05% Household Income $ 947,309.06 $ 5,325,358.39 $ 180,089.47 Difference from new billing model $(1,169,156.02) $455,921.18 $(80,842.62) -123.42% 8.56% -44.89% Difference from former contract $(776,937.10) $ (796,183.04) $49,376.83 -82.02% -14.95% 27.42% Property Value $1,160,953.89 $4,810,396.16 $364,505.76 Difference from new billing model $(955,511.19) $(59,041.06) $103,573.67 -82.30% -1.23% 28.41% Difference from former contract $(563,292.27) $(1,311,145.27) $233,793.12 -48.52% -27.26% 64.14%","title":"Police Billing Structure"},{"location":"HBR_Reports/PolicePricing/#pricing-police-case","text":"","title":"Pricing Police Case"},{"location":"HBR_Reports/PolicePricing/#new-billing-model-impact","text":"The new billing model uses two cost drivers: base policing services and the calls for service. As shown in exhibit 1, the base policing services are calculated at a rate of $195.96 per property within a given municipality, while the calls for service are calculated at $83.97 per hour of weighted time spent on calls for service over the preceding four years. These numbers are generated for 2015, and will be updated annually using the most recent calculations for calls for service and property counts. Using this model, the Alder, Balsam, and Cedar municipalities will all see a change in their billing rates, shown in exhibit 2. Alder and Cedar will both see an increase for 2015, with Cedar's fees increasing by almost 50% and Alder just shy of 20%. Balsam, on the other hand, will see a decrease of just over 25% in their billing rate. This is logical since Alder, Balsam, and Cedar were each using a billing rate from different years: 2011, 2013, and 2009 respectively, as shown in exhibit 3. Additionally, the former billing model used a single cost driver based on the number of properties in a given municipality for billing purposes. However, the number of properties do not correlate directly to the number of service call hours used. For example, in the preceding four years, the percentage of total weighed time for service calls that Alder used was nearly double the percentage of properties they have to the total service area, as shown in exhibits 3 and 4.","title":"New Billing Model Impact"},{"location":"HBR_Reports/PolicePricing/#single-cost-driver-impact","text":"When considering proposed single cost driver options, we can use property counts, calls for service, population, household income, or property value as the cost drivers. The applicable rates for each cost driver is shown in exhibit 5. The cost drivers were determined by dividing the combined expense totals for base services and calls for service (exhibit 6) by the applicable municipality information provided (exhibit 3). The resulting calculations for the Adler, Balsam, and Cedar municipalities are available in exhibit 7, including the difference in cost for each between the new two driver model and the former model. It is important to note that the first option, calculating based on properties, is the same as the current model and so if no changes were made this would be the model used. For each of the proposed single cost driver options, there will be an impact on each municipalities expenses. For Alder, calculating by either population or calls for service would increase their expenses when compared to either the current model or the proposed new model, while all other options would be a reduction in expenses. Balsam would see a reduction in expenses for any of the single cost drivers when compared to the current plan, but would see a less than 10% change (either positive or negative) when comparing to the proposed new plan. Lastly, Cedar would see an increase in expenses for any of the options when compared to the current model, by as much as 67%. The new model, though, has variable differences to single cost driver options.","title":"Single Cost Driver Impact"},{"location":"HBR_Reports/PolicePricing/#summary-recommendation","text":"The large variance in how different billing models impact different municipalities lays in the fact that each municipality is unique. It's important to account not just for a single cost driver, such as population, but to also note how much time and effort each municipality requires of OPP. There are not direct correlations between a single factor and the needs of a municipality for OPP. As such, it is fairest to use the two-driver model where the base services are calculated using population while using historical calls for service to anticipate future call for service needs. While some municipalities may be up and coming and will require fewer calls for service or less time on each service call, using the preceding four years to calculate anticipated needs provides an ongoing method of updating the calculation on a year-by-year basis. If there is an extreme change in CFS needs one year, that will reflect in the following year's expenses. This is a flexible calculation that provides room for growth and changing municipality needs, which is not accounted for in a single cost driver model.","title":"Summary &amp; Recommendation"},{"location":"HBR_Reports/PolicePricing/#exhibits","text":"","title":"Exhibits"},{"location":"HBR_Reports/PolicePricing/#exhibit-1-cost-drivers-for-the-new-billing-model","text":"Cost Driver Activity Rate Unit Base Policing Services $ 195.96 property Calls for Service $ 83.97 hour of weighted time","title":"Exhibit 1: Cost Drivers for the New Billing Model"},{"location":"HBR_Reports/PolicePricing/#exhibit-2-per-municipality-service-costs-for-the-new-billing-model","text":"Service Costs Alder Balsam Cedar Base Policing Services $927,262.28 $3,059,064.14 $147,358.67 Calls for Service $1,189,202.80 $1,810,373.07 $ 113,573.42 Total $2,116,465.08 $4,869,437.21 $260,932.09 Former Contract Expense $1,724,246.16 $6,121,541.43 $ 130,712.64 Difference from former contract $392,218.92 $(1,252,104.22) $130,219.45 Percentage change 18.53% -25.71% 49.91%","title":"Exhibit 2 -- Per-Municipality Service Costs for the New Billing Model"},{"location":"HBR_Reports/PolicePricing/#exhibit-3-municipality-details-former-contract","text":"Category Alder Balsam Cedar All Areas Region East Central North West n/a Population 14,220 27,221 2,370 2,146,789 Area (km 2 ) 17.24 22.37 12.90 1,035.57 Pop. Density (km 2 ) 824.83 1,216.85 183.72 2.07 Households 4,430 15,082 671 1,069,951 Commercial Prop. 302 529 81 58,847 Total Properties 4,732 15,611 752 1,128,798 Avg. Household Income $ 49,769 $ 82,179 $ 62,465 $ 78,160 Avg. Household Size 3.21 1.80 3.53 2.01 Total Property Assessment $891,836,576 $3,695,312,326 $280,010,747 $276,024,606,014 Average Property Assessment $ 188,469 $ 236,712 $ 372,355 $ 244,530 Year of Prev. Contract 2011 2013 2009 n/a Cost per Property per Prev. Contract $ 364.38 $ 392.13 $ 173.82 n/a","title":"Exhibit 3: Municipality Details &amp; Former Contract"},{"location":"HBR_Reports/PolicePricing/#exhibit-4-2015-calls-for-service-totals","text":"Calls for service totals Avg. Calls Total Weighted Time % of weighted time Alder 3,215 14,162 0.86% Balsam 5,348 21,559 1.31% Cedar 340 1,353 0.08% ALL SERVICE AREAS 399,243 1,644,880.28","title":"Exhibit 4: 2015 Calls for Service Totals"},{"location":"HBR_Reports/PolicePricing/#exhibit-5-single-cost-driver-model","text":"Cost Driver Activity Rate Unit Properties $ 318.32 property CFS $ 900.00 call Population $ 167.37 resident Household Income $ 0.004297 dollar of household income Property Value $ 0.001302 dollar of property assessment","title":"Exhibit 5: Single Cost Driver Model"},{"location":"HBR_Reports/PolicePricing/#exhibit-6-summary-base-service-and-calls-for-service","text":"Category FTE's Base Service Calls for Service Total (all areas) Total uniform salaries & benefits 2,237.47 $173,098,725 $107,444,219 $280,542,944 Total detachment civilian salaries & benefits 192.32 $8,932,506 $5,928,200 $14,860,706 Total support staff salaries & benefits costs $16,279,664 $10,285,817 $26,565,481 Total salaries & benefits $198,310,895 $123,658,236 $321,969,131 Total other direct operating expenses $22,883,486 $14,464,157 $37,347,643 Total municipal base & calls for service cost $221,194,381 $138,122,393 $359,316,774 Total OPP-policed municipal properties 1,128,798 Base service cost per property $195.96","title":"Exhibit 6: Summary Base Service and Calls for Service"},{"location":"HBR_Reports/PolicePricing/#exhibit-7-single-cost-driver-model","text":"Cost Driver Dollar Amounts Percentage Change Alder Balsam Cedar Alder Balsam Cedar Properties $1,506,280.99 $4,969,263.02 $239,375.17 Difference from new billing model $(610,184.09) $99,825.81 $(21,556.92) -40.51% 2.01% -9.01% Difference from former contract $(217,965.17) $(1,152,278.41) $108,662.53 -14.47% -23.19% 45.39% Calls for Service $2,893,484.49 $4,813,174.20 $305,998.36 Difference from new billing model* $777,019.41 $(56,263.01) $45,066.27 26.85% -1.17% 14.73% Difference from former contract $1,169,238.33 $(1,308,367.23) $175,285.72 40.41% -27.18% 57.28% Population $2,380,059.02 $4,556,089.07 $396,676.50 Difference from new billing model $263,593.94 $(313,348.14) $135,744.41 11.08% -6.88% 34.22% Difference from former contract $655,812.86 $(1,565,452.36) $265,963.86 27.55% -34.36% 67.05% Household Income $ 947,309.06 $ 5,325,358.39 $ 180,089.47 Difference from new billing model $(1,169,156.02) $455,921.18 $(80,842.62) -123.42% 8.56% -44.89% Difference from former contract $(776,937.10) $ (796,183.04) $49,376.83 -82.02% -14.95% 27.42% Property Value $1,160,953.89 $4,810,396.16 $364,505.76 Difference from new billing model $(955,511.19) $(59,041.06) $103,573.67 -82.30% -1.23% 28.41% Difference from former contract $(563,292.27) $(1,311,145.27) $233,793.12 -48.52% -27.26% 64.14%","title":"Exhibit 7: Single Cost Driver Model"},{"location":"HBR_Reports/TidalCloud/","text":"Implementing ABC at Tidal Cloud Inc Background Tidal Cloud Inc (Tidal) is a hosting provider that offers both managed hosting using physical servers as well as cloud-based hosting with two unique service offerings: public cloud and private cloud (Marea Cloud). Public cloud uses shared cloud servers to manage data, while Marea Cloud offers private servers for each client, a benefit for clients that need to ensure the privacy and security of their data. This company is facing pressure from its investors to increase profit margins as a means to start creating an exit strategy for the company. The end goal is to demonstrate the best profitability they can, to make the most out of selling the company. To help with this process Tidal began heavily investing in cloud infrastructure as that technology became increasingly popular, but their competitors are lowering prices on public cloud services at a time when Tidal's profit margins on those services are already tight. However, they've been able to increase pricing on their private cloud hosting service, Marea Cloud, without a drop in customer base. The managed hosting service is essentially in a \"set it and forget it\" status, where their customer base is stable provided nothing goes wrong, and they're not looking to increase that service due to concerns around resiliency. According to Tidal's existing simple cost system, which allocates a percentage of indirect budget expenses to each service according to the number of virtual servers in use at the end of each month per service, while also accounting for direct costs in the form of equipment leasing. Under that cost system, the company is operating at a 2% profit margin, managed hosting operating at a 3% profit margin, public hosting operating at a loss with -5.6% and Marea Cloud having the best profit margin of 13.7% (see Appendix I). There are two main issues with this simplified costing method, though. First, the managed services don't actually use virtual servers and so their operating expenses have to be assumed. Second, allocation of budget is based on virtual machines, but in the public cloud service, virtual machines may be shared while in the private model they are always individual to the company. As such, allocating based on virtual machine does not account for actual expenses. Recommendation By using Activity Based Costing, Tidal would be able to identify true operating costs for each service and get a more accurate picture of their profit margins for each service. Appendix II shows what the activity rate is for each cost driver accounted for within the previously mass-allocated indirect costs, as well as the source data for those rates which shows how many \"units\" of each cost driver the different products used. When multiplying the activity rates by the units of each rate used per product, you get the ABC method of costing, shown in Appendix III. Using this method, the profit margins look very different, with managed hosting operating at a 14.03% profit margin, public cloud operating with a 5.3% profit margin, and Marea Cloud operating at a loss of 11.36%. Using this information, Tidal can make more informed business decisions since what was previously shown as their primary profit driver, Marea Cloud, is actually their loss leader. Appendices Appendix I: Simple Cost System Product Profitability for Month Ended June 30, 2016 Company Managed Hosting Public Cloud Marea Cloud Sales $950,000 $191,900 $469,300 $288,800 Equipment Leasing $(237,500) $(47,500) $(114,000) $(76,000) Allocated Indirect Costs $(693,500) $(138,700) $(381,425) $(173,375) Operating profit before SG&A $19,000 $5,700 $(26,125) $39,425 Profit Margin 2.0% 3.0% (5.6%) 13.7% Appendix II: Activity Rates & Cost Drivers Activity Rates Activity Rate Unit Provide Computing Resources $6.33 VM Process Payments $0.02 Dollar Earned Onboard New Customers $72.44 Hour Support Existing Customers $103.61 Help Ticket Build & Improve Products $54.15 Code Commit Advertise & Promote $855.00 Percent of Spending Budgeted Quantity of Activity Driver Core Activities Budgeted Activity Driver (per month) Company Managed Hosting Public Cloud Marea Cloud Provide Computing Resources $228,000 Number of VMs at month end 36,000 7,200 19,800 9,000 Process Payments $19,000 $ Sales $950,000 $191,900 $469,300 $288,800 Onboard New Customers $86,925 Hours spent onboarding new VMs 1,200 60 420 720 Support Existing Customers $165,775 Help tickets addressed 1,600 480 640 480 Build & Improve Products $108,300 Number of code commits 2,000 100 800 1,100 Advertise & Promote $85,500 Targeted ratio of spending 100 10 65 25 Appendix III: Activity Based Costing Cost Drivers Managed Hosting Public Cloud Marea Cloud Company Equipment Leasing (Direct) $(47,500) $(114,000) $(76,000) $(237,500) Provide Computing Resources $ (45,600.00) $(125,400.00) $(57,000.00) $(228,000) Process Payments $(3,838.00) $(9,386.00) $(5,776.00) $(19,000) Onboard New Customers $(4,346.25) $(30,423.75) $(52,155.00) $(86,925) Support Existing Customers $(49,732.50) $(66,310.00) $(49,732.50) $(165,775) Build & Improve Products $(5,415.00) $(43,320.00) $(59,565.00) $(108,300) Advertise & Promote $(8,550.00) $(55,575.00) $(21,375.00) $(85,500) Total $(164,982) $(444,415) $(321,604) $(931,000) Managed Hosting Public Cloud Marea Cloud Company Sales $191,900 $469,300 $288,800 $950,000 Operating Profit $26,918 $24,885 $(32,804) $19,000 Profit margin 14.03% 5.30% (11.36%) 2.00%","title":"Tidal Cloud Accounting"},{"location":"HBR_Reports/TidalCloud/#implementing-abc-at-tidal-cloud-inc","text":"","title":"Implementing ABC at Tidal Cloud Inc"},{"location":"HBR_Reports/TidalCloud/#background","text":"Tidal Cloud Inc (Tidal) is a hosting provider that offers both managed hosting using physical servers as well as cloud-based hosting with two unique service offerings: public cloud and private cloud (Marea Cloud). Public cloud uses shared cloud servers to manage data, while Marea Cloud offers private servers for each client, a benefit for clients that need to ensure the privacy and security of their data. This company is facing pressure from its investors to increase profit margins as a means to start creating an exit strategy for the company. The end goal is to demonstrate the best profitability they can, to make the most out of selling the company. To help with this process Tidal began heavily investing in cloud infrastructure as that technology became increasingly popular, but their competitors are lowering prices on public cloud services at a time when Tidal's profit margins on those services are already tight. However, they've been able to increase pricing on their private cloud hosting service, Marea Cloud, without a drop in customer base. The managed hosting service is essentially in a \"set it and forget it\" status, where their customer base is stable provided nothing goes wrong, and they're not looking to increase that service due to concerns around resiliency. According to Tidal's existing simple cost system, which allocates a percentage of indirect budget expenses to each service according to the number of virtual servers in use at the end of each month per service, while also accounting for direct costs in the form of equipment leasing. Under that cost system, the company is operating at a 2% profit margin, managed hosting operating at a 3% profit margin, public hosting operating at a loss with -5.6% and Marea Cloud having the best profit margin of 13.7% (see Appendix I). There are two main issues with this simplified costing method, though. First, the managed services don't actually use virtual servers and so their operating expenses have to be assumed. Second, allocation of budget is based on virtual machines, but in the public cloud service, virtual machines may be shared while in the private model they are always individual to the company. As such, allocating based on virtual machine does not account for actual expenses.","title":"Background"},{"location":"HBR_Reports/TidalCloud/#recommendation","text":"By using Activity Based Costing, Tidal would be able to identify true operating costs for each service and get a more accurate picture of their profit margins for each service. Appendix II shows what the activity rate is for each cost driver accounted for within the previously mass-allocated indirect costs, as well as the source data for those rates which shows how many \"units\" of each cost driver the different products used. When multiplying the activity rates by the units of each rate used per product, you get the ABC method of costing, shown in Appendix III. Using this method, the profit margins look very different, with managed hosting operating at a 14.03% profit margin, public cloud operating with a 5.3% profit margin, and Marea Cloud operating at a loss of 11.36%. Using this information, Tidal can make more informed business decisions since what was previously shown as their primary profit driver, Marea Cloud, is actually their loss leader.","title":"Recommendation"},{"location":"HBR_Reports/TidalCloud/#appendices","text":"","title":"Appendices"},{"location":"HBR_Reports/TidalCloud/#appendix-i-simple-cost-system","text":"Product Profitability for Month Ended June 30, 2016 Company Managed Hosting Public Cloud Marea Cloud Sales $950,000 $191,900 $469,300 $288,800 Equipment Leasing $(237,500) $(47,500) $(114,000) $(76,000) Allocated Indirect Costs $(693,500) $(138,700) $(381,425) $(173,375) Operating profit before SG&A $19,000 $5,700 $(26,125) $39,425 Profit Margin 2.0% 3.0% (5.6%) 13.7%","title":"Appendix I: Simple Cost System"},{"location":"HBR_Reports/TidalCloud/#appendix-ii-activity-rates-cost-drivers","text":"Activity Rates Activity Rate Unit Provide Computing Resources $6.33 VM Process Payments $0.02 Dollar Earned Onboard New Customers $72.44 Hour Support Existing Customers $103.61 Help Ticket Build & Improve Products $54.15 Code Commit Advertise & Promote $855.00 Percent of Spending Budgeted Quantity of Activity Driver Core Activities Budgeted Activity Driver (per month) Company Managed Hosting Public Cloud Marea Cloud Provide Computing Resources $228,000 Number of VMs at month end 36,000 7,200 19,800 9,000 Process Payments $19,000 $ Sales $950,000 $191,900 $469,300 $288,800 Onboard New Customers $86,925 Hours spent onboarding new VMs 1,200 60 420 720 Support Existing Customers $165,775 Help tickets addressed 1,600 480 640 480 Build & Improve Products $108,300 Number of code commits 2,000 100 800 1,100 Advertise & Promote $85,500 Targeted ratio of spending 100 10 65 25","title":"Appendix II: Activity Rates &amp; Cost Drivers"},{"location":"HBR_Reports/TidalCloud/#appendix-iii-activity-based-costing","text":"Cost Drivers Managed Hosting Public Cloud Marea Cloud Company Equipment Leasing (Direct) $(47,500) $(114,000) $(76,000) $(237,500) Provide Computing Resources $ (45,600.00) $(125,400.00) $(57,000.00) $(228,000) Process Payments $(3,838.00) $(9,386.00) $(5,776.00) $(19,000) Onboard New Customers $(4,346.25) $(30,423.75) $(52,155.00) $(86,925) Support Existing Customers $(49,732.50) $(66,310.00) $(49,732.50) $(165,775) Build & Improve Products $(5,415.00) $(43,320.00) $(59,565.00) $(108,300) Advertise & Promote $(8,550.00) $(55,575.00) $(21,375.00) $(85,500) Total $(164,982) $(444,415) $(321,604) $(931,000) Managed Hosting Public Cloud Marea Cloud Company Sales $191,900 $469,300 $288,800 $950,000 Operating Profit $26,918 $24,885 $(32,804) $19,000 Profit margin 14.03% 5.30% (11.36%) 2.00%","title":"Appendix III: Activity Based Costing"},{"location":"HBR_Reports/Toliza/","text":"Toliza Museum of Art Current Expensing Structure The Toliza Museum of Art is in the process of reviewing its performance from 2008 when it ran at a deficit in three well-received special exhibits. This deficit impacted the museum's finances as a whole, producing a deficit of almost $1 million rather than the budgeted surplus of just over $2.5 million, as seen in Exhibit 1. This is particularly concerning since the museum sold more admission tickets than anticipated, and also garnered $2 million more in admission fees than expected, as seen in Exhibit 2. While there was a decrease in the average price of each ticket, museum admissions saw an increase of 25% in the actual number of admissions sold as compared to the budgeted number. The increase in quantity of sales more than made up for the decreased price. However, part of the surplus/deficit issue is illuminated by Exhibit 3 which shows that the curatorial staff expenses also saw changes in both pricing and number of hours between what was budgeted and what was actually used. However, although the actual hourly rate was on average $1 less than budgeted, the number of actual hours used were severely increased by almost 30%. Due to this, the actual expenses for the curatorial staff were over $500k more than budgeted. Options for Improving Profitability Option 1: Change the Discounted Rate Although budgeted expenses are vastly different than actual, we do have the option to simply bring in more money through admissions to offset the increased expenses. Discounted admissions are one third of the price of standard admission, but account for almost half the total admissions. Although increasing the price of discounted admissions will necessitate a decrease in users of that admission rate, Exhibit 4 shows that increasing the price to $10 will vastly improve the profitability of museum admissions, even with a decrease of 10% in discounted admission sales. However, this ability to bring in additional funds through adjusting admissions rates does not solve the mystery of the increase in expenses. That's where Activity Based Costing comes in. Option 2: Use Activity Based Costing For activity-based costing, we have direct costs allocated to regular and special exhibits, along with added allocations for curatorial and security activities which do not have direct cost comparatives (Exhibit 5). While there is not a direct cost associated with each exhibit type for curatorial and security activities, those expense pools can be calculated using the average hourly rate for each employee type and the actual hours used for each exhibit type. Exhibit 6 shows the expenses associated with each exhibit type for direct, curatorial, and security costs. With that, it's clear that the regular exhibits do have an increased expense as compared to special exhibits. However, the revenue brought in for each exhibit type, also in Exhibit 6, demonstrate that regular exhibits are operating at a profit of almost $2 million while the special exhibits are operating at a deficit of over $2.5 million. Recommendation Recommendations for making the museum more profitable are two-fold. To start with, I would recommend the museum adjust the price of discounted admission as the increase in revenue will more than offset the decrease in the number of admission tickets sold for that rate. That will provide a stop-gap solution, however the solution will be a temporary one unless the increase in expenses is accounted for. As such, my secondary recommendation would be to thoroughly analyze the profitability of special exhibits as compared to their expense to ensure increased profits are not negated by equally increased expenditures. Exhibits Exhibit 1: Actual vs. Budgeted Revenue and Expense Variance Actual Budgeted Varience F/U Revenue 30,779,500 \u20ac 29,060,000 \u20ac 1,719,500 \u20ac F Expense 31,772,550 \u20ac 26,361,000 \u20ac 5,411,550 \u20ac U Surplus/Defecit (993,050) \u20ac 2,699,000 \u20ac 3,692,050 \u20ac U Exhibit 2: Museum Admission Revenue per Visitor Actual Price Budgeted Price Quantity Varience F/U Price 12.00 \u20ac 12.50 \u20ac 1,000,000 500,000 \u20ac U Actual Quantity Budgeted Quantity Price Varience F/U Quantity 1,000,000 800,000 12.00 \u20ac 2,400,000 \u20ac F Actual Budgeted Varience F/U Admissions Varience 12,000,000 \u20ac 10,000,000 \u20ac 2,000,000 \u20ac F Exhibit 3: Curatorial Staff Expenses Per Hour Actual Price Budgeted Price Quantity Varience F/U Price 19.00 \u20ac 20.00 \u20ac 139,650 139,650 \u20ac F Actual Quantity Budgeted Quantity Price Varience F/U Quantity 139,650 104,000 19.00 \u20ac 677,350 \u20ac U Actual Budgeted Varience F/U Curatorial Staff Varience 2,653,350 \u20ac 2,080,000 \u20ac 573,350 \u20ac U Exhibit 4: Prospective Changes to Discounted Admission Rates Actual Price Budgeted Price Quantity Varience F/U Price 13.61 \u20ac 12.50 \u20ac 970,000 1,075,000 \u20ac F Actual Quantity Budgeted Quantity Price Varience F/U Quantity 970,000 800,000 13.61 \u20ac 2,313,402 \u20ac F Actual Budgeted Varience F/U Admissions Varience 13,200,000 \u20ac 10,000,000 \u20ac 3,200,000 \u20ac F Exhibit 5: Activity Cost Pool Amounts Regular Exhibits Special Exhibits Curatorial Activities Security Activities Total Executive Director & Fringe 122,500 \u20ac 105,000 \u20ac 70,000 \u20ac 52,500 \u20ac 350,000 \u20ac Director of Development & Fringe 115,500 \u20ac 73,500 \u20ac 10,500 \u20ac 10,500 \u20ac 210,000 \u20ac Head Curator & Fringe - \u20ac - \u20ac 175,000 \u20ac - \u20ac 175,000 \u20ac Administrative Staff & Fringe 58,800 \u20ac 50,400 \u20ac 33,600 \u20ac 25,200 \u20ac 168,000 \u20ac Curatorial Staff - \u20ac - \u20ac 2,653,350 \u20ac - \u20ac 2,653,350 \u20ac Security Guards - \u20ac - \u20ac - \u20ac 3,304,100 \u20ac 3,304,100 \u20ac Insurance 3,250,000 \u20ac 3,250,000 \u20ac - \u20ac - \u20ac 6,500,000 \u20ac Maintenance 787,652 \u20ac 387,948 \u20ac - \u20ac - \u20ac 1,175,600 \u20ac Utilities 2,618,250 \u20ac 2,618,250 \u20ac - \u20ac - \u20ac 5,236,500 \u20ac Mortgage 8,040,000 \u20ac 3,960,000 \u20ac - \u20ac - \u20ac 12,000,000 \u20ac Total 14,992,702 \u20ac 10,445,098 \u20ac 2,942,450 \u20ac 3,392,300 \u20ac 31,772,550 \u20ac Exhibit 6: ABC for Regular and Special Exhibits Regular Exhibits Special Exhibits Directly Allocated Costs 18,483,489 \u20ac 12,998,326 \u20ac Curatorial Activities - \u20ac - \u20ac Security Activities - \u20ac - \u20ac Total Costs 18,483,489 \u20ac 12,998,326 \u20ac Revenue 20,385,250 \u20ac 10,394,250 \u20ac Total Costs 18,483,489 \u20ac 12,998,326 \u20ac Surplus/Deficit 1,901,761 \u20ac (2,604,076) \u20ac","title":"Toliza Museum of Art Admission"},{"location":"HBR_Reports/Toliza/#toliza-museum-of-art","text":"","title":"Toliza Museum of Art"},{"location":"HBR_Reports/Toliza/#current-expensing-structure","text":"The Toliza Museum of Art is in the process of reviewing its performance from 2008 when it ran at a deficit in three well-received special exhibits. This deficit impacted the museum's finances as a whole, producing a deficit of almost $1 million rather than the budgeted surplus of just over $2.5 million, as seen in Exhibit 1. This is particularly concerning since the museum sold more admission tickets than anticipated, and also garnered $2 million more in admission fees than expected, as seen in Exhibit 2. While there was a decrease in the average price of each ticket, museum admissions saw an increase of 25% in the actual number of admissions sold as compared to the budgeted number. The increase in quantity of sales more than made up for the decreased price. However, part of the surplus/deficit issue is illuminated by Exhibit 3 which shows that the curatorial staff expenses also saw changes in both pricing and number of hours between what was budgeted and what was actually used. However, although the actual hourly rate was on average $1 less than budgeted, the number of actual hours used were severely increased by almost 30%. Due to this, the actual expenses for the curatorial staff were over $500k more than budgeted.","title":"Current Expensing Structure"},{"location":"HBR_Reports/Toliza/#options-for-improving-profitability","text":"","title":"Options for Improving Profitability"},{"location":"HBR_Reports/Toliza/#option-1-change-the-discounted-rate","text":"Although budgeted expenses are vastly different than actual, we do have the option to simply bring in more money through admissions to offset the increased expenses. Discounted admissions are one third of the price of standard admission, but account for almost half the total admissions. Although increasing the price of discounted admissions will necessitate a decrease in users of that admission rate, Exhibit 4 shows that increasing the price to $10 will vastly improve the profitability of museum admissions, even with a decrease of 10% in discounted admission sales. However, this ability to bring in additional funds through adjusting admissions rates does not solve the mystery of the increase in expenses. That's where Activity Based Costing comes in.","title":"Option 1: Change the Discounted Rate"},{"location":"HBR_Reports/Toliza/#option-2-use-activity-based-costing","text":"For activity-based costing, we have direct costs allocated to regular and special exhibits, along with added allocations for curatorial and security activities which do not have direct cost comparatives (Exhibit 5). While there is not a direct cost associated with each exhibit type for curatorial and security activities, those expense pools can be calculated using the average hourly rate for each employee type and the actual hours used for each exhibit type. Exhibit 6 shows the expenses associated with each exhibit type for direct, curatorial, and security costs. With that, it's clear that the regular exhibits do have an increased expense as compared to special exhibits. However, the revenue brought in for each exhibit type, also in Exhibit 6, demonstrate that regular exhibits are operating at a profit of almost $2 million while the special exhibits are operating at a deficit of over $2.5 million.","title":"Option 2: Use Activity Based Costing"},{"location":"HBR_Reports/Toliza/#recommendation","text":"Recommendations for making the museum more profitable are two-fold. To start with, I would recommend the museum adjust the price of discounted admission as the increase in revenue will more than offset the decrease in the number of admission tickets sold for that rate. That will provide a stop-gap solution, however the solution will be a temporary one unless the increase in expenses is accounted for. As such, my secondary recommendation would be to thoroughly analyze the profitability of special exhibits as compared to their expense to ensure increased profits are not negated by equally increased expenditures.","title":"Recommendation"},{"location":"HBR_Reports/Toliza/#exhibits","text":"","title":"Exhibits"},{"location":"HBR_Reports/Toliza/#exhibit-1-actual-vs-budgeted-revenue-and-expense-variance","text":"Actual Budgeted Varience F/U Revenue 30,779,500 \u20ac 29,060,000 \u20ac 1,719,500 \u20ac F Expense 31,772,550 \u20ac 26,361,000 \u20ac 5,411,550 \u20ac U Surplus/Defecit (993,050) \u20ac 2,699,000 \u20ac 3,692,050 \u20ac U","title":"Exhibit 1: Actual vs. Budgeted Revenue and Expense Variance"},{"location":"HBR_Reports/Toliza/#exhibit-2-museum-admission-revenue-per-visitor","text":"Actual Price Budgeted Price Quantity Varience F/U Price 12.00 \u20ac 12.50 \u20ac 1,000,000 500,000 \u20ac U Actual Quantity Budgeted Quantity Price Varience F/U Quantity 1,000,000 800,000 12.00 \u20ac 2,400,000 \u20ac F Actual Budgeted Varience F/U Admissions Varience 12,000,000 \u20ac 10,000,000 \u20ac 2,000,000 \u20ac F","title":"Exhibit 2: Museum Admission Revenue per Visitor"},{"location":"HBR_Reports/Toliza/#exhibit-3-curatorial-staff-expenses-per-hour","text":"Actual Price Budgeted Price Quantity Varience F/U Price 19.00 \u20ac 20.00 \u20ac 139,650 139,650 \u20ac F Actual Quantity Budgeted Quantity Price Varience F/U Quantity 139,650 104,000 19.00 \u20ac 677,350 \u20ac U Actual Budgeted Varience F/U Curatorial Staff Varience 2,653,350 \u20ac 2,080,000 \u20ac 573,350 \u20ac U","title":"Exhibit 3: Curatorial Staff Expenses Per Hour"},{"location":"HBR_Reports/Toliza/#exhibit-4-prospective-changes-to-discounted-admission-rates","text":"Actual Price Budgeted Price Quantity Varience F/U Price 13.61 \u20ac 12.50 \u20ac 970,000 1,075,000 \u20ac F Actual Quantity Budgeted Quantity Price Varience F/U Quantity 970,000 800,000 13.61 \u20ac 2,313,402 \u20ac F Actual Budgeted Varience F/U Admissions Varience 13,200,000 \u20ac 10,000,000 \u20ac 3,200,000 \u20ac F","title":"Exhibit 4: Prospective Changes to Discounted Admission Rates"},{"location":"HBR_Reports/Toliza/#exhibit-5-activity-cost-pool-amounts","text":"Regular Exhibits Special Exhibits Curatorial Activities Security Activities Total Executive Director & Fringe 122,500 \u20ac 105,000 \u20ac 70,000 \u20ac 52,500 \u20ac 350,000 \u20ac Director of Development & Fringe 115,500 \u20ac 73,500 \u20ac 10,500 \u20ac 10,500 \u20ac 210,000 \u20ac Head Curator & Fringe - \u20ac - \u20ac 175,000 \u20ac - \u20ac 175,000 \u20ac Administrative Staff & Fringe 58,800 \u20ac 50,400 \u20ac 33,600 \u20ac 25,200 \u20ac 168,000 \u20ac Curatorial Staff - \u20ac - \u20ac 2,653,350 \u20ac - \u20ac 2,653,350 \u20ac Security Guards - \u20ac - \u20ac - \u20ac 3,304,100 \u20ac 3,304,100 \u20ac Insurance 3,250,000 \u20ac 3,250,000 \u20ac - \u20ac - \u20ac 6,500,000 \u20ac Maintenance 787,652 \u20ac 387,948 \u20ac - \u20ac - \u20ac 1,175,600 \u20ac Utilities 2,618,250 \u20ac 2,618,250 \u20ac - \u20ac - \u20ac 5,236,500 \u20ac Mortgage 8,040,000 \u20ac 3,960,000 \u20ac - \u20ac - \u20ac 12,000,000 \u20ac Total 14,992,702 \u20ac 10,445,098 \u20ac 2,942,450 \u20ac 3,392,300 \u20ac 31,772,550 \u20ac","title":"Exhibit 5: Activity Cost Pool Amounts"},{"location":"HBR_Reports/Toliza/#exhibit-6-abc-for-regular-and-special-exhibits","text":"Regular Exhibits Special Exhibits Directly Allocated Costs 18,483,489 \u20ac 12,998,326 \u20ac Curatorial Activities - \u20ac - \u20ac Security Activities - \u20ac - \u20ac Total Costs 18,483,489 \u20ac 12,998,326 \u20ac Revenue 20,385,250 \u20ac 10,394,250 \u20ac Total Costs 18,483,489 \u20ac 12,998,326 \u20ac Surplus/Deficit 1,901,761 \u20ac (2,604,076) \u20ac","title":"Exhibit 6: ABC for Regular and Special Exhibits"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/","text":"Housing Price Prediction Introduction For this report, I\u2019ll be working with the Housing Price Prediction dataset , found on Kaggle . Each sample in the dataset represents a single house in an unnamed sqft of India. As shown below, there are no missing values within the dataset and we have a mix of categorical and continuous variables. Each sample is numerically labeled by its row number. To make the calculations easier to understand for a US audience, we\u2019re going to transform the price (currently in Rupees) to USD. The current exchange rate as of this report is 0.012 USD for every 1 rupee, so that will be the calculation used. Additionally, one of the columns (area) is in square feet, so we will rename that column for clarity. Dataset Head 1 2 3 4 5 6 price 13300000 12250000 12250000 12215000 11410000 10850000 sqft 7420 8960 9960 7500 7420 7500 bedrooms 4 4 3 4 4 3 bathrooms 2 4 2 2 1 3 stories 3 4 2 2 2 1 mainroad yes yes yes yes yes yes guestroom no no no no yes no basement no no yes yes yes yes hotwaterheating no no no no no no airconditioning yes yes no yes yes yes parking 2 3 2 3 2 2 prefarea yes no yes yes no yes furnishingstatus furnished furnished semi-furnished furnished furnished semi-furnished price.usd 159600 147000 147000 146580 136920 130200 Each variable represents the following house features: Integers price : The price of the house in rupees. price.usd : The price of the house in USD. sqft : The total sqft of the house in square feet. bedrooms : The number of bedrooms in the house. bathrooms : The number of bathrooms in the house. stories : The number of stories in the house. parking : The number of parking spaces available within the house. Categorical mainroad : Whether the house is connected to the main road (Yes/No). guestroom : Whether the house has a guest room (Yes/No). basement : Whether the house has a basement (Yes/No). hotwaterheating : Whether the house has a hot water heating system (Yes/No). airconditioning : Whether the house has an air conditioning system (Yes/No). prefarea : Whether the house is located in a preferred sqft (Yes/No). furnishingstatus : The furnishing status of the house (Fully Furnished, Semi-Furnished, Unfurnished). Variables of Interest For this report, we will work with a single target variable: price.usd. Since the full dataset has a total of 13 variables (including our targets), we will proactively reduce the number of variables for the sake of simplicity in this project. With that, the variables we will include (including our target) are price.usd, sqft, bedrooms, bathrooms, stories, parking, basement, airconditioning, and prefarea. Analysis The intention of our analysis is to generate a model for predicting the price (in USD) of a given home. The target variable is price.usd and the predictor variables are: sqft (continuous) bedrooms (continuous) bathrooms (continuous) stories (continuous) basementyes (binary dummy variable) airconditioningyes (binary dummy variable) parking (continuous) prefareayes (binary dummy variable) The dummy variables for basement, airconditioning, and prefarea were automatically generated using the lessR regression analysis instruction; the \u201cno\u201d selections were dropped so the variables were updated to include \u201cyes\u201d at the end. A 0 in these fields would indicate \u201cno\u201d while a 1 would indicate \u201cyes\u201d for these variables. Scatterplot As we begin our work, we\u2019ll want to review some basic information on the regression analysis to ensure we\u2019re working with predictors that correlate to the target variable, but not to each other. The first step in that process is to review the scatterplots for each predictor alongside the target (price.usd). As an example, we\u2019ll generate two plots for two predictors: sqft found in and bedrooms. The first set of plots, shown in @ref(fig:lm.scatter), uses the best-fitting least squares line while the other set of plots, shown in @ref(fig:exp.scatter), uses the best-fitting exponential curve. Each plot does appear to fit the data, though the exponential curve has slightly better metrics with R 2 when reviewing the variables on an individual basis: Area : Least Squares Line: Fit: MSE = 359,716,100; R 2 = 0.287 Exponential Curve: MSE = 0.098; R 2 = 0.295 Bedrooms : Least Squares Line: MSE = 436,925,507; R 2 = 0.134 Exponential Curve: MSE = 0.120; R 2 = 0.137 With that, for the sake of this project, we\u2019re going to conduct a logistic regression for the target variable prior to continuing our analysis. To do so, we\u2019re making a logarithmic transformation on the target variable, and will rerun our regression analysis using the transformed variable. Correlation Matrix At this point, we\u2019re going to review our Correlation Coefficient output from the regression analysis. Correlation Coefficients price.usd.log sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes price.usd.log 1.00 0.54 0.37 0.49 0.42 0.22 0.46 0.37 0.34 sqft 0.54 1.00 0.15 0.19 0.08 0.05 0.22 0.35 0.23 bedrooms 0.37 0.15 1.00 0.37 0.41 0.10 0.16 0.14 0.08 bathrooms 0.49 0.19 0.37 1.00 0.33 0.10 0.19 0.18 0.06 stories 0.42 0.08 0.41 0.33 1.00 -0.17 0.29 0.05 0.04 basementyes 0.22 0.05 0.10 0.10 -0.17 1.00 0.05 0.05 0.23 airconditioningyes 0.46 0.22 0.16 0.19 0.29 0.05 1.00 0.16 0.12 parking 0.37 0.35 0.14 0.18 0.05 0.05 0.16 1.00 0.09 prefareayes 0.34 0.23 0.08 0.06 0.04 0.23 0.12 0.09 1.00 From the information provided, we can determine that the predictor variables do relate to the target variable, to differing degrees. Sqft (the house\u2019s area), bathrooms (count of bathrooms in the house), airconditioningyes (if the house has air conditioning) and stories (the number of floors) have the highest correlation scores with reference to our target (price.usd). The lowest score is basementyes (if the house has a basement), at 0.22. Collinearity between predictor variables does not appear to be a problem in this dataset given that the highest correlation coefficient between two predictor variables is 0.41 with most of the scores well below that threshold. As such, based on this cursory review, to predict the price.usd of a given house the final model would likely include sqft, bathrooms, airconditioningyesm and stories. Some of the other variables be included as well, however the actual selection will be dependent on how the interactions between variables are adjusted with further model adjustments. Estimated Model The estimates from our final model are included in the table below. It is important to note that while sqft shows as having a 0.000 estimated slope coefficient to three decimal points, the actual slope coefficient is 0.000056. Although this may seem small, in the scale of our analysis it is actually quite impactful (as will be shown in our hypothesis test). Initial Model Estimates Estimate Std Err t-value p-value Lower 95 Upper 95 (Intercept) 9.956 0.041 241.399 0.000 9.875 10.037 sqft 0.000 0.000 11.929 0.000 0.000 0.000 bedrooms 0.018 0.014 1.265 0.206 -0.010 0.046 bathrooms 0.188 0.020 9.303 0.000 0.149 0.228 stories 0.102 0.012 8.322 0.000 0.078 0.126 basementyes 0.116 0.020 5.770 0.000 0.076 0.155 airconditioningyes 0.168 0.022 7.605 0.000 0.125 0.212 parking 0.054 0.011 4.835 0.000 0.032 0.077 prefareayes 0.158 0.022 7.144 0.000 0.115 0.202 From the information shown above, we can generate an the following regression model. For the model, we are using 6 decimal points rather than 3, to better illustrate the model. This model uses the intercept and the sample slope coefficients for each predictor variable, which will be multiplied by provided values to generate the natural logarithm of the estimated price.usd. To get an actual price.usd prediction, we will need to take the result and provide its inverse (the exponential function). Each sample slope coefficient is an indication of how much the (natural log) price.usd of a house approximately changes within our sample data with a one-unit change in the specified predictor variable, when the other predictor variables held constant (i.e. no change in their values). As an example, the sample slope coefficient of 0.000056 for b s q f t indicates, for these sample data only when all other variables are held constant , every unit (square foot) increase in sqft produces, on average, a 0.000056 increase in the natural log of the house\u2019s price in USD. To provide an example of how the estimated model applies to an actual house, we\u2019ll work with the second row in the dataset, house 2 . That house has the following details: Second row of dataset price.usd price.usd.log sqft bedrooms bathrooms stories basement airconditioning parking prefarea 2 147000 11.89819 8960 4 4 4 no yes 3 no The manually calculated estimation for this house\u2019s price.usd is: The result of this calculation is l o g y\u0302 \u2004=\u200411.99592. When we take the inverse of the log for that value, the estimated house price in USD is approximately 162,092. Do note that the log of price.USD provided in the readout for the house is more accurate, as it is not rounded. However, we\u2019re including these calculations as representations of the process. Since this is just an estimate, there will of course be a residual (difference between the fitted value and the actual value). The residual can be calculated with the formula e i \u2004=\u2004 Y i \u2005\u2212\u2005 Y\u0302 \u2004=\u2004147000\u2005\u2212\u2005162092\u2004=\u2004\u2005\u2212\u200515092, which tells us that when given the predictor variable values for house 2 , our model generates a fitted value that is 15,092 more than the actual value for this house in USD. Hypothesis Test The core of the hypothesis test is to determine if there is a relation between each predictor variable and our target variable (price.usd). What\u2019s being tested is the null hypothesis, that \u03b2 i \u2004=\u20040 and therefore in the population, a change in predictor i for a house does not lead to consistent increases or decreases in the price of a house, when all other predictor variables are held constant. The alternative to the null hypothes is \u03b2 i \u2004\u2260\u20040 and therefore in the population, a change in predictor i for a house does lead to consistent increases or decreases in the price of a house, when all other predictor variables are held constant. Two key values in this test are the t-value and the p-value . The t-value is the number of estimated standard errors \u03b2 i is from the null hypothesized slope (0). The p-value is the probability of obtaining an estimated slope coefficient \u03b2 i with a given t-value ; the cutoff for the p-value is generally \u03b1 \u2004=\u20040.05 so if the p-value is above that cutoff we fail to reject the null hypothesis. For this, we will provide examples using two of the predictor variables: sqft and bedrooms. We will provide the null hypothesis for each, the alternative, and calculations that lead to reject or failing to reject the null hypothesis. Hypothesis Test for Area Null hypothesis: \u03b2 s q f t \u2004=\u20040 Alternative: \u03b2 s q f t \u2004\u2260\u20040 t-value: $t_{sqft} = \\frac{b_{sqft} - 0}{s_{sqft}} = \\frac{0.000056 - 0}{0.000005} = 11.2$ p-value: 0.000 to three decimal points Decision : If the null hypothesis that \u03b2 s q f t \u2004=\u20040 is true, obtaining a value for b sqft more than eleven standard errors from 0 is very unlikely, with a probability of 0.000 to three decimal digits. As such, we reject the null hypothesis for \u03b2 s q f t . Executive Summary : Our calculations from the sample dataset indicate there is a reasonable relationship between the sqft of a house and the price of the house. When the sqft of a house increases, with all other predictor variables unchanged, the price of the house will generally also increase (this can be applied to either the USD or rupee value for the home). Hypothesis Test for Bedrooms Null hypothesis: \u03b2 b e d r o o m s \u2004=\u20040 Alternative: \u03b2 b e d r o o m s \u2004\u2260\u20040 t-value: $t_{bedrooms} = \\frac{b_{bedrooms} - 0}{s_{bedrooms}} = \\frac{0.026280 - 0}{0.014961} = 1.757$ p-value: 0.080 to three decimal points Decision : If the null hypothesis that \u03b2 b e d r o o m s \u2004=\u20040 is true, obtaining a value for b sqft less than two standard errors from 0 is highly likely, with a probability of 0.080 to three decimal digits. As such, we fail to reject the null hypothesis for \u03b2 b e d r o o m s . Executive Summary : Our calculations from the sample dataset indicate there is not a reasonable relationship between the number of bedrooms listed for a house and the price of the house. When the number of bedrooms of a house increases, with all other predictor variables unchanged, there is no reliable impact on the price of the house (this can be applied to either the USD or rupee value for the home). Confidence Interval The confidence interval for the slope coefficient b i provides an estimated range for the value of the population slope coefficient, \u03b2 i . Knowing that in a normal distribution, approximately 95% of values will fall within approximately 2 standard deviations of the mean, the confidence interval uses the estimated slope coefficient b i and it\u2019s standard deviation to determine upper and lower bounds for the population slope coefficient \u03b2 i . As with prior sections, we will work with sqft and bedrooms. Confidence Interval for Area Sample Slope Coefficient: b s q f t \u2004=\u20040.000056 Standard Error: s b \u2005\u2212\u2005 s q f t \u2004=\u20040.000005 Margin of Error: b s q f t \u2005\u00b1\u20052\u2005\u22c5\u2005 s b \u2005\u2212\u2005 s q f t \u2004=\u20040.000056\u2005\u00b1\u20052\u2005\u22c5\u20050.000005 Lower Bound: 0.000056\u2005\u2212\u20052\u2005\u22c5\u20050.000005\u2004=\u20040.000056\u2005\u2212\u20050.00001\u2004=\u20040.000046 Upper Bound: 0.000056\u2005+\u20052\u2005\u22c5\u20050.000005\u2004=\u20040.000056\u2005+\u20050.00001\u2004=\u20040.000066 Executive Summary : Based on our calculations with the provided dataset we can estimate that when the sqft of a house increases by 1 square foot, and no changes are made to other predictor variables, the price of the house will increase within an expected range for about 95% of houses within the main population. Consistancy : The range of values in the confidence interval are all positive values, not including 0, which is consistent with our rejection of the null hypothesis and our resulting determination that b s q f t \u2004\u2260\u20040. Confidence Interval for Bedrooms Sample Slope Coefficient: b b e d r o o m s \u2004=\u20040.026280 Standard Error: s b \u2005\u2212\u2005 b e d r o o m s \u2004=\u200474745.616 Margin of Error: b b e d r o o m s \u2005\u00b1\u20052\u2005\u22c5\u2005 s b \u2005\u2212\u2005 b e d r o o m s \u2004=\u20040.026280\u2005\u00b1\u20052\u2005\u22c5\u20050.014961 Lower Bound: 0.026280\u2005\u2212\u20052\u2005\u22c5\u20050.014961\u2004=\u20040.026280\u2005\u2212\u20050.029922\u2004=\u2004\u2005\u2212\u20050.003642 Upper Bound: 0.026280\u2005+\u20052\u2005\u22c5\u20050.014961\u2004=\u20040.026280\u2005+\u20050.029922\u2004=\u20040.056202 Executive Summary : Based on our calculations with the provided dataset we are unable to provide a reliable estimation on price.usd change for when the number of bedrooms of a house increases and no changes are made to other predictor variables. Consistancy : The range of values in the confidence interval are include 0 as the lower bound is a negative number and the upper bound is positive. This is consistent with our failure to reject the null hypothesis and our resulting determination that b s q f t \u2004=\u20040. Model Fit For the model fit, there are two main elements we\u2019re considering: R 2 , and PRESS R 2 . The values for our model are included below: R 2 : 0.649 PRESS R 2 : 0.637 Best practices for most models needs an R 2 value above .6, which these results display. Additionally, PRESS R 2 , which tests the model against a simluation of testing data, has a score above .6 as well. As such, the model in its current status fits relatively well. However, it\u2019s always worth further analysis to continue refining the model. Outliers The first thing we\u2019ll consider is outliers. There are 11 potential outliers, shown below. We identified these outliers by conducting a search for possible outliers that have at least one of the followign features: a studentized residual greater than 2.5 or less than -2.5, a DFFITS score greater than 0.5, or a Cook\u2019s Distance greater than 0.5. Potential Outliers sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes price.usd fitted resid rstdnt dffits cooks 500 3630 3 3 2 0 0 0 0 10.371 10.942 -0.571 -2.634 -0.503 0.028 532 5300 3 1 1 0 1 0 1 10.196 10.896 -0.700 -3.214 -0.487 0.026 402 9500 3 1 2 0 0 3 1 10.645 11.250 -0.604 -2.779 -0.484 0.026 14 3500 4 2 2 0 0 2 0 11.616 10.910 0.706 3.238 0.448 0.022 541 3000 2 1 1 1 0 2 0 9.991 10.679 -0.688 -3.154 -0.446 0.022 537 3420 5 1 2 0 0 0 0 10.066 10.639 -0.573 -2.632 -0.441 0.021 21 4320 3 1 2 1 0 2 0 11.562 10.885 0.677 3.099 0.401 0.018 510 3600 2 2 2 1 0 1 0 10.344 10.929 -0.585 -2.679 -0.399 0.017 5 7420 4 1 2 1 1 2 0 11.827 11.254 0.573 2.624 0.386 0.016 16 6000 4 1 2 1 0 2 0 11.601 11.005 0.595 2.723 0.382 0.016 28 8875 3 1 1 0 0 1 0 11.521 10.850 0.671 3.067 0.367 0.015 Although there are some potential outliers that could be kept in the model, most are ripe for removal due to their studentized residual scores. Additionally, for the size of our dataset, removing 11 values will provide a minimal impact on the available data. As such, we\u2019ll remove all the potential outliers. To verify the removal, we\u2019ll check what row/house 5 (included in the outlier list) is before and after removing the outliers. 5th Row Before and After Outlier Removal price.usd price.usd.log sqft bedrooms bathrooms stories basement airconditioning parking prefarea Before 136920 11.82715 7420 4 1 2 yes yes 2 no After 130200 11.77683 7500 3 3 1 yes yes 2 yes Model Selection Now that we\u2019ve reviewed various statistics on the model and also removed our outliers, we\u2019ll want to continue reviewing the p-values for each predictor variable as that is a good indicator of if the variable would be beneficial to the overall model. As shown below, when reviewing the p-values for the estimated slope coefficients for each predictor variable, the bedrooms (number of bedrooms) has a p-value well above the cutoff of \u03b1 \u2004=\u20040 at 0.206 (an increase since removing the outliers). As such, that predictor variable is likely to be removed from the model. Revised Model Estimates Estimate Std Err t-value p-value Lower 95 Upper 95 (Intercept) 9.956 0.041 241.399 0.000 9.875 10.037 sqft 0.000 0.000 11.929 0.000 0.000 0.000 bedrooms 0.018 0.014 1.265 0.206 -0.010 0.046 bathrooms 0.188 0.020 9.303 0.000 0.149 0.228 stories 0.102 0.012 8.322 0.000 0.078 0.126 basementyes 0.116 0.020 5.770 0.000 0.076 0.155 airconditioningyes 0.171 0.021 8.294 0.000 0.130 0.211 parking 0.054 0.011 4.835 0.000 0.032 0.077 prefareayes 0.158 0.022 7.144 0.000 0.115 0.202 We also want to consider possible collinearity between the predictor variables, to ensure we\u2019re not over-emphasizing a particular feature. The tolerance and VIF scores for each of the predictor variables, displayed int he following table, are within acceptable ranges (tolerance > 0.2 and VIF < 5) to indicate no issues with collinearity. Predictor Collinearity Tolerance VIF sqft 0.799 1.251 bedrooms 0.732 1.365 bathrooms 0.767 1.303 stories 0.689 1.452 basementyes 0.855 1.169 airconditioningyes 0.852 1.174 parking 0.849 1.177 prefareayes 0.887 1.128 Finally, we can conduct a best subset analysis to identify the best options for the final model. From this analysis, shown below, I would select the second model which uses 7 of the 8 predictor variables (removing bedrooms, as indicated by the p-value previously). This model provides a minimal reduction in the Adjusted R 2 values while providing a reduction on our overall model. With that, we\u2019ll re-run our analysis to remove the predictor that does not directly benefit our efforts. Best Subsets sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes R2adj X\u2019s 1 1 1 1 1 1 1 1 0.682 8 1 0 1 1 1 1 1 1 0.681 7 1 1 1 1 1 1 0 1 0.668 7 1 0 1 1 1 1 0 1 0.667 6 1 1 1 1 0 1 1 1 0.662 7 1 0 1 1 0 1 1 1 0.660 6 1 1 1 1 1 1 1 0 0.651 7 1 0 1 1 1 1 1 0 0.651 6 1 1 1 1 0 1 0 1 0.649 6 1 0 1 1 0 1 0 1 0.646 5 Our reduced model produces the following formula: Prediction Intervals For the final element of this project, we\u2019ll run a further reduced model (due to limitations on using code to predict on more than 6 variables) against the median values for each variable in our dataset with the intent of representing the prediction interval for a given sample. To simplify the model, we will remove the value for basement. Median values for new prediction sqft bathrooms stories airconditioning parking prefarea 4600 1 2 yes 0 no Prediction Intervals for Price on New Data, Natural Log pred s pred pi lwr pi upr 10.850 0.212 10.434 11.268 To calculate our upper and lower prediction intervals, we will use the following formulas: PI lower bound: l o g y\u0302 \u2005\u2212\u2005( t . c u t \u22c5 s p )\u2004=\u200410.850\u2005\u2212\u2005(2\u22c50.212)\u2004=\u200410.426 PI upper bound: l o g y\u0302 \u2005+\u2005( t . c u t \u22c5 s p )\u2004=\u200410.850\u2005+\u2005(2\u22c50.212)\u2004=\u200411.274 Again, though, this is the logarithmic version of the price, so we need to back transform the information to get the actual prediction intervals: PI lower bound: y\u0302 \u2004=\u2004 e x p 10.426\u2004\u2248\u200433725 PI upper bound: y\u0302 \u2004=\u2004 e x p 11.274\u2004\u2248\u200478747 From this information, we can assert that if the trend from the past extends into the future without any additional changes to the underlying dynamics, 95% of all possible prices for a 4,600sqft, 2 story house with 1 bathroom, airconditioning, and no parking that is not in a preferred area will be priced in the range of 33,725 to 78,747 (USD). Do keep in mind that this is an estimate for a regioon outside the US, we are simply using USD for easy of understanding the scale of the pricing structure. Business Application This particular analysis is applicable to a multitude of situations for businesses. Real estate companies can use an analysis like this to identify housing trends and assist with setting the selling or purchase price of a house. Mortgage companies and appraisers can use this kind of data to general comparables to ensure a mortgage is not outsized for the given loan. House flippers can use this kind of information to help identify if they\u2019re making a purchase that will provide a good return on investment after rennovations. Really, anyone who is in the housing market could benefit from having the ability to generate predictions on housing prices based on relevant factors. Conclusion: Summary & Interpretation This analysis allowed us a glimpse into the housing market in an unnamed area of India, where the house prices can vary vastly depending on factors not always considered within the US market. In conducting this analysis, we were able to get an idea of the variance within possible house prices, and also the overall volatility of the market due to the wide ranges by which home prices can vary with identical features. One of the main challenges to this dataset was the fact that it wasn\u2019t based in the US, and so the financial figures had to be adapted to make it easily interpretable to someone used to using USD. Additionally, not being US-based, it is not applicable to situations in which I would eprsonally find myself as I am not planning on moving to India any time soon. Additionally, the dataset was relatively small, with only 525 values. To get a better idea of market prices, additional samples would ideally be included. The data provided was also quite open to interpretation, for example what is the definition of a preferred area? IT was also missing key factors, such as the house type and lot area. Overall, while this analysis could be applied to additional situations, it would need some adjustments by using better suited data and further refinement than I had time or space for within this analysis (i.e. I preemptively removed many of the variables for simplicity, but it would be good to include those initially to get a better picture of the factors that actually influence housing prices in India). Appendix: Code for this report # call needed packages knitr::opts_chunk$set(echo = TRUE, message = FALSE) library(lessR) library(dplyr) library(tidyr) library(rlang) library(stringr) library(glmnet) library(kableExtra) library(formatR) # read in dataset, display details d <- Read(\"data/HOUSING-details.csv\") d$price.usd <- 0.012 * (d$price) colnames(d)[2] <- \"sqft\" details(d) # display dataset head kbl(t(head(d)), booktabs = T, escape = F, align = \"c\", caption = \"Dataset Head\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # conduct regression analysis reg <- reg(price.usd ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate least squares scatterplots on single predictor # variables Plot(sqft, price.usd, fit = \"lm\") Plot(bedrooms, price.usd, fit = \"lm\") # generate exponential curve scatterplots on single # predictor variables Plot(sqft, price.usd, fit = \"exp\") Plot(bedrooms, price.usd, fit = \"exp\") # logarithmic transform price.usd d$price.usd.log <- log(d$price.usd) # verify change details(d) # run new analysis with log transformed target reg.log <- reg(price.usd.log ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate a display of the correlation coefficients reg.cor <- matrix(c(1, 0.54, 0.37, 0.49, 0.42, 0.22, 0.46, 0.37, 0.34, 0.54, 1, 0.15, 0.19, 0.08, 0.05, 0.22, 0.35, 0.23, 0.37, 0.15, 1, 0.37, 0.41, 0.1, 0.16, 0.14, 0.08, 0.49, 0.19, 0.37, 1, 0.33, 0.1, 0.19, 0.18, 0.06, 0.42, 0.08, 0.41, 0.33, 1, -0.17, 0.29, 0.05, 0.04, 0.22, 0.05, 0.1, 0.1, -0.17, 1, 0.05, 0.05, 0.23, 0.46, 0.22, 0.16, 0.19, 0.29, 0.05, 1, 0.16, 0.12, 0.37, 0.35, 0.14, 0.18, 0.05, 0.05, 0.16, 1, 0.09, 0.34, 0.23, 0.08, 0.06, 0.04, 0.23, 0.12, 0.09, 1), nrow = 9) rownames(reg.cor) <- c(reg.log$vars) colnames(reg.cor) <- c(reg.log$vars) kbl(reg.cor, booktabs = T, escape = F, align = \"c\", caption = \"Correlation Coefficients\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # generate a display of the estimates for the dataset reg.est <- matrix(format(c(9.956, 0, 0.018, 0.188, 0.102, 0.116, 0.171, 0.054, 0.158, 0.041, 0, 0.014, 0.02, 0.012, 0.02, 0.021, 0.011, 0.022, 241.399, 11.929, 1.265, 9.303, 8.322, 5.77, 8.294, 4.835, 7.144, 0, 0, 0.206, 0, 0, 0, 0, 0, 0, 9.875, 0, -0.01, 0.149, 0.078, 0.076, 0.13, 0.032, 0.115, 10.037, 0, 0.046, 0.228, 0.126, 0.155, 0.211, 0.077, 0.202), nsmall = 3), nrow = 9) rownames(reg.est) <- c(\"(Intercept)\", reg.log$vars[2:9]) colnames(reg.est) <- c(\"Estimate\", \"Std Err\", \"t-value\", \"p-value\", \"Lower 95\", \"Upper 95\") reg.est[\"airconditioningyes\", ] <- c(0.168, 0.022, 7.605, format(0, nsmall = 3), 0.125, 0.212) kbl(reg.est, booktabs = T, escape = F, align = \"c\", caption = \"Initial Model Estimates\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) kbl(d[2, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")], booktabs = T, escape = F, align = \"c\", caption = \"Second row of dataset\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\")) # generate a display of the potential outliers outliers <- matrix(reg.log$out_residuals[6:25], nrow = 20) outliers <- str_split_fixed(outliers[, 1], \"\\\\s+\", 16) columns <- c(c(reg.log$vars[2:9]), \"price.usd\", \"fitted\", \"resid\", \"rstdnt\", \"dffits\", \"cooks\") rows <- outliers[, 2] outliers <- matrix(round(as.numeric(outliers[, -c(1:2)]), 3), ncol = 14, dimnames = (list(rows, columns))) outliers <- subset(outliers, outliers[, \"rstdnt\"] > 2.5 | outliers[, \"rstdnt\"] < (-2.5) | outliers[, \"dffits\"] > 0.5 | outliers[, \"cooks\"] > 0.5) kbl(outliers, booktabs = T, escape = F, align = \"c\", caption = \"Potential Outliers\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # row 5 before removal before <- d[5, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")] # remove outliers from dataset d <- d[-c(500, 532, 402, 14, 541, 537, 21, 510, 5, 16, 28), ] after <- d[5, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")] # display outlier removal removed <- rbind(before, after) rownames(removed) <- c(\"Before\", \"After\") kbl(removed, booktabs = T, escape = F, align = \"c\", caption = \"5th Row Before and After Outlier Removal\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\")) # run standardized analysis after removing outliers reg.log2 <- reg(price.usd.log ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate a display of the estimates for the dataset reg.est2 <- matrix(format(c(9.956, 0, 0.018, 0.188, 0.102, 0.116, 0.171, 0.054, 0.158, 0.041, 0, 0.014, 0.02, 0.012, 0.02, 0.021, 0.011, 0.022, 241.399, 11.929, 1.265, 9.303, 8.322, 5.77, 8.294, 4.835, 7.144, 0, 0, 0.206, 0, 0, 0, 0, 0, 0, 9.875, 0, -0.01, 0.149, 0.078, 0.076, 0.13, 0.032, 0.115, 10.037, 0, 0.046, 0.228, 0.126, 0.155, 0.211, 0.077, 0.202), nsmall = 3), ncol = 6) rownames(reg.est2) <- c(\"(Intercept)\", reg.log2$vars[2:9]) colnames(reg.est2) <- c(\"Estimate\", \"Std Err\", \"t-value\", \"p-value\", \"Lower 95\", \"Upper 95\") reg.est2[\"airconditioningyes\", ] <- c(0.171, 0.021, 8.294, format(0, nsmall = 3), format(0.13, nsmall = 3), 0.211) kbl(reg.est2, booktabs = T, escape = F, align = \"c\", caption = \"Revised Model Estimates\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) # generate a display of the collinearity measurements reg.col <- matrix(format(c(0.799, 0.732, 0.767, 0.689, 0.855, 0.852, 0.849, 0.887, 1.251, 1.365, 1.303, 1.452, 1.169, 1.174, 1.177, 1.128), nsmall = 3), ncol = 2) rownames(reg.col) <- c(reg.log2$vars[2:9]) colnames(reg.col) <- c(\"Tolerance\", \"VIF\") kbl(reg.col, booktabs = T, escape = F, align = \"c\", caption = \"Predictor Collinearity\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) # generate a display of the best subsets best.subset <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0.682, 0.681, 0.668, 0.667, 0.662, 0.66, 0.651, 0.651, 0.649, 0.646, 8, 7, 7, 6, 7, 6, 7, 6, 6, 5), ncol = 10) colnames(best.subset) <- c(\"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basementyes\", \"airconditioningyes\", \"parking\", \"prefareayes\", \"R2adj\", \"X's\") kbl(best.subset, booktabs = T, escape = F, align = \"c\", caption = \"Best Subsets\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # run standardized analysis removing bedrooms reg.log3 <- reg(price.usd.log ~ sqft + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate new values for prediction interval based on # median values new <- cbind(median(d$sqft), median(d$bathrooms), median(d$stories), \"yes\", median(d$parking), \"no\") colnames(new) <- c(\"sqft\", \"bathrooms\", \"stories\", \"airconditioning\", \"parking\", \"prefarea\") kbl(new, booktabs = T, escape = F, align = \"c\", caption = \"Median values for new prediction\") |> kable_styling(latex_options = c(\"hold_position\")) # run simplified analysis with new data r.new <- reg(price.usd.log ~ sqft + bathrooms + stories + airconditioning + parking + prefarea, X1.new = 4600, X2.new = 1, X3.new = 2, X4.new = 1, X5.new = 0, X6.new = 0) new.pred <- cbind(format(10.85, nsmall = 3), 0.212, 10.434, 11.268) colnames(new.pred) <- c(\"pred\", \"s pred\", \"pi lwr\", \"pi upr\") kbl(new.pred, booktabs = T, escape = F, align = \"c\", caption = \"Prediction Intervals for Price on New Data, Natural Log\") |> kable_styling(latex_options = c(\"hold_position\"))","title":"Housing Price Prediction"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#housing-price-prediction","text":"","title":"Housing Price Prediction"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#introduction","text":"For this report, I\u2019ll be working with the Housing Price Prediction dataset , found on Kaggle . Each sample in the dataset represents a single house in an unnamed sqft of India. As shown below, there are no missing values within the dataset and we have a mix of categorical and continuous variables. Each sample is numerically labeled by its row number. To make the calculations easier to understand for a US audience, we\u2019re going to transform the price (currently in Rupees) to USD. The current exchange rate as of this report is 0.012 USD for every 1 rupee, so that will be the calculation used. Additionally, one of the columns (area) is in square feet, so we will rename that column for clarity. Dataset Head 1 2 3 4 5 6 price 13300000 12250000 12250000 12215000 11410000 10850000 sqft 7420 8960 9960 7500 7420 7500 bedrooms 4 4 3 4 4 3 bathrooms 2 4 2 2 1 3 stories 3 4 2 2 2 1 mainroad yes yes yes yes yes yes guestroom no no no no yes no basement no no yes yes yes yes hotwaterheating no no no no no no airconditioning yes yes no yes yes yes parking 2 3 2 3 2 2 prefarea yes no yes yes no yes furnishingstatus furnished furnished semi-furnished furnished furnished semi-furnished price.usd 159600 147000 147000 146580 136920 130200 Each variable represents the following house features: Integers price : The price of the house in rupees. price.usd : The price of the house in USD. sqft : The total sqft of the house in square feet. bedrooms : The number of bedrooms in the house. bathrooms : The number of bathrooms in the house. stories : The number of stories in the house. parking : The number of parking spaces available within the house. Categorical mainroad : Whether the house is connected to the main road (Yes/No). guestroom : Whether the house has a guest room (Yes/No). basement : Whether the house has a basement (Yes/No). hotwaterheating : Whether the house has a hot water heating system (Yes/No). airconditioning : Whether the house has an air conditioning system (Yes/No). prefarea : Whether the house is located in a preferred sqft (Yes/No). furnishingstatus : The furnishing status of the house (Fully Furnished, Semi-Furnished, Unfurnished).","title":"Introduction"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#variables-of-interest","text":"For this report, we will work with a single target variable: price.usd. Since the full dataset has a total of 13 variables (including our targets), we will proactively reduce the number of variables for the sake of simplicity in this project. With that, the variables we will include (including our target) are price.usd, sqft, bedrooms, bathrooms, stories, parking, basement, airconditioning, and prefarea.","title":"Variables of Interest"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#analysis","text":"The intention of our analysis is to generate a model for predicting the price (in USD) of a given home. The target variable is price.usd and the predictor variables are: sqft (continuous) bedrooms (continuous) bathrooms (continuous) stories (continuous) basementyes (binary dummy variable) airconditioningyes (binary dummy variable) parking (continuous) prefareayes (binary dummy variable) The dummy variables for basement, airconditioning, and prefarea were automatically generated using the lessR regression analysis instruction; the \u201cno\u201d selections were dropped so the variables were updated to include \u201cyes\u201d at the end. A 0 in these fields would indicate \u201cno\u201d while a 1 would indicate \u201cyes\u201d for these variables.","title":"Analysis"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#scatterplot","text":"As we begin our work, we\u2019ll want to review some basic information on the regression analysis to ensure we\u2019re working with predictors that correlate to the target variable, but not to each other. The first step in that process is to review the scatterplots for each predictor alongside the target (price.usd). As an example, we\u2019ll generate two plots for two predictors: sqft found in and bedrooms. The first set of plots, shown in @ref(fig:lm.scatter), uses the best-fitting least squares line while the other set of plots, shown in @ref(fig:exp.scatter), uses the best-fitting exponential curve. Each plot does appear to fit the data, though the exponential curve has slightly better metrics with R 2 when reviewing the variables on an individual basis: Area : Least Squares Line: Fit: MSE = 359,716,100; R 2 = 0.287 Exponential Curve: MSE = 0.098; R 2 = 0.295 Bedrooms : Least Squares Line: MSE = 436,925,507; R 2 = 0.134 Exponential Curve: MSE = 0.120; R 2 = 0.137 With that, for the sake of this project, we\u2019re going to conduct a logistic regression for the target variable prior to continuing our analysis. To do so, we\u2019re making a logarithmic transformation on the target variable, and will rerun our regression analysis using the transformed variable.","title":"Scatterplot"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#correlation-matrix","text":"At this point, we\u2019re going to review our Correlation Coefficient output from the regression analysis. Correlation Coefficients price.usd.log sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes price.usd.log 1.00 0.54 0.37 0.49 0.42 0.22 0.46 0.37 0.34 sqft 0.54 1.00 0.15 0.19 0.08 0.05 0.22 0.35 0.23 bedrooms 0.37 0.15 1.00 0.37 0.41 0.10 0.16 0.14 0.08 bathrooms 0.49 0.19 0.37 1.00 0.33 0.10 0.19 0.18 0.06 stories 0.42 0.08 0.41 0.33 1.00 -0.17 0.29 0.05 0.04 basementyes 0.22 0.05 0.10 0.10 -0.17 1.00 0.05 0.05 0.23 airconditioningyes 0.46 0.22 0.16 0.19 0.29 0.05 1.00 0.16 0.12 parking 0.37 0.35 0.14 0.18 0.05 0.05 0.16 1.00 0.09 prefareayes 0.34 0.23 0.08 0.06 0.04 0.23 0.12 0.09 1.00 From the information provided, we can determine that the predictor variables do relate to the target variable, to differing degrees. Sqft (the house\u2019s area), bathrooms (count of bathrooms in the house), airconditioningyes (if the house has air conditioning) and stories (the number of floors) have the highest correlation scores with reference to our target (price.usd). The lowest score is basementyes (if the house has a basement), at 0.22. Collinearity between predictor variables does not appear to be a problem in this dataset given that the highest correlation coefficient between two predictor variables is 0.41 with most of the scores well below that threshold. As such, based on this cursory review, to predict the price.usd of a given house the final model would likely include sqft, bathrooms, airconditioningyesm and stories. Some of the other variables be included as well, however the actual selection will be dependent on how the interactions between variables are adjusted with further model adjustments.","title":"Correlation Matrix"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#estimated-model","text":"The estimates from our final model are included in the table below. It is important to note that while sqft shows as having a 0.000 estimated slope coefficient to three decimal points, the actual slope coefficient is 0.000056. Although this may seem small, in the scale of our analysis it is actually quite impactful (as will be shown in our hypothesis test). Initial Model Estimates Estimate Std Err t-value p-value Lower 95 Upper 95 (Intercept) 9.956 0.041 241.399 0.000 9.875 10.037 sqft 0.000 0.000 11.929 0.000 0.000 0.000 bedrooms 0.018 0.014 1.265 0.206 -0.010 0.046 bathrooms 0.188 0.020 9.303 0.000 0.149 0.228 stories 0.102 0.012 8.322 0.000 0.078 0.126 basementyes 0.116 0.020 5.770 0.000 0.076 0.155 airconditioningyes 0.168 0.022 7.605 0.000 0.125 0.212 parking 0.054 0.011 4.835 0.000 0.032 0.077 prefareayes 0.158 0.022 7.144 0.000 0.115 0.202 From the information shown above, we can generate an the following regression model. For the model, we are using 6 decimal points rather than 3, to better illustrate the model. This model uses the intercept and the sample slope coefficients for each predictor variable, which will be multiplied by provided values to generate the natural logarithm of the estimated price.usd. To get an actual price.usd prediction, we will need to take the result and provide its inverse (the exponential function). Each sample slope coefficient is an indication of how much the (natural log) price.usd of a house approximately changes within our sample data with a one-unit change in the specified predictor variable, when the other predictor variables held constant (i.e. no change in their values). As an example, the sample slope coefficient of 0.000056 for b s q f t indicates, for these sample data only when all other variables are held constant , every unit (square foot) increase in sqft produces, on average, a 0.000056 increase in the natural log of the house\u2019s price in USD. To provide an example of how the estimated model applies to an actual house, we\u2019ll work with the second row in the dataset, house 2 . That house has the following details: Second row of dataset price.usd price.usd.log sqft bedrooms bathrooms stories basement airconditioning parking prefarea 2 147000 11.89819 8960 4 4 4 no yes 3 no The manually calculated estimation for this house\u2019s price.usd is: The result of this calculation is l o g y\u0302 \u2004=\u200411.99592. When we take the inverse of the log for that value, the estimated house price in USD is approximately 162,092. Do note that the log of price.USD provided in the readout for the house is more accurate, as it is not rounded. However, we\u2019re including these calculations as representations of the process. Since this is just an estimate, there will of course be a residual (difference between the fitted value and the actual value). The residual can be calculated with the formula e i \u2004=\u2004 Y i \u2005\u2212\u2005 Y\u0302 \u2004=\u2004147000\u2005\u2212\u2005162092\u2004=\u2004\u2005\u2212\u200515092, which tells us that when given the predictor variable values for house 2 , our model generates a fitted value that is 15,092 more than the actual value for this house in USD.","title":"Estimated Model"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#hypothesis-test","text":"The core of the hypothesis test is to determine if there is a relation between each predictor variable and our target variable (price.usd). What\u2019s being tested is the null hypothesis, that \u03b2 i \u2004=\u20040 and therefore in the population, a change in predictor i for a house does not lead to consistent increases or decreases in the price of a house, when all other predictor variables are held constant. The alternative to the null hypothes is \u03b2 i \u2004\u2260\u20040 and therefore in the population, a change in predictor i for a house does lead to consistent increases or decreases in the price of a house, when all other predictor variables are held constant. Two key values in this test are the t-value and the p-value . The t-value is the number of estimated standard errors \u03b2 i is from the null hypothesized slope (0). The p-value is the probability of obtaining an estimated slope coefficient \u03b2 i with a given t-value ; the cutoff for the p-value is generally \u03b1 \u2004=\u20040.05 so if the p-value is above that cutoff we fail to reject the null hypothesis. For this, we will provide examples using two of the predictor variables: sqft and bedrooms. We will provide the null hypothesis for each, the alternative, and calculations that lead to reject or failing to reject the null hypothesis.","title":"Hypothesis Test"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#hypothesis-test-for-area","text":"Null hypothesis: \u03b2 s q f t \u2004=\u20040 Alternative: \u03b2 s q f t \u2004\u2260\u20040 t-value: $t_{sqft} = \\frac{b_{sqft} - 0}{s_{sqft}} = \\frac{0.000056 - 0}{0.000005} = 11.2$ p-value: 0.000 to three decimal points Decision : If the null hypothesis that \u03b2 s q f t \u2004=\u20040 is true, obtaining a value for b sqft more than eleven standard errors from 0 is very unlikely, with a probability of 0.000 to three decimal digits. As such, we reject the null hypothesis for \u03b2 s q f t . Executive Summary : Our calculations from the sample dataset indicate there is a reasonable relationship between the sqft of a house and the price of the house. When the sqft of a house increases, with all other predictor variables unchanged, the price of the house will generally also increase (this can be applied to either the USD or rupee value for the home).","title":"Hypothesis Test for Area"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#hypothesis-test-for-bedrooms","text":"Null hypothesis: \u03b2 b e d r o o m s \u2004=\u20040 Alternative: \u03b2 b e d r o o m s \u2004\u2260\u20040 t-value: $t_{bedrooms} = \\frac{b_{bedrooms} - 0}{s_{bedrooms}} = \\frac{0.026280 - 0}{0.014961} = 1.757$ p-value: 0.080 to three decimal points Decision : If the null hypothesis that \u03b2 b e d r o o m s \u2004=\u20040 is true, obtaining a value for b sqft less than two standard errors from 0 is highly likely, with a probability of 0.080 to three decimal digits. As such, we fail to reject the null hypothesis for \u03b2 b e d r o o m s . Executive Summary : Our calculations from the sample dataset indicate there is not a reasonable relationship between the number of bedrooms listed for a house and the price of the house. When the number of bedrooms of a house increases, with all other predictor variables unchanged, there is no reliable impact on the price of the house (this can be applied to either the USD or rupee value for the home).","title":"Hypothesis Test for Bedrooms"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#confidence-interval","text":"The confidence interval for the slope coefficient b i provides an estimated range for the value of the population slope coefficient, \u03b2 i . Knowing that in a normal distribution, approximately 95% of values will fall within approximately 2 standard deviations of the mean, the confidence interval uses the estimated slope coefficient b i and it\u2019s standard deviation to determine upper and lower bounds for the population slope coefficient \u03b2 i . As with prior sections, we will work with sqft and bedrooms.","title":"Confidence Interval"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#confidence-interval-for-area","text":"Sample Slope Coefficient: b s q f t \u2004=\u20040.000056 Standard Error: s b \u2005\u2212\u2005 s q f t \u2004=\u20040.000005 Margin of Error: b s q f t \u2005\u00b1\u20052\u2005\u22c5\u2005 s b \u2005\u2212\u2005 s q f t \u2004=\u20040.000056\u2005\u00b1\u20052\u2005\u22c5\u20050.000005 Lower Bound: 0.000056\u2005\u2212\u20052\u2005\u22c5\u20050.000005\u2004=\u20040.000056\u2005\u2212\u20050.00001\u2004=\u20040.000046 Upper Bound: 0.000056\u2005+\u20052\u2005\u22c5\u20050.000005\u2004=\u20040.000056\u2005+\u20050.00001\u2004=\u20040.000066 Executive Summary : Based on our calculations with the provided dataset we can estimate that when the sqft of a house increases by 1 square foot, and no changes are made to other predictor variables, the price of the house will increase within an expected range for about 95% of houses within the main population. Consistancy : The range of values in the confidence interval are all positive values, not including 0, which is consistent with our rejection of the null hypothesis and our resulting determination that b s q f t \u2004\u2260\u20040.","title":"Confidence Interval for Area"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#confidence-interval-for-bedrooms","text":"Sample Slope Coefficient: b b e d r o o m s \u2004=\u20040.026280 Standard Error: s b \u2005\u2212\u2005 b e d r o o m s \u2004=\u200474745.616 Margin of Error: b b e d r o o m s \u2005\u00b1\u20052\u2005\u22c5\u2005 s b \u2005\u2212\u2005 b e d r o o m s \u2004=\u20040.026280\u2005\u00b1\u20052\u2005\u22c5\u20050.014961 Lower Bound: 0.026280\u2005\u2212\u20052\u2005\u22c5\u20050.014961\u2004=\u20040.026280\u2005\u2212\u20050.029922\u2004=\u2004\u2005\u2212\u20050.003642 Upper Bound: 0.026280\u2005+\u20052\u2005\u22c5\u20050.014961\u2004=\u20040.026280\u2005+\u20050.029922\u2004=\u20040.056202 Executive Summary : Based on our calculations with the provided dataset we are unable to provide a reliable estimation on price.usd change for when the number of bedrooms of a house increases and no changes are made to other predictor variables. Consistancy : The range of values in the confidence interval are include 0 as the lower bound is a negative number and the upper bound is positive. This is consistent with our failure to reject the null hypothesis and our resulting determination that b s q f t \u2004=\u20040.","title":"Confidence Interval for Bedrooms"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#model-fit","text":"For the model fit, there are two main elements we\u2019re considering: R 2 , and PRESS R 2 . The values for our model are included below: R 2 : 0.649 PRESS R 2 : 0.637 Best practices for most models needs an R 2 value above .6, which these results display. Additionally, PRESS R 2 , which tests the model against a simluation of testing data, has a score above .6 as well. As such, the model in its current status fits relatively well. However, it\u2019s always worth further analysis to continue refining the model.","title":"Model Fit"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#outliers","text":"The first thing we\u2019ll consider is outliers. There are 11 potential outliers, shown below. We identified these outliers by conducting a search for possible outliers that have at least one of the followign features: a studentized residual greater than 2.5 or less than -2.5, a DFFITS score greater than 0.5, or a Cook\u2019s Distance greater than 0.5. Potential Outliers sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes price.usd fitted resid rstdnt dffits cooks 500 3630 3 3 2 0 0 0 0 10.371 10.942 -0.571 -2.634 -0.503 0.028 532 5300 3 1 1 0 1 0 1 10.196 10.896 -0.700 -3.214 -0.487 0.026 402 9500 3 1 2 0 0 3 1 10.645 11.250 -0.604 -2.779 -0.484 0.026 14 3500 4 2 2 0 0 2 0 11.616 10.910 0.706 3.238 0.448 0.022 541 3000 2 1 1 1 0 2 0 9.991 10.679 -0.688 -3.154 -0.446 0.022 537 3420 5 1 2 0 0 0 0 10.066 10.639 -0.573 -2.632 -0.441 0.021 21 4320 3 1 2 1 0 2 0 11.562 10.885 0.677 3.099 0.401 0.018 510 3600 2 2 2 1 0 1 0 10.344 10.929 -0.585 -2.679 -0.399 0.017 5 7420 4 1 2 1 1 2 0 11.827 11.254 0.573 2.624 0.386 0.016 16 6000 4 1 2 1 0 2 0 11.601 11.005 0.595 2.723 0.382 0.016 28 8875 3 1 1 0 0 1 0 11.521 10.850 0.671 3.067 0.367 0.015 Although there are some potential outliers that could be kept in the model, most are ripe for removal due to their studentized residual scores. Additionally, for the size of our dataset, removing 11 values will provide a minimal impact on the available data. As such, we\u2019ll remove all the potential outliers. To verify the removal, we\u2019ll check what row/house 5 (included in the outlier list) is before and after removing the outliers. 5th Row Before and After Outlier Removal price.usd price.usd.log sqft bedrooms bathrooms stories basement airconditioning parking prefarea Before 136920 11.82715 7420 4 1 2 yes yes 2 no After 130200 11.77683 7500 3 3 1 yes yes 2 yes","title":"Outliers"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#model-selection","text":"Now that we\u2019ve reviewed various statistics on the model and also removed our outliers, we\u2019ll want to continue reviewing the p-values for each predictor variable as that is a good indicator of if the variable would be beneficial to the overall model. As shown below, when reviewing the p-values for the estimated slope coefficients for each predictor variable, the bedrooms (number of bedrooms) has a p-value well above the cutoff of \u03b1 \u2004=\u20040 at 0.206 (an increase since removing the outliers). As such, that predictor variable is likely to be removed from the model. Revised Model Estimates Estimate Std Err t-value p-value Lower 95 Upper 95 (Intercept) 9.956 0.041 241.399 0.000 9.875 10.037 sqft 0.000 0.000 11.929 0.000 0.000 0.000 bedrooms 0.018 0.014 1.265 0.206 -0.010 0.046 bathrooms 0.188 0.020 9.303 0.000 0.149 0.228 stories 0.102 0.012 8.322 0.000 0.078 0.126 basementyes 0.116 0.020 5.770 0.000 0.076 0.155 airconditioningyes 0.171 0.021 8.294 0.000 0.130 0.211 parking 0.054 0.011 4.835 0.000 0.032 0.077 prefareayes 0.158 0.022 7.144 0.000 0.115 0.202 We also want to consider possible collinearity between the predictor variables, to ensure we\u2019re not over-emphasizing a particular feature. The tolerance and VIF scores for each of the predictor variables, displayed int he following table, are within acceptable ranges (tolerance > 0.2 and VIF < 5) to indicate no issues with collinearity. Predictor Collinearity Tolerance VIF sqft 0.799 1.251 bedrooms 0.732 1.365 bathrooms 0.767 1.303 stories 0.689 1.452 basementyes 0.855 1.169 airconditioningyes 0.852 1.174 parking 0.849 1.177 prefareayes 0.887 1.128 Finally, we can conduct a best subset analysis to identify the best options for the final model. From this analysis, shown below, I would select the second model which uses 7 of the 8 predictor variables (removing bedrooms, as indicated by the p-value previously). This model provides a minimal reduction in the Adjusted R 2 values while providing a reduction on our overall model. With that, we\u2019ll re-run our analysis to remove the predictor that does not directly benefit our efforts. Best Subsets sqft bedrooms bathrooms stories basementyes airconditioningyes parking prefareayes R2adj X\u2019s 1 1 1 1 1 1 1 1 0.682 8 1 0 1 1 1 1 1 1 0.681 7 1 1 1 1 1 1 0 1 0.668 7 1 0 1 1 1 1 0 1 0.667 6 1 1 1 1 0 1 1 1 0.662 7 1 0 1 1 0 1 1 1 0.660 6 1 1 1 1 1 1 1 0 0.651 7 1 0 1 1 1 1 1 0 0.651 6 1 1 1 1 0 1 0 1 0.649 6 1 0 1 1 0 1 0 1 0.646 5 Our reduced model produces the following formula:","title":"Model Selection"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#prediction-intervals","text":"For the final element of this project, we\u2019ll run a further reduced model (due to limitations on using code to predict on more than 6 variables) against the median values for each variable in our dataset with the intent of representing the prediction interval for a given sample. To simplify the model, we will remove the value for basement. Median values for new prediction sqft bathrooms stories airconditioning parking prefarea 4600 1 2 yes 0 no Prediction Intervals for Price on New Data, Natural Log pred s pred pi lwr pi upr 10.850 0.212 10.434 11.268 To calculate our upper and lower prediction intervals, we will use the following formulas: PI lower bound: l o g y\u0302 \u2005\u2212\u2005( t . c u t \u22c5 s p )\u2004=\u200410.850\u2005\u2212\u2005(2\u22c50.212)\u2004=\u200410.426 PI upper bound: l o g y\u0302 \u2005+\u2005( t . c u t \u22c5 s p )\u2004=\u200410.850\u2005+\u2005(2\u22c50.212)\u2004=\u200411.274 Again, though, this is the logarithmic version of the price, so we need to back transform the information to get the actual prediction intervals: PI lower bound: y\u0302 \u2004=\u2004 e x p 10.426\u2004\u2248\u200433725 PI upper bound: y\u0302 \u2004=\u2004 e x p 11.274\u2004\u2248\u200478747 From this information, we can assert that if the trend from the past extends into the future without any additional changes to the underlying dynamics, 95% of all possible prices for a 4,600sqft, 2 story house with 1 bathroom, airconditioning, and no parking that is not in a preferred area will be priced in the range of 33,725 to 78,747 (USD). Do keep in mind that this is an estimate for a regioon outside the US, we are simply using USD for easy of understanding the scale of the pricing structure.","title":"Prediction Intervals"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#business-application","text":"This particular analysis is applicable to a multitude of situations for businesses. Real estate companies can use an analysis like this to identify housing trends and assist with setting the selling or purchase price of a house. Mortgage companies and appraisers can use this kind of data to general comparables to ensure a mortgage is not outsized for the given loan. House flippers can use this kind of information to help identify if they\u2019re making a purchase that will provide a good return on investment after rennovations. Really, anyone who is in the housing market could benefit from having the ability to generate predictions on housing prices based on relevant factors.","title":"Business Application"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#conclusion-summary-interpretation","text":"This analysis allowed us a glimpse into the housing market in an unnamed area of India, where the house prices can vary vastly depending on factors not always considered within the US market. In conducting this analysis, we were able to get an idea of the variance within possible house prices, and also the overall volatility of the market due to the wide ranges by which home prices can vary with identical features. One of the main challenges to this dataset was the fact that it wasn\u2019t based in the US, and so the financial figures had to be adapted to make it easily interpretable to someone used to using USD. Additionally, not being US-based, it is not applicable to situations in which I would eprsonally find myself as I am not planning on moving to India any time soon. Additionally, the dataset was relatively small, with only 525 values. To get a better idea of market prices, additional samples would ideally be included. The data provided was also quite open to interpretation, for example what is the definition of a preferred area? IT was also missing key factors, such as the house type and lot area. Overall, while this analysis could be applied to additional situations, it would need some adjustments by using better suited data and further refinement than I had time or space for within this analysis (i.e. I preemptively removed many of the variables for simplicity, but it would be good to include those initially to get a better picture of the factors that actually influence housing prices in India).","title":"Conclusion: Summary &amp; Interpretation"},{"location":"Housing_Price_Prediction/Housing_Price_Prediction/#appendix-code-for-this-report","text":"# call needed packages knitr::opts_chunk$set(echo = TRUE, message = FALSE) library(lessR) library(dplyr) library(tidyr) library(rlang) library(stringr) library(glmnet) library(kableExtra) library(formatR) # read in dataset, display details d <- Read(\"data/HOUSING-details.csv\") d$price.usd <- 0.012 * (d$price) colnames(d)[2] <- \"sqft\" details(d) # display dataset head kbl(t(head(d)), booktabs = T, escape = F, align = \"c\", caption = \"Dataset Head\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # conduct regression analysis reg <- reg(price.usd ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate least squares scatterplots on single predictor # variables Plot(sqft, price.usd, fit = \"lm\") Plot(bedrooms, price.usd, fit = \"lm\") # generate exponential curve scatterplots on single # predictor variables Plot(sqft, price.usd, fit = \"exp\") Plot(bedrooms, price.usd, fit = \"exp\") # logarithmic transform price.usd d$price.usd.log <- log(d$price.usd) # verify change details(d) # run new analysis with log transformed target reg.log <- reg(price.usd.log ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate a display of the correlation coefficients reg.cor <- matrix(c(1, 0.54, 0.37, 0.49, 0.42, 0.22, 0.46, 0.37, 0.34, 0.54, 1, 0.15, 0.19, 0.08, 0.05, 0.22, 0.35, 0.23, 0.37, 0.15, 1, 0.37, 0.41, 0.1, 0.16, 0.14, 0.08, 0.49, 0.19, 0.37, 1, 0.33, 0.1, 0.19, 0.18, 0.06, 0.42, 0.08, 0.41, 0.33, 1, -0.17, 0.29, 0.05, 0.04, 0.22, 0.05, 0.1, 0.1, -0.17, 1, 0.05, 0.05, 0.23, 0.46, 0.22, 0.16, 0.19, 0.29, 0.05, 1, 0.16, 0.12, 0.37, 0.35, 0.14, 0.18, 0.05, 0.05, 0.16, 1, 0.09, 0.34, 0.23, 0.08, 0.06, 0.04, 0.23, 0.12, 0.09, 1), nrow = 9) rownames(reg.cor) <- c(reg.log$vars) colnames(reg.cor) <- c(reg.log$vars) kbl(reg.cor, booktabs = T, escape = F, align = \"c\", caption = \"Correlation Coefficients\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # generate a display of the estimates for the dataset reg.est <- matrix(format(c(9.956, 0, 0.018, 0.188, 0.102, 0.116, 0.171, 0.054, 0.158, 0.041, 0, 0.014, 0.02, 0.012, 0.02, 0.021, 0.011, 0.022, 241.399, 11.929, 1.265, 9.303, 8.322, 5.77, 8.294, 4.835, 7.144, 0, 0, 0.206, 0, 0, 0, 0, 0, 0, 9.875, 0, -0.01, 0.149, 0.078, 0.076, 0.13, 0.032, 0.115, 10.037, 0, 0.046, 0.228, 0.126, 0.155, 0.211, 0.077, 0.202), nsmall = 3), nrow = 9) rownames(reg.est) <- c(\"(Intercept)\", reg.log$vars[2:9]) colnames(reg.est) <- c(\"Estimate\", \"Std Err\", \"t-value\", \"p-value\", \"Lower 95\", \"Upper 95\") reg.est[\"airconditioningyes\", ] <- c(0.168, 0.022, 7.605, format(0, nsmall = 3), 0.125, 0.212) kbl(reg.est, booktabs = T, escape = F, align = \"c\", caption = \"Initial Model Estimates\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) kbl(d[2, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")], booktabs = T, escape = F, align = \"c\", caption = \"Second row of dataset\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\")) # generate a display of the potential outliers outliers <- matrix(reg.log$out_residuals[6:25], nrow = 20) outliers <- str_split_fixed(outliers[, 1], \"\\\\s+\", 16) columns <- c(c(reg.log$vars[2:9]), \"price.usd\", \"fitted\", \"resid\", \"rstdnt\", \"dffits\", \"cooks\") rows <- outliers[, 2] outliers <- matrix(round(as.numeric(outliers[, -c(1:2)]), 3), ncol = 14, dimnames = (list(rows, columns))) outliers <- subset(outliers, outliers[, \"rstdnt\"] > 2.5 | outliers[, \"rstdnt\"] < (-2.5) | outliers[, \"dffits\"] > 0.5 | outliers[, \"cooks\"] > 0.5) kbl(outliers, booktabs = T, escape = F, align = \"c\", caption = \"Potential Outliers\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # row 5 before removal before <- d[5, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")] # remove outliers from dataset d <- d[-c(500, 532, 402, 14, 541, 537, 21, 510, 5, 16, 28), ] after <- d[5, c(\"price.usd\", \"price.usd.log\", \"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basement\", \"airconditioning\", \"parking\", \"prefarea\")] # display outlier removal removed <- rbind(before, after) rownames(removed) <- c(\"Before\", \"After\") kbl(removed, booktabs = T, escape = F, align = \"c\", caption = \"5th Row Before and After Outlier Removal\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\")) # run standardized analysis after removing outliers reg.log2 <- reg(price.usd.log ~ sqft + bedrooms + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate a display of the estimates for the dataset reg.est2 <- matrix(format(c(9.956, 0, 0.018, 0.188, 0.102, 0.116, 0.171, 0.054, 0.158, 0.041, 0, 0.014, 0.02, 0.012, 0.02, 0.021, 0.011, 0.022, 241.399, 11.929, 1.265, 9.303, 8.322, 5.77, 8.294, 4.835, 7.144, 0, 0, 0.206, 0, 0, 0, 0, 0, 0, 9.875, 0, -0.01, 0.149, 0.078, 0.076, 0.13, 0.032, 0.115, 10.037, 0, 0.046, 0.228, 0.126, 0.155, 0.211, 0.077, 0.202), nsmall = 3), ncol = 6) rownames(reg.est2) <- c(\"(Intercept)\", reg.log2$vars[2:9]) colnames(reg.est2) <- c(\"Estimate\", \"Std Err\", \"t-value\", \"p-value\", \"Lower 95\", \"Upper 95\") reg.est2[\"airconditioningyes\", ] <- c(0.171, 0.021, 8.294, format(0, nsmall = 3), format(0.13, nsmall = 3), 0.211) kbl(reg.est2, booktabs = T, escape = F, align = \"c\", caption = \"Revised Model Estimates\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) # generate a display of the collinearity measurements reg.col <- matrix(format(c(0.799, 0.732, 0.767, 0.689, 0.855, 0.852, 0.849, 0.887, 1.251, 1.365, 1.303, 1.452, 1.169, 1.174, 1.177, 1.128), nsmall = 3), ncol = 2) rownames(reg.col) <- c(reg.log2$vars[2:9]) colnames(reg.col) <- c(\"Tolerance\", \"VIF\") kbl(reg.col, booktabs = T, escape = F, align = \"c\", caption = \"Predictor Collinearity\") |> kable_styling(latex_options = c(\"hold_position\", \"striped\")) # generate a display of the best subsets best.subset <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0.682, 0.681, 0.668, 0.667, 0.662, 0.66, 0.651, 0.651, 0.649, 0.646, 8, 7, 7, 6, 7, 6, 7, 6, 6, 5), ncol = 10) colnames(best.subset) <- c(\"sqft\", \"bedrooms\", \"bathrooms\", \"stories\", \"basementyes\", \"airconditioningyes\", \"parking\", \"prefareayes\", \"R2adj\", \"X's\") kbl(best.subset, booktabs = T, escape = F, align = \"c\", caption = \"Best Subsets\") |> kable_styling(latex_options = c(\"hold_position\", \"scale_down\", \"striped\")) # run standardized analysis removing bedrooms reg.log3 <- reg(price.usd.log ~ sqft + bathrooms + stories + basement + airconditioning + parking + prefarea) # generate new values for prediction interval based on # median values new <- cbind(median(d$sqft), median(d$bathrooms), median(d$stories), \"yes\", median(d$parking), \"no\") colnames(new) <- c(\"sqft\", \"bathrooms\", \"stories\", \"airconditioning\", \"parking\", \"prefarea\") kbl(new, booktabs = T, escape = F, align = \"c\", caption = \"Median values for new prediction\") |> kable_styling(latex_options = c(\"hold_position\")) # run simplified analysis with new data r.new <- reg(price.usd.log ~ sqft + bathrooms + stories + airconditioning + parking + prefarea, X1.new = 4600, X2.new = 1, X3.new = 2, X4.new = 1, X5.new = 0, X6.new = 0) new.pred <- cbind(format(10.85, nsmall = 3), 0.212, 10.434, 11.268) colnames(new.pred) <- c(\"pred\", \"s pred\", \"pi lwr\", \"pi upr\") kbl(new.pred, booktabs = T, escape = F, align = \"c\", caption = \"Prediction Intervals for Price on New Data, Natural Log\") |> kable_styling(latex_options = c(\"hold_position\"))","title":"Appendix: Code for this report"},{"location":"Traffic_Analysis/Traffic_Analysis/","text":"SpeedWatch: Predictive Analysis of Speeding Incidences by Vehicle Type, Location, and Time Authors:Morgan Mohan, David Moroney, Ashley Nilson, Anita Pathak Executive Summary Portland's roadways are grappling with a significant challenge: an uptick in speeding incidents despite the efforts of the Portland Transportation Department to enforce speed limit regulations. These incidents present a clear threat to road safety, resulting in accidents, injuries, and even fatalities. To tackle this pressing issue, our project undertook a comprehensive analysis of data sourced from the Portland Transportation Department, focusing on speed, volume, and classes of vehicles. Our methodology encompassed several key stages. Initially, we merged datasets related to speed, traffic volume, and vehicle classes. Through extensive exploratory data analysis, we sought to understand the distribution of speed, traffic behavior by day, disparities between posted speed limits and actual speeds, and historical trends in overspeeding. Feature selection was then carried out to identify relevant variables for modeling and an initial Decision Tree model was used to further reduce predictor varaibles. Subsequently, we implemented Logistic Regression, K-Nearest Neighbors, and a Stacked Ensemble model to predict speeding incidents. Our analysis yielded critical insights into the nature and extent of speeding incidents on Portland's roadways. We identified the top locations where vehicles consistently exceeded posted speed limits and observed fluctuations in overspeeding trends over the years. Notably, certain areas such as SE Division St E of SE 33rd Ave ranked highest for mean speed exceeding limits, indicating areas of particular concern. Moreover, we identified key predictors for speeding behaviors, including traffic volume and time of day. Predictive modeling using Decision Tree Classifier and Logistic Regression demonstrated strong performance metrics, with the logistic regression model achieving an accuracy of 0.802 and a recall of 1 for both training and testing datasets. These findings have significant implications for improving road safety and transportation management in Portland. By pinpointing areas with high instances of speeding and understanding the factors driving these behaviors, transportation authorities can implement targeted interventions to mitigate risks and enhance road safety. Possible interventions include deploying speed enforcement measures such as speed cameras or increased police patrols in identified hotspot areas. Additionally, insights gleaned from predictive modeling can inform the development of proactive strategies to anticipate and address future challenges related to speeding incidents. Our project underscores the importance of data-driven approaches in tackling speeding incidents and enhancing road safety. By leveraging comprehensive analysis of traffic data, stakeholders can make informed decisions to optimize transportation management and improve the quality of life for Portland residents. Moving forward, continued collaboration between transportation authorities, law enforcement agencies, and urban planners will be essential for the successful implementation of road safety initiatives. Introduction Speeding incidents on Portland's roadways have become a prominent and persistent challenge, despite concerted efforts by the Portland Transportation Department to enforce speed limit regulations. These incidents not only pose a threat to public safety but also impede effective mitigation measures. In response to this ongoing issue, our project endeavors to conduct a thorough analysis of data sourced from the Portland Transportation Department, focusing specifically on variables such as speed, volume, and vehicle classifications. The urgency of addressing this issue cannot be overstated, as speeding incidents contribute significantly to road safety concerns, including a rise in accidents, injuries, and fatalities. Despite the department's efforts, instances of vehicles surpassing assigned speed limits persist, necessitating a deeper understanding of the underlying factors driving such behavior. By merging and analyzing datasets related to speed, volume, and vehicle classes, our project aims to identify key areas where drivers tend to overspeed and discern trends in overspeeding over time. Additionally, we seek to explore correlations between speeding incidents and various factors such as location, types of vehicles, time of day, and speed limits. Ultimately, the objective of our analysis is to provide actionable insights that can support the Portland Transportation Department's efforts in reducing speeding incidents, enhancing road safety, and ultimately improving the quality of life for Portland residents. Through a data-driven approach, we aim to uncover valuable information that can inform targeted interventions and strategies to address the root causes of speeding behaviors on Portland's roadways. By emphasizing the gravity of the issue and the necessity for a comprehensive analysis, this introduction sets the stage for the subsequent sections of our project, which delve into our methodology, findings, and recommendations. Problem Statement What are the significant threats to road safety posed by the prevalence of speeding vehicles in Portland despite existing speed limit regulations, instances of vehicles exceeding assigned speed limits persist, leading to a range of road safety issues including accidents, injuries, and fatalities. What are the underlying factors contributing to speeding behaviors hampers effective mitigation efforts by the Portland Transportation Department. The underlying factors contributing to speeding behaviors are multifaceted, encompassing individual, environmental, and societal factors. These include driver attitudes, perceptions of risk, roadway characteristics, inadequate signage, societal norms, and limited enforcement resources. These factors collectively create a complex landscape that hampers the department's ability to effectively address speeding incidents. In light of these challenges, the problem statement for this report is to comprehensively analyze data sourced from the Portland Transportation Department to identify areas where drivers tend to overspeed and discern trends in overspeeding over the years. Additionally, the report aims to explore correlations between speeding and various factors such as location, types of vehicles, time, and speed limits. Ultimately, the analysis aims to provide actionable insights to support efforts in reducing speeding incidents, enhancing road safety, and improving the quality of life for Portland residents. Methodology Data Source & Description The data was compiled from three CSV files, all of which were obtained from the City of Portland, Oregon's PortlandMaps Open Data system , powered by ArcGIS. The three CSVs were from: Traffic Volume Count : Information on the volume of traffic flow observed at various locations within Portland. It includes data on the number of vehicles passing through specific points over designated time periods, offering insights into traffic patterns and congestion levels. Vehicle Class Count : The distribution of vehicles by class or type observed on Portland's roadways. It categorizes vehicles into different classes such as cars, trucks, motorcycles, and bicycles, providing valuable information on the composition of the traffic fleet. Traffic Speed Count Count : Data on the speeds at which vehicles are traveling along different segments of Portland's road network. It includes information on both average and individual vehicle speeds, allowing for the analysis of speeding trends and patterns across various locations and times. Each CSV had multiple shared variables, mostly related to the date, location, and other identifying elements of each sample count. The different CSV's each had their own unique variables, as well, that were specific to what was being counted. For example, Traffic Volume Count provides details on AM volume vs. PM volumes, Vehicle Class Count includes percentages of trucks vs. cars, and Traffic Speed Count Count provides percentages of passing traffic that fall into ranges based on the local speed limit. Data Preparation Prior to conducting any sort of analysis, we needed to clean and combine the three CSV files. This process incolved the following steps: Read in Data After reading in each CSV as its own dataframe, we dropped the following columns in each dataframe: * StartTime * EndTime * ChannelNum * NumChannels * Comment * LocationClass * Conditions * DeviceRef These columns were specifically selected because they were either duplicative, had minimal value to our end analysis, or frequently had no data. Establish IDs In reviewing the dataframes we were able to establish that the same sample counts were used to provide the data for each CSV. This was determined because the values for different identifiers, such as location and date, were the same when the samples between different dataframe had matching values for the beginning of CountID (a CSV-specific identifier composed of 8 digits followed by by a CSV label) and Bound (the traffic direction). With that information, we were able to establish identifiers that connected samples between the three dataframes. To do so, we cleaned the Bound values as they had varying capitalization and other structural differences, then connected that value to the beginning digits of the CountID. For example, the same sample count in each dataframe had the following CountID and Bound values: * Speed: 10010422.SP2 \u2013 E * Class: 10010422.CL2 \u2013 E * Volume: 10010422.VL2 \u2013 E We were then able to create a unique ID in each dataframe for these samples that matched across the three dataframes: 10010422_E We needed to include the Bound element because the same CountID would be used multiple times for cross-directional traffic. For example, in the instance provided above, each dataframe had another sample with an identical CountID, but the Bound value was W (rather than E). As such, that sample was given the ID of 10010422_W. Merge Dataframes After the unique identifiers were established, we could merge the three dataframes into one. For this, we chose the newly created ID column, along with StartDate and EndDate as the merge points, while all other duplicated columns were given a suffix that included the source dataframe's column name. This ensured we could continue reviewing the unique information in each dataframe prior to removing duplicative details. After the initial merge, there were 440 samples with duplicate IDs from a total of 22,736 samples. As such, we dropped the rows with duplicate IDs. Following the removal of the duplicated IDs, we iterated through the dataframe to fill in NAN values in columns repeated in multiple dataframes with the values from the other dataframes (i.e. if the Speed dataframe was missing a value in LocationID, but the Volume dataframe contained that value, then the value from Volume replaced the NAN for Speed). Once we ensured there were no missing values in any of the duplicated variables, and ran through the input values to check for consistancy, we selected the columns imported from the Speed dataframe as the \"source of truth\" and removed the redundant columns. Following this, we reviewed samples with missing values that could be calculated and filled in those values where it made sense. For example, the values in PctCars and PctTrucks always added up to 1, so if one value was missing it could be calculated by subtracting the provided value from 1. Lastly, we removed remaining samples with NAN values. This had the impact of reducing the final dataframe from about 20,000 samples to just over 7,000. However, a large reason for this is that the Class dataframe had just over 7,000 samples so there was a large discrepency between that dataframe's samples and the other two. To ensure we didn't have large gaps in our timeline data, though, we also ran an analysis comparing the four dataframes (Speed, Volume, Class, and the merged dataframe) and their relative counts by turning the counts for each month into a percent of the total count for that dataframe. As shown in the gragh below, the darker blue line representing the merged dataframe has the same approximate amount of counts each month as the other three, even though the final total count is much smaller: Data Exploration Prior to feature selection the newly merged data frame was explored for further understanding. Firstly, the distribution of values within the Posted Speed and PctOverPosted columns were explored utilizing the pyplot package for boxplots and simple IQR calculations to remove outliers. The following boxplots depict PctOverPosted and PostedSpeed ebfore and after outlier removal: Next, aggregate statistics were calculated by Start Day for PctOverPosted, PctOverPosted10, and ADTVolume to explore the variation in speeding and traffic behavior by day. In order to better understand the difference between posted speed limits and the reported speed percentiles, differences were calculated for each data point. The top 15 largest differences are displayed below, grouped by location description: Finally, the following graphs depicting PctOverPosted and PctOverPosted10 over month and year timeframes were created using pyplot to uncover any potential historical or seasonal trends within the data: Feature Selection PctOverPosted10 was identified as the initial target variable for this analysis however, after early modelling, the team determined to create a new binary classification from PctOverPosted10 for greater simplicity and explainability. This binary variable is 1 for any row with a PctOverPosted10 greater than 0, otherwise 0. In other words, classifying any measurements where speeds 10MPH or greater over the speed limit were measured. This new binary variable was named 10OverSpeeding and was utilized as the target variable in the final analysis. Future research using this same dataset could vary the cuytoff threshold for this binary variable in order to better target the most severe speeding cases. An initial list of features of interest was defined including the following variables: 10OverSpeeding, AMPkHrVol, AMVolume, PMPkHrVol, PMVolume, PctCars, PostedSpeed, X, ADTVolume, Y, StartDay, and Month. Dummy variables were created for StartDay using the pandas get_dummies() function. These features were then run through automated selection where scikit-learn was used to rank them by importance in the Recursive Feature Elimination (RFE) process with a Decision Tree classifier as the estimator. The top 10 features were maintained and the following final correlation matrix was generated using matplotlib and seaborn for heatmap visualization: Modeling Decision Tree The first model analyzed in this project was a Decision Tree Classifier. First, the data were split into training and testing sets using scikit-learn's train_test_split function for the features defined in the previous section. The data was divided into 80% training and 20% testing with stratification to ensure the class distribution between sets was not imbalanced. Next, scikit-learn's Grid Search Cross-Validation was used to determine the ideal parameters for the Decision Tree. The Tree was then fit on the training data, and predicted on the training and testing sets. The accuracy of the model was assessed using scikit-learn's accuracy_score, precision_score, and recall_score for both training and testing sets. A confusion matrix was also generated for both sets using pyplot and seaborn packages, as shown here: Based on the Decision Tree rules, the predictive features for the next models were narrowed down from 10 to just the top 5. The feature's importance was calculated and then ranked. The data was then scaled using scikit-learn's StandardScaler and the training and testing sets were redefined using the new top 5 features. Logistic Regression A Logistic Regression model was fit on the training data for the top 5 features defined above. The data was divided into 80% training and 20% testing sets with stratification just like the previous Decision Tree model. Predictions were then made on both the training and testing sets. To evaluate the model's performance and ability to generalize on new data confusion matrices plus accuracy, precision, and recall scores were calculated and printed for both sets. The following confusion matrix depicts the predictive outcomes from the Logistic Regressions's testing set: K-Nearest Neighbors Following the procedure of the Logistic Regression analysis, a K-NN Classification model was also utilized. The K-NN model was fit using the same training set as defined above and then predictions were made on both the training and testing sets to assess performance and generalizability. Predictions from the model were then visualized using confusion matrices, with the testing matrix shown here: Stacked Ensemble The final model was a stacked ensemble, which combined the three preceding models to develop a robust model encompassing the strengths of the prior three. With the model, we were able to attain the following results: * Accuracy: .863 * Precision: .89 * Recall: .947 The confusion matrix shown here demonstrates the overall model reliability: The final performance indicators for each model, shown below, demonstrate the benefit of using the stacked ensemble, with the prior three models as its backbone, for our final model: \\begin{array}{ c c c c } \\textbf{Model} & \\textbf{Accuracy} & \\textbf{Precision} & \\textbf{Recall} \\ Decision Tree & 0.836 & 0.883 & 0.917 \\ Logistic Regression & 0.802 & 0.802 & 1.0 \\ KNN Classifier & 0.828 & 0.884 & 0.905 \\ Stacked Ensemble & 0.863 & 0.890 & 0.947 \\end{array} Analysis & Findings The analysis and findings reveal critical insights into speeding incidents on Portland's roadways, derived from comprehensive examination of traffic volume, vehicle class, and traffic speed data sets. Through this analysis, we identified the top 15 locations with the greatest mean speed exceeding posted limits, comparing them with overspeed percentiles. Notably, SE Division St E of SE 33rd Ave ranked highest, with SE 112th Ave S of SE Ogden St and Columbia Way - Columbia Blvd Ramp W of N Columbia Blvd following closely. Additionally, we examined the top 15 locations with the highest speeds for the 90th percentile, further highlighting areas of concern for overspeeding. Moreover, historical trends from 2010 to 2023 indicated fluctuations in overspeeding, with peak incidents occurring during AM and PM rush hours. Predictive modeling using decision trees, logistic regression, K-NN, and stacked ensembles identified key predictors such as AM Volume, X, Y, ADT Volume, and Pct Cars. Our models demonstrated strong performance metrics, with logistic regression showing an accuracy of 0.802, precision of 0.802, and recall of 1 for both training and testing data sets. Similarly, the KNN classifier exhibited robust performance with comparable metrics. The greatest overall accuracy was seen in the stacked ensemble model which had an accuracy of 0.863, precision of 0.890, and recall of 0.947. The comparison of all models yielded valuable insights, with the stacked ensemble having the greatest overall performance. This high performance on the testing datasets indicates that the models are able to generalize well to new data. This could make them useful tools for predicting whether overspeeding incidences will occur based on key traffic and location metrics. Overall, these findings underscore the importance of data-driven approaches in understanding and addressing speeding behaviors to enhance road safety in Portland. Business Applications The findings and analysis presented in this project hold significant implications for improving road safety and transportation management in Portland. By identifying key locations with high instances of speeding and understanding the underlying factors contributing to these behaviors, transportation authorities can implement targeted interventions to mitigate risks and enhance road safety. These interventions may include the deployment of speed enforcement measures, such as speed cameras or increased police patrols, in identified hotspot areas. Additionally, the insights gained from historical trends and predictive modeling can inform the development of proactive strategies to anticipate and address future challenges related to speeding incidents. The identification of key predictors for speeding behaviors, such as traffic volume and time of day, can inform urban planning and infrastructure development initiatives. For instance, city planners can use this information to optimize traffic flow and design safer roadways that effectively accommodate varying traffic conditions. Moreover, the predictive models developed in this project can serve as valuable decision support tools for transportation agencies, enabling them to allocate resources more efficiently and prioritize interventions based on the likelihood of speeding incidents occurring in specific locations. Recommendations Based on the analysis and findings presented in this project, several key recommendations can be made to Portland's Bureau of Transportation to improve road safety and address the issue of speeding incidents. Firstly, targeted enforcement measures may be implemented using the insights gained from the identification of high-speeding locations. Enforcement measures could include speed cameras or increased police presence in hotspot areas aimed at deterring speeding behaviors. Secondly, utilizing insights from traffic volume and vehicle classes, infrastructure investments may be made in targeted areas to calm traffic using features such as speed bumps or traffic islands. Most importantly, this project acts as a proof of concept that data driven decision-making is possible from the wealth of data collected by the City of Portland. Our key recommendation is that PBOT continues to update and maintain this data, and further supplementing it with other relevant factors to inform decision-making processes related to road safety and transportation management. While the data currently available provides some helpful insights, any additional data which can be incorporated for speed enforcement measures, infrastructure updates, or other key influences will be crucial for measuring and predicting future success. Finally, PBOT should continue to collaborate closely with community stakeholders and provide regular data updates and reports. Collaborative efforts can help build consensus around proposed interventions and ensure their effectiveness in addressing community concerns. Conclusion The project \"Predictive Analysis of Speeding Incidents by Vehicle Type, Location, and Time\" has provided valuable insights into the pressing issue of speeding incidents on Portland's roadways. Through comprehensive data analysis and predictive modeling, we have identified key factors contributing to speeding behaviors, highlighted high-speeding locations, and assessed historical trends in speeding incidents over time. Our findings underscore the urgent need for targeted interventions to address speeding and enhance road safety in Portland. This project has laid the foundation for evidence-based interventions to improve road safety and enhance the quality of life for Portland residents and visitors. By leveraging data analytics and predictive modeling, we can work towards creating a safer and more sustainable transportation system that benefits all members of the community. References Traffic Counts Portland.gov NE Ainsworth Traffic Calming Projects Code Appendix Preliminaries # import packages import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from tabulate import tabulate from sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor from datetime import datetime as dt from sklearn.feature_selection import RFE from sklearn.ensemble import StackingClassifier from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, precision_score, recall_score from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from prettytable import PrettyTable # set analysis date/time now = dt.now() print (\"Analysis on\", now.strftime(\"%Y-%m-%d\"), \"at\", now.strftime(\"%H:%M\")) Analysis on 2024-07-27 at 10:52 Analysis on 2024-07-27 at 10:52 Data Preparation # read in and name speed csv SpeedDF = pd.read_csv('data/TRAFFIC-Speed_Counts.csv') SpeedDF.name = 'SpeedDF' SpeedDF.head().transpose() <positron-console-cell-8>:2: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663930.6133 -13663930.6133 -13663904.8261 Y 5722466.9793 5722466.9793 5711463.6992 5711463.6992 5711466.2873 OBJECTID 1 2 3 4 5 CountID 10010422.SP2 10010422.SP2 10010430.sp2 10010430.sp2 10010421.SP1 ChannelID 505638 505639 505648 505649 505642 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW 61ST AVE S of FRONT AVE NW 61ST AVE S of FRONT AVE NW FRONT AVE E of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 StartDay MON MON MON MON MON StartTime 12:00:00 12:00:00 13:00:00 13:00:00 13:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 EndDay TUE TUE TUE TUE WED EndTime 23:00:00 23:00:00 23:00:00 23:00:00 14:00:00 ADTVolume 3048 3405 847 852 1524 Spd50th 39 39 20 19 33 Spd70th 43 44 23 22 37 Spd85th 46 48 26 25 42 Spd90th 47 49 27 26 44 PostedSpeed 45 45 25 25 40 PctOverPosted 17.9 25.7 17.7 14.1 20.6 PctOverPosted10 0.6 1.5 0.2 0.2 2.2 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 2 2 1 ChannelNum 1 2 1 2 1 NumSlots 13 13 13 13 13 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 36 36 35 35 50 IntervalLen 60 60 60 60 60 DeviceRef 0022 0022 0030 0030 0021 LocationID LEG11527 LEG11527 LEG36313 LEG36313 LEG37236 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW 61ST AVE W/NW FRONT AVE NW 61ST AVE W/NW FRONT AVE NW FRONT AVE S/NW 61ST AVE CountType SPEED SPEED SPEED SPEED SPEED # read in and name class csv ClassDF = pd.read_csv('data/TRAFFIC-Class_Counts.csv') ClassDF.name = 'ClassDF' ClassDF.head().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663904.8261 -13663904.8261 -13663933.6073 Y 5722466.9793 5722466.9793 5711466.2873 5711466.2873 5711498.9549 OBJECTID 1 2 3 4 5 CountID 10010422.CL2 10010422.CL2 10010421.CL1 10010432.CL1 10011227.CL2 ChannelID 500769 500770 500773 500776 500781 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW FRONT AVE E of 61ST AVE NW FRONT AVE E of 61ST AVE NW FRONT AVE N of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/12 00:00:00+00 StartDay MON MON MON MON TUE StartTime 12:00:00 12:00:00 13:00:00 13:00:00 15:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 2010/01/06 00:00:00+00 2010/01/14 00:00:00+00 EndDay TUE TUE WED WED THU EndTime 23:00:00 23:00:00 14:00:00 13:00:00 07:00:00 ADTVolume 3049 3409 1523 1450 640 PctCars 63.5 65.6 65.3 63.3 92.1 PctTrucks 36.5 34.4 34.7 36.7 7.9 TwoAxleCF 0.749 0.773 0.857 0.781 0.972 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 1 1 2 ChannelNum 1 2 1 1 1 NumCategories 14 14 14 14 14 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 36 36 50 49 41 IntervalLen 60 60 60 60 60 DeviceRef 0022 0022 0021 0032 0027 LocationID LEG11527 LEG11527 LEG37236 LEG37236 LEG37233 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW FRONT AVE S/NW 61ST AVE NW FRONT AVE S/NW 61ST AVE NW FRONT AVE N/NW 61ST AVE CountType CLASS CLASS CLASS CLASS CLASS # read in and name volume csv VolumeDF = pd.read_csv('data/TRAFFIC-Volume_Counts.csv') VolumeDF.name = 'VolumeDF' VolumeDF.head().transpose() <positron-console-cell-10>:2: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663930.6133 -13663930.6133 -13663904.8261 Y 5722466.9793 5722466.9793 5711463.6992 5711463.6992 5711466.2873 OBJECTID 1 2 3 4 5 CountID 10010422.VL2 10010422.VL2 10010430.vl2 10010430.vl2 10010421.VL1 ChannelID 506851 506852 506872 506873 506855 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW 61ST AVE S of FRONT AVE NW 61ST AVE S of FRONT AVE NW FRONT AVE E of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 StartDay MON MON MON MON MON StartTime 11:15:00 11:15:00 12:30:00 12:30:00 13:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 EndDay TUE TUE TUE TUE WED EndTime 23:45:00 23:45:00 23:45:00 23:45:00 15:15:00 ADTVolume 3033 3376 839 852 1491 AMVolume 1227 1992 432 512 632 AMPkHrVol 217 343 109 107 149 AMPkHrTime 2010/01/04 07:30:00+00 2010/01/04 06:15:00+00 2010/01/04 10:30:00+00 2010/01/04 09:30:00+00 2010/01/04 06:45:00+00 AMPkHrFactor 0.798 0.78 0.619 0.622 0.532 PMVolume 1806 1384 407 340 859 PMPkHrVol 333 253 77 79 134 PMPkHrTime 2010/01/04 15:15:00+00 2010/01/04 14:15:00+00 2010/01/04 12:15:00+00 2010/01/04 12:30:00+00 2010/01/04 18:30:00+00 PMPkHrFactor 0.771 0.917 0.962 0.859 0.435 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 2 2 1 ChannelNum 1 2 1 2 1 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 147 147 142 142 202 IntervalLen 15 15 15 15 15 DeviceRef 0022 0022 0030 0030 0021 LocationID LEG11527 LEG11527 LEG36313 LEG36313 LEG37236 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW 61ST AVE W/NW FRONT AVE NW 61ST AVE W/NW FRONT AVE NW FRONT AVE S/NW 61ST AVE CountType VOLUME VOLUME VOLUME VOLUME VOLUME # establish dataframe iterable DFs = [VolumeDF, ClassDF, SpeedDF] # drop columns known to not be needed for i in DFs: i.drop(columns=['StartTime', 'EndTime', 'ChannelNum', 'NumChannels', 'Comment', 'LocationClass', 'Conditions', 'DeviceRef'], inplace=True) i.info() print('\\n') <class 'pandas.core.frame.DataFrame'> RangeIndex: 21056 entries, 0 to 21055 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 21056 non-null float64 1 Y 21056 non-null float64 2 OBJECTID 21056 non-null int64 3 CountID 21056 non-null object 4 ChannelID 21056 non-null int64 5 LocationDesc 21056 non-null object 6 Bound 21056 non-null object 7 StartDate 21056 non-null object 8 StartDay 21056 non-null object 9 EndDate 21056 non-null object 10 EndDay 21056 non-null object 11 ADTVolume 21056 non-null int64 12 AMVolume 21056 non-null int64 13 AMPkHrVol 21056 non-null int64 14 AMPkHrTime 21056 non-null object 15 AMPkHrFactor 21056 non-null float64 16 PMVolume 21056 non-null int64 17 PMPkHrVol 21056 non-null int64 18 PMPkHrTime 21056 non-null object 19 PMPkHrFactor 21056 non-null float64 20 ExceptType 21056 non-null object 21 Duration 21056 non-null int64 22 IntervalLen 21056 non-null int64 23 LocationID 21056 non-null object 24 CountLocDesc 21056 non-null object 25 CountType 21056 non-null object dtypes: float64(4), int64(9), object(13) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 7849 entries, 0 to 7848 Data columns (total 22 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 7849 non-null float64 1 Y 7849 non-null float64 2 OBJECTID 7849 non-null int64 3 CountID 7849 non-null object 4 ChannelID 7849 non-null int64 5 LocationDesc 7849 non-null object 6 Bound 7844 non-null object 7 StartDate 7849 non-null object 8 StartDay 7849 non-null object 9 EndDate 7849 non-null object 10 EndDay 7849 non-null object 11 ADTVolume 7849 non-null int64 12 PctCars 7849 non-null float64 13 PctTrucks 7849 non-null float64 14 TwoAxleCF 7849 non-null float64 15 ExceptType 7849 non-null object 16 NumCategories 7849 non-null int64 17 Duration 7849 non-null int64 18 IntervalLen 7849 non-null int64 19 LocationID 7849 non-null object 20 CountLocDesc 7849 non-null object 21 CountType 7849 non-null object dtypes: float64(5), int64(6), object(11) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 17016 entries, 0 to 17015 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 17016 non-null float64 1 Y 17016 non-null float64 2 OBJECTID 17016 non-null int64 3 CountID 17016 non-null object 4 ChannelID 17016 non-null int64 5 LocationDesc 17016 non-null object 6 Bound 17011 non-null object 7 StartDate 17016 non-null object 8 StartDay 17016 non-null object 9 EndDate 17016 non-null object 10 EndDay 17016 non-null object 11 ADTVolume 17016 non-null int64 12 Spd50th 17016 non-null int64 13 Spd70th 17016 non-null int64 14 Spd85th 17016 non-null int64 15 Spd90th 17016 non-null int64 16 PostedSpeed 17016 non-null int64 17 PctOverPosted 17016 non-null float64 18 PctOverPosted10 17016 non-null float64 19 ExceptType 17016 non-null object 20 NumSlots 17016 non-null int64 21 Duration 17016 non-null int64 22 IntervalLen 17016 non-null int64 23 LocationID 17016 non-null object 24 CountLocDesc 17016 non-null object 25 CountType 17016 non-null object dtypes: float64(4), int64(11), object(11) memory usage: 3.4+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 21056 entries, 0 to 21055 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 21056 non-null float64 1 Y 21056 non-null float64 2 OBJECTID 21056 non-null int64 3 CountID 21056 non-null object 4 ChannelID 21056 non-null int64 5 LocationDesc 21056 non-null object 6 Bound 21056 non-null object 7 StartDate 21056 non-null object 8 StartDay 21056 non-null object 9 EndDate 21056 non-null object 10 EndDay 21056 non-null object 11 ADTVolume 21056 non-null int64 12 AMVolume 21056 non-null int64 13 AMPkHrVol 21056 non-null int64 14 AMPkHrTime 21056 non-null object 15 AMPkHrFactor 21056 non-null float64 16 PMVolume 21056 non-null int64 17 PMPkHrVol 21056 non-null int64 18 PMPkHrTime 21056 non-null object 19 PMPkHrFactor 21056 non-null float64 20 ExceptType 21056 non-null object 21 Duration 21056 non-null int64 22 IntervalLen 21056 non-null int64 23 LocationID 21056 non-null object 24 CountLocDesc 21056 non-null object 25 CountType 21056 non-null object dtypes: float64(4), int64(9), object(13) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 7849 entries, 0 to 7848 Data columns (total 22 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 7849 non-null float64 1 Y 7849 non-null float64 2 OBJECTID 7849 non-null int64 3 CountID 7849 non-null object 4 ChannelID 7849 non-null int64 5 LocationDesc 7849 non-null object 6 Bound 7844 non-null object 7 StartDate 7849 non-null object 8 StartDay 7849 non-null object 9 EndDate 7849 non-null object 10 EndDay 7849 non-null object 11 ADTVolume 7849 non-null int64 12 PctCars 7849 non-null float64 13 PctTrucks 7849 non-null float64 14 TwoAxleCF 7849 non-null float64 15 ExceptType 7849 non-null object 16 NumCategories 7849 non-null int64 17 Duration 7849 non-null int64 18 IntervalLen 7849 non-null int64 19 LocationID 7849 non-null object 20 CountLocDesc 7849 non-null object 21 CountType 7849 non-null object dtypes: float64(5), int64(6), object(11) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 17016 entries, 0 to 17015 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 17016 non-null float64 1 Y 17016 non-null float64 2 OBJECTID 17016 non-null int64 3 CountID 17016 non-null object 4 ChannelID 17016 non-null int64 5 LocationDesc 17016 non-null object 6 Bound 17011 non-null object 7 StartDate 17016 non-null object 8 StartDay 17016 non-null object 9 EndDate 17016 non-null object 10 EndDay 17016 non-null object 11 ADTVolume 17016 non-null int64 12 Spd50th 17016 non-null int64 13 Spd70th 17016 non-null int64 14 Spd85th 17016 non-null int64 15 Spd90th 17016 non-null int64 16 PostedSpeed 17016 non-null int64 17 PctOverPosted 17016 non-null float64 18 PctOverPosted10 17016 non-null float64 19 ExceptType 17016 non-null object 20 NumSlots 17016 non-null int64 21 Duration 17016 non-null int64 22 IntervalLen 17016 non-null int64 23 LocationID 17016 non-null object 24 CountLocDesc 17016 non-null object 25 CountType 17016 non-null object dtypes: float64(4), int64(11), object(11) memory usage: 3.4+ MB Here, the DataFrame in the list 'DFs' and generates value counts for the 'Bound', 'StartDay', and 'EndDay' columns, providing insights into the distribution of these variables. # review values in the bound column for i in DFs: print(i['Bound'].value_counts()) print(i['StartDay'].value_counts()) print(i['EndDay'].value_counts(),'\\n') Bound E 5703 W 5691 N 4819 S 4735 D 19 s 19 n 18 e 16 w 16 E-S 7 L 4 T 2 / 2 SE 1 NW 1 B 1 C 1 M 1 Name: count, dtype: int64 StartDay MON 5728 TUE 5470 WED 4878 THU 3556 SAT 664 SUN 554 FRI 206 Name: count, dtype: int64 EndDay FRI 10404 THU 4298 WED 3458 TUE 1499 SUN 638 SAT 590 MON 169 Name: count, dtype: int64 Bound E 2123 W 2084 N 1808 S 1779 NB LL 8 EB 5 NB RL 4 BND 4 S RL 4 30 2 EW 2 S.BND 2 SB 2 WB 2 WBND 2 .W 2 NS 1 NB 1 s 1 n 1 20 1 35 1 NBND 1 EBND 1 /S 1 e 1 B 1 Name: count, dtype: int64 StartDay MON 2325 TUE 2229 WED 1906 THU 1180 SAT 106 SUN 85 FRI 18 Name: count, dtype: int64 EndDay FRI 4082 THU 1777 WED 1314 TUE 454 SUN 100 SAT 98 MON 24 Name: count, dtype: int64 Bound E 4489 W 4454 N 4018 S 4002 BND 11 EB 6 .W 3 e 3 w 2 EW 2 SB 2 B 2 25 2 NS 2 n 1 s 1 NB 1 NBND 1 EBND 1 30 1 WBND 1 /S 1 NB LL 1 WB 1 SE 1 NW 1 S.BND 1 Name: count, dtype: int64 StartDay MON 4722 TUE 4644 WED 4078 THU 2878 SAT 365 SUN 261 FRI 68 Name: count, dtype: int64 EndDay FRI 8656 THU 3612 WED 2873 TUE 1172 SUN 337 SAT 308 MON 58 Name: count, dtype: int64 Bound E 5703 W 5691 N 4819 S 4735 D 19 s 19 n 18 e 16 w 16 E-S 7 L 4 T 2 / 2 SE 1 NW 1 B 1 C 1 M 1 Name: count, dtype: int64 StartDay MON 5728 TUE 5470 WED 4878 THU 3556 SAT 664 SUN 554 FRI 206 Name: count, dtype: int64 EndDay FRI 10404 THU 4298 WED 3458 TUE 1499 SUN 638 SAT 590 MON 169 Name: count, dtype: int64 Bound E 2123 W 2084 N 1808 S 1779 NB LL 8 EB 5 NB RL 4 BND 4 S RL 4 30 2 EW 2 S.BND 2 SB 2 WB 2 WBND 2 .W 2 NS 1 NB 1 s 1 n 1 20 1 35 1 NBND 1 EBND 1 /S 1 e 1 B 1 Name: count, dtype: int64 StartDay MON 2325 TUE 2229 WED 1906 THU 1180 SAT 106 SUN 85 FRI 18 Name: count, dtype: int64 EndDay FRI 4082 THU 1777 WED 1314 TUE 454 SUN 100 SAT 98 MON 24 Name: count, dtype: int64 Bound E 4489 W 4454 N 4018 S 4002 BND 11 EB 6 .W 3 e 3 w 2 EW 2 SB 2 B 2 25 2 NS 2 n 1 s 1 NB 1 NBND 1 EBND 1 30 1 WBND 1 /S 1 NB LL 1 WB 1 SE 1 NW 1 S.BND 1 Name: count, dtype: int64 StartDay MON 4722 TUE 4644 WED 4078 THU 2878 SAT 365 SUN 261 FRI 68 Name: count, dtype: int64 EndDay FRI 8656 THU 3612 WED 2873 TUE 1172 SUN 337 SAT 308 MON 58 Name: count, dtype: int64 # update bound values for uniformity for i in DFs: i['Bound'] = i['Bound'].str.upper() i['Bound'] = i['Bound'].str.replace('E-S', 'SE', regex=False) i['Bound'] = i['Bound'].str.replace('EB.*', 'E', regex=True) i['Bound'] = i['Bound'].str.replace('\\?.WB.*', 'W', regex=True) i['Bound'] = i['Bound'].str.replace('(NB.*)|(BND)', 'N', regex=True) i['Bound'] = i['Bound'].str.replace('\\?/S?/s?\\.B.*', 'S', regex=True) # establish df to input duplicate ids duplicateIDs = pd.DataFrame() # iterate through dfs to create ids & check for duplicates for i in DFs: i['ID'] = i['CountID'].str.split('.', n=1, expand=True)[0]+'_'+i['Bound']+'_' # generate IDs i.insert(0, 'ID', i.pop('ID')) # move IDs to first columns duplicateIDs = pd.concat([duplicateIDs,i[i['ID'].duplicated()]]) # add duplicated ID in single df to duplicateIDs i.drop(columns=['CountID', 'Bound'], inplace=True) print(i.name,':') print(' ',len(i.index),'rows') print(' ',i['ID'].nunique(),'unique IDs\\n') <positron-console-cell-13>:6: SyntaxWarning: invalid escape sequence '\\?' <positron-console-cell-13>:8: SyntaxWarning: invalid escape sequence '\\?' VolumeDF : 21056 rows 21040 unique IDs ClassDF : 7849 rows 7839 unique IDs VolumeDF : 21056 rows 21040 unique IDs ClassDF : 7849 rows 7839 unique IDs SpeedDF : 17016 rows 17005 unique IDs SpeedDF : 17016 rows 17005 unique IDs # check size of duplicate ids duplicateIDs.shape (35, 39) # drop duplicate id rows from each df (if low impact) for i in DFs: i.drop(i[i['ID'].duplicated()].index, axis=0, inplace=True) # list column names used for merge connections = ['ID', 'StartDate', 'EndDate'] # set name suffixes for columns suffixes = ['_Volume', '_Class', '_Speed'] # same order as DFs variable # establish list of repeated column names repeats = list(set(SpeedDF.columns) & set(VolumeDF.columns) & set(ClassDF.columns)) repeats = [i for i in repeats if i not in connections] # iterate through dfs & repeated columns to add suffixes y = 0 for i in DFs: for x in repeats: i.rename(columns={x: x + suffixes[y]}, inplace=True) i.info() y += 1 <class 'pandas.core.frame.DataFrame'> Index: 21040 entries, 0 to 21055 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 21040 non-null object 1 X_Volume 21040 non-null float64 2 Y_Volume 21040 non-null float64 3 OBJECTID_Volume 21040 non-null int64 4 ChannelID_Volume 21040 non-null int64 5 LocationDesc_Volume 21040 non-null object 6 StartDate 21040 non-null object 7 StartDay_Volume 21040 non-null object 8 EndDate 21040 non-null object 9 EndDay_Volume 21040 non-null object 10 ADTVolume_Volume 21040 non-null int64 11 AMVolume 21040 non-null int64 12 AMPkHrVol 21040 non-null int64 13 AMPkHrTime 21040 non-null object 14 AMPkHrFactor 21040 non-null float64 15 PMVolume 21040 non-null int64 16 PMPkHrVol 21040 non-null int64 17 PMPkHrTime 21040 non-null object 18 PMPkHrFactor 21040 non-null float64 19 ExceptType_Volume 21040 non-null object 20 Duration_Volume 21040 non-null int64 21 IntervalLen_Volume 21040 non-null int64 22 LocationID_Volume 21040 non-null object 23 CountLocDesc_Volume 21040 non-null object 24 CountType_Volume 21040 non-null object dtypes: float64(4), int64(9), object(12) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 7840 entries, 0 to 7848 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 7839 non-null object 1 X_Class 7840 non-null float64 2 Y_Class 7840 non-null float64 3 OBJECTID_Class 7840 non-null int64 4 ChannelID_Class 7840 non-null int64 5 LocationDesc_Class 7840 non-null object 6 StartDate 7840 non-null object 7 StartDay_Class 7840 non-null object 8 EndDate 7840 non-null object 9 EndDay_Class 7840 non-null object 10 ADTVolume_Class 7840 non-null int64 11 PctCars 7840 non-null float64 12 PctTrucks 7840 non-null float64 13 TwoAxleCF 7840 non-null float64 14 ExceptType_Class 7840 non-null object 15 NumCategories 7840 non-null int64 16 Duration_Class 7840 non-null int64 17 IntervalLen_Class 7840 non-null int64 18 LocationID_Class 7840 non-null object 19 CountLocDesc_Class 7840 non-null object 20 CountType_Class 7840 non-null object dtypes: float64(5), int64(6), object(10) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> Index: 17006 entries, 0 to 17015 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 17005 non-null object 1 X_Speed 17006 non-null float64 2 Y_Speed 17006 non-null float64 3 OBJECTID_Speed 17006 non-null int64 4 ChannelID_Speed 17006 non-null int64 5 LocationDesc_Speed 17006 non-null object 6 StartDate 17006 non-null object 7 StartDay_Speed 17006 non-null object 8 EndDate 17006 non-null object 9 EndDay_Speed 17006 non-null object 10 ADTVolume_Speed 17006 non-null int64 11 Spd50th 17006 non-null int64 12 Spd70th 17006 non-null int64 13 Spd85th 17006 non-null int64 14 Spd90th 17006 non-null int64 15 PostedSpeed 17006 non-null int64 16 PctOverPosted 17006 non-null float64 17 PctOverPosted10 17006 non-null float64 18 ExceptType_Speed 17006 non-null object 19 NumSlots 17006 non-null int64 20 Duration_Speed 17006 non-null int64 21 IntervalLen_Speed 17006 non-null int64 22 LocationID_Speed 17006 non-null object 23 CountLocDesc_Speed 17006 non-null object 24 CountType_Speed 17006 non-null object dtypes: float64(4), int64(11), object(10) memory usage: 3.4+ MB <class 'pandas.core.frame.DataFrame'> Index: 21040 entries, 0 to 21055 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 21040 non-null object 1 X_Volume 21040 non-null float64 2 Y_Volume 21040 non-null float64 3 OBJECTID_Volume 21040 non-null int64 4 ChannelID_Volume 21040 non-null int64 5 LocationDesc_Volume 21040 non-null object 6 StartDate 21040 non-null object 7 StartDay_Volume 21040 non-null object 8 EndDate 21040 non-null object 9 EndDay_Volume 21040 non-null object 10 ADTVolume_Volume 21040 non-null int64 11 AMVolume 21040 non-null int64 12 AMPkHrVol 21040 non-null int64 13 AMPkHrTime 21040 non-null object 14 AMPkHrFactor 21040 non-null float64 15 PMVolume 21040 non-null int64 16 PMPkHrVol 21040 non-null int64 17 PMPkHrTime 21040 non-null object 18 PMPkHrFactor 21040 non-null float64 19 ExceptType_Volume 21040 non-null object 20 Duration_Volume 21040 non-null int64 21 IntervalLen_Volume 21040 non-null int64 22 LocationID_Volume 21040 non-null object 23 CountLocDesc_Volume 21040 non-null object 24 CountType_Volume 21040 non-null object dtypes: float64(4), int64(9), object(12) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 7840 entries, 0 to 7848 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 7839 non-null object 1 X_Class 7840 non-null float64 2 Y_Class 7840 non-null float64 3 OBJECTID_Class 7840 non-null int64 4 ChannelID_Class 7840 non-null int64 5 LocationDesc_Class 7840 non-null object 6 StartDate 7840 non-null object 7 StartDay_Class 7840 non-null object 8 EndDate 7840 non-null object 9 EndDay_Class 7840 non-null object 10 ADTVolume_Class 7840 non-null int64 11 PctCars 7840 non-null float64 12 PctTrucks 7840 non-null float64 13 TwoAxleCF 7840 non-null float64 14 ExceptType_Class 7840 non-null object 15 NumCategories 7840 non-null int64 16 Duration_Class 7840 non-null int64 17 IntervalLen_Class 7840 non-null int64 18 LocationID_Class 7840 non-null object 19 CountLocDesc_Class 7840 non-null object 20 CountType_Class 7840 non-null object dtypes: float64(5), int64(6), object(10) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> Index: 17006 entries, 0 to 17015 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 17005 non-null object 1 X_Speed 17006 non-null float64 2 Y_Speed 17006 non-null float64 3 OBJECTID_Speed 17006 non-null int64 4 ChannelID_Speed 17006 non-null int64 5 LocationDesc_Speed 17006 non-null object 6 StartDate 17006 non-null object 7 StartDay_Speed 17006 non-null object 8 EndDate 17006 non-null object 9 EndDay_Speed 17006 non-null object 10 ADTVolume_Speed 17006 non-null int64 11 Spd50th 17006 non-null int64 12 Spd70th 17006 non-null int64 13 Spd85th 17006 non-null int64 14 Spd90th 17006 non-null int64 15 PostedSpeed 17006 non-null int64 16 PctOverPosted 17006 non-null float64 17 PctOverPosted10 17006 non-null float64 18 ExceptType_Speed 17006 non-null object 19 NumSlots 17006 non-null int64 20 Duration_Speed 17006 non-null int64 21 IntervalLen_Speed 17006 non-null int64 22 LocationID_Speed 17006 non-null object 23 CountLocDesc_Speed 17006 non-null object 24 CountType_Speed 17006 non-null object dtypes: float64(4), int64(11), object(10) memory usage: 3.4+ MB # merge dfs into one big one, keeping all values df = SpeedDF.merge(ClassDF, how='outer', on=connections).merge(VolumeDF, how='outer', on=connections) # check how many duplicated ids there are in the big df df[df['ID'].duplicated()].shape (440, 65) # compare the size of the whole df df.shape (22736, 65) # drop duplicated id rows if it makes sense df.drop(df[df['ID'].duplicated()].index, axis=0, inplace=True) df[df['ID'].duplicated()] # verify .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID X_Speed Y_Speed OBJECTID_Speed ChannelID_Speed LocationDesc_Speed StartDate StartDay_Speed EndDate EndDay_Speed ADTVolume_Speed Spd50th Spd70th Spd85th Spd90th PostedSpeed PctOverPosted PctOverPosted10 ExceptType_Speed NumSlots Duration_Speed IntervalLen_Speed LocationID_Speed CountLocDesc_Speed CountType_Speed X_Class Y_Class OBJECTID_Class ChannelID_Class LocationDesc_Class StartDay_Class EndDay_Class ADTVolume_Class PctCars PctTrucks TwoAxleCF ExceptType_Class NumCategories Duration_Class IntervalLen_Class LocationID_Class CountLocDesc_Class CountType_Class X_Volume Y_Volume OBJECTID_Volume ChannelID_Volume LocationDesc_Volume StartDay_Volume EndDay_Volume ADTVolume_Volume AMVolume AMPkHrVol AMPkHrTime AMPkHrFactor PMVolume PMPkHrVol PMPkHrTime PMPkHrFactor ExceptType_Volume Duration_Volume IntervalLen_Volume LocationID_Volume CountLocDesc_Volume CountType_Volume # check for null counts df.isnull().sum() ID 1 X_Speed 5486 Y_Speed 5486 OBJECTID_Speed 5486 ChannelID_Speed 5486 ... Duration_Volume 1511 IntervalLen_Volume 1511 LocationID_Volume 1511 CountLocDesc_Volume 1511 CountType_Volume 1511 Length: 65, dtype: int64 # fill na in for repeated columns using values from other dfs for i in repeats: df[i+'_Speed'] = df[i+'_Speed'].fillna(df[i+'_Volume']) df[i+'_Volume'] = df[i+'_Volume'].fillna(df[i+'_Class']) df[i+'_Class'] = df[i+'_Class'].fillna(df[i+'_Speed']) df[i+'_Speed'] = df[i+'_Speed'].fillna(df[i+'_Volume']) df[i+'_Volume'] = df[i+'_Volume'].fillna(df[i+'_Class']) df[i+'_Class'] = df[i+'_Class'].fillna(df[i+'_Speed']) # sort columns alphabetically df = df.reindex(sorted(df.columns), axis=1) df.insert(0, 'ID', df.pop('ID')) # move ID to first column # initial condensed df df.info() <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 65 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 ADTVolume_Class 22296 non-null float64 2 ADTVolume_Speed 22296 non-null float64 3 ADTVolume_Volume 22296 non-null float64 4 AMPkHrFactor 20785 non-null float64 5 AMPkHrTime 20785 non-null object 6 AMPkHrVol 20785 non-null float64 7 AMVolume 20785 non-null float64 8 ChannelID_Class 22296 non-null float64 9 ChannelID_Speed 22296 non-null float64 10 ChannelID_Volume 22296 non-null float64 11 CountLocDesc_Class 22296 non-null object 12 CountLocDesc_Speed 22296 non-null object 13 CountLocDesc_Volume 22296 non-null object 14 CountType_Class 22296 non-null object 15 CountType_Speed 22296 non-null object 16 CountType_Volume 22296 non-null object 17 Duration_Class 22296 non-null float64 18 Duration_Speed 22296 non-null float64 19 Duration_Volume 22296 non-null float64 20 EndDate 22296 non-null object 21 EndDay_Class 22296 non-null object 22 EndDay_Speed 22296 non-null object 23 EndDay_Volume 22296 non-null object 24 ExceptType_Class 22296 non-null object 25 ExceptType_Speed 22296 non-null object 26 ExceptType_Volume 22296 non-null object 27 IntervalLen_Class 22296 non-null float64 28 IntervalLen_Speed 22296 non-null float64 29 IntervalLen_Volume 22296 non-null float64 30 LocationDesc_Class 22296 non-null object 31 LocationDesc_Speed 22296 non-null object 32 LocationDesc_Volume 22296 non-null object 33 LocationID_Class 22296 non-null object 34 LocationID_Speed 22296 non-null object 35 LocationID_Volume 22296 non-null object 36 NumCategories 7747 non-null float64 37 NumSlots 16810 non-null float64 38 OBJECTID_Class 22296 non-null float64 39 OBJECTID_Speed 22296 non-null float64 40 OBJECTID_Volume 22296 non-null float64 41 PMPkHrFactor 20785 non-null float64 42 PMPkHrTime 20785 non-null object 43 PMPkHrVol 20785 non-null float64 44 PMVolume 20785 non-null float64 45 PctCars 7747 non-null float64 46 PctOverPosted 16810 non-null float64 47 PctOverPosted10 16810 non-null float64 48 PctTrucks 7747 non-null float64 49 PostedSpeed 16810 non-null float64 50 Spd50th 16810 non-null float64 51 Spd70th 16810 non-null float64 52 Spd85th 16810 non-null float64 53 Spd90th 16810 non-null float64 54 StartDate 22296 non-null object 55 StartDay_Class 22296 non-null object 56 StartDay_Speed 22296 non-null object 57 StartDay_Volume 22296 non-null object 58 TwoAxleCF 7747 non-null float64 59 X_Class 22296 non-null float64 60 X_Speed 22296 non-null float64 61 X_Volume 22296 non-null float64 62 Y_Class 22296 non-null float64 63 Y_Speed 22296 non-null float64 64 Y_Volume 22296 non-null float64 dtypes: float64(39), object(26) memory usage: 11.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 65 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 ADTVolume_Class 22296 non-null float64 2 ADTVolume_Speed 22296 non-null float64 3 ADTVolume_Volume 22296 non-null float64 4 AMPkHrFactor 20785 non-null float64 5 AMPkHrTime 20785 non-null object 6 AMPkHrVol 20785 non-null float64 7 AMVolume 20785 non-null float64 8 ChannelID_Class 22296 non-null float64 9 ChannelID_Speed 22296 non-null float64 10 ChannelID_Volume 22296 non-null float64 11 CountLocDesc_Class 22296 non-null object 12 CountLocDesc_Speed 22296 non-null object 13 CountLocDesc_Volume 22296 non-null object 14 CountType_Class 22296 non-null object 15 CountType_Speed 22296 non-null object 16 CountType_Volume 22296 non-null object 17 Duration_Class 22296 non-null float64 18 Duration_Speed 22296 non-null float64 19 Duration_Volume 22296 non-null float64 20 EndDate 22296 non-null object 21 EndDay_Class 22296 non-null object 22 EndDay_Speed 22296 non-null object 23 EndDay_Volume 22296 non-null object 24 ExceptType_Class 22296 non-null object 25 ExceptType_Speed 22296 non-null object 26 ExceptType_Volume 22296 non-null object 27 IntervalLen_Class 22296 non-null float64 28 IntervalLen_Speed 22296 non-null float64 29 IntervalLen_Volume 22296 non-null float64 30 LocationDesc_Class 22296 non-null object 31 LocationDesc_Speed 22296 non-null object 32 LocationDesc_Volume 22296 non-null object 33 LocationID_Class 22296 non-null object 34 LocationID_Speed 22296 non-null object 35 LocationID_Volume 22296 non-null object 36 NumCategories 7747 non-null float64 37 NumSlots 16810 non-null float64 38 OBJECTID_Class 22296 non-null float64 39 OBJECTID_Speed 22296 non-null float64 40 OBJECTID_Volume 22296 non-null float64 41 PMPkHrFactor 20785 non-null float64 42 PMPkHrTime 20785 non-null object 43 PMPkHrVol 20785 non-null float64 44 PMVolume 20785 non-null float64 45 PctCars 7747 non-null float64 46 PctOverPosted 16810 non-null float64 47 PctOverPosted10 16810 non-null float64 48 PctTrucks 7747 non-null float64 49 PostedSpeed 16810 non-null float64 50 Spd50th 16810 non-null float64 51 Spd70th 16810 non-null float64 52 Spd85th 16810 non-null float64 53 Spd90th 16810 non-null float64 54 StartDate 22296 non-null object 55 StartDay_Class 22296 non-null object 56 StartDay_Speed 22296 non-null object 57 StartDay_Volume 22296 non-null object 58 TwoAxleCF 7747 non-null float64 59 X_Class 22296 non-null float64 60 X_Speed 22296 non-null float64 61 X_Volume 22296 non-null float64 62 Y_Class 22296 non-null float64 63 Y_Speed 22296 non-null float64 64 Y_Volume 22296 non-null float64 dtypes: float64(39), object(26) memory usage: 11.2+ MB # iterate to find columns that are perfectly identical for i in repeats: if df[i+'_Speed'].equals(df[i+'_Volume']) is True: print(i,'Speed and Volume match') if df[i+'_Class'].equals(df[i+'_Volume']) is True: print(i,'Class and Volume match') if df[i+'_Speed'].equals(df[i+'_Class']) is True: print(i,'Speed and Class match') EndDay Speed and Volume match EndDay Class and Volume match EndDay Speed and Class match EndDay Speed and Volume match EndDay Class and Volume match EndDay Speed and Class match # clear up the low-hanging fruit (columns that perfectly match) df.drop(columns=['EndDay_Speed', 'EndDay_Class'], inplace=True) df.rename(columns={'EndDay_Volume':'EndDay'}, inplace=True) repeats.remove('EndDay') # figure out how many rows have mismatched values in repeated columns rep = {} for i in repeats: print(i,':') speed_volume = df.iloc[np.where((df[i+'_Speed'] != df[i+'_Volume']))] speed_class = df.iloc[np.where((df[i+'_Speed'] != df[i+'_Class']))] class_volume = df.iloc[np.where((df[i+'_Class'] != df[i+'_Volume']))] mask = pd.concat([speed_volume, speed_class, class_volume]) mask = mask.drop_duplicates() rep[i] = {i+'_Speed': mask[i+'_Speed'], i+'_Volume': mask[i+'_Volume'], i+'_Class': mask[i+'_Class']} print(' ',len(mask),'\\n') ADTVolume : 14130 CountLocDesc : 197 OBJECTID : ADTVolume : 14130 CountLocDesc : 197 OBJECTID : 15968 Y : 55 X : 55 IntervalLen : 15530 StartDay : 96 ChannelID : 15968 Y : 55 X : 55 IntervalLen : 15530 StartDay : 96 ChannelID : 15974 ExceptType : 57 LocationID : 55 LocationDesc : 47 CountType : 15974 ExceptType : 57 LocationID : 55 LocationDesc : 47 CountType : 15974 Duration : 15591 15974 Duration : 15591 # select speed as the df of truth for repeated columns, drop others for i in repeats: df[i] = df[i+'_Speed'] df.drop(i+'_Speed', axis=1, inplace=True) df.drop(i+'_Volume', axis=1, inplace=True) df.drop(i+'_Class', axis=1, inplace=True) df.info() <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 37 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 AMPkHrFactor 20785 non-null float64 2 AMPkHrTime 20785 non-null object 3 AMPkHrVol 20785 non-null float64 4 AMVolume 20785 non-null float64 5 EndDate 22296 non-null object 6 EndDay 22296 non-null object 7 NumCategories 7747 non-null float64 8 NumSlots 16810 non-null float64 9 PMPkHrFactor 20785 non-null float64 10 PMPkHrTime 20785 non-null object 11 PMPkHrVol 20785 non-null float64 12 PMVolume 20785 non-null float64 13 PctCars 7747 non-null float64 14 PctOverPosted 16810 non-null float64 15 PctOverPosted10 16810 non-null float64 16 PctTrucks 7747 non-null float64 17 PostedSpeed 16810 non-null float64 18 Spd50th 16810 non-null float64 19 Spd70th 16810 non-null float64 20 Spd85th 16810 non-null float64 21 Spd90th 16810 non-null float64 22 StartDate 22296 non-null object 23 TwoAxleCF 7747 non-null float64 24 ADTVolume 22296 non-null float64 25 CountLocDesc 22296 non-null object 26 OBJECTID 22296 non-null float64 27 Y 22296 non-null float64 28 X 22296 non-null float64 29 IntervalLen 22296 non-null float64 30 StartDay 22296 non-null object 31 ChannelID 22296 non-null float64 32 ExceptType 22296 non-null object 33 LocationID 22296 non-null object 34 LocationDesc 22296 non-null object 35 CountType 22296 non-null object 36 Duration 22296 non-null float64 dtypes: float64(25), object(12) memory usage: 6.5+ MB <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 37 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 AMPkHrFactor 20785 non-null float64 2 AMPkHrTime 20785 non-null object 3 AMPkHrVol 20785 non-null float64 4 AMVolume 20785 non-null float64 5 EndDate 22296 non-null object 6 EndDay 22296 non-null object 7 NumCategories 7747 non-null float64 8 NumSlots 16810 non-null float64 9 PMPkHrFactor 20785 non-null float64 10 PMPkHrTime 20785 non-null object 11 PMPkHrVol 20785 non-null float64 12 PMVolume 20785 non-null float64 13 PctCars 7747 non-null float64 14 PctOverPosted 16810 non-null float64 15 PctOverPosted10 16810 non-null float64 16 PctTrucks 7747 non-null float64 17 PostedSpeed 16810 non-null float64 18 Spd50th 16810 non-null float64 19 Spd70th 16810 non-null float64 20 Spd85th 16810 non-null float64 21 Spd90th 16810 non-null float64 22 StartDate 22296 non-null object 23 TwoAxleCF 7747 non-null float64 24 ADTVolume 22296 non-null float64 25 CountLocDesc 22296 non-null object 26 OBJECTID 22296 non-null float64 27 Y 22296 non-null float64 28 X 22296 non-null float64 29 IntervalLen 22296 non-null float64 30 StartDay 22296 non-null object 31 ChannelID 22296 non-null float64 32 ExceptType 22296 non-null object 33 LocationID 22296 non-null object 34 LocationDesc 22296 non-null object 35 CountType 22296 non-null object 36 Duration 22296 non-null float64 dtypes: float64(25), object(12) memory usage: 6.5+ MB # remove any other columns known to not be needed df = df.drop(columns=['NumCategories']) # identify where one pct exists but the other doens't pct_missingcars = df[(df['PctTrucks'].notnull()) & (df['PctCars'].isnull())] pct_missingtrucks = df[(df['PctCars'].notnull()) & (df['PctTrucks'].isnull())] pct_missingall = df[(df['PctCars'].isnull()) & (df['PctTrucks'].isnull())] # fill in missing pct df.loc[pct_missingcars.index, 'PctCars'] = 100-df['PctTrucks'] df.loc[pct_missingtrucks.index, 'PctTrucks'] = 100-df['PctCars'] # check how many are missing both pcts pct_missingall.shape (14549, 36) # review values in TwoAxleCF df['TwoAxleCF'].describe() count 7747.000000 mean 0.985774 std 0.033495 min 0.632000 25% 0.990000 50% 0.995000 75% 0.998000 max 1.125000 Name: TwoAxleCF, dtype: float64 # fill missing TwoAxleCF values with median df['TwoAxleCF'] = df['TwoAxleCF'].fillna(df['TwoAxleCF'].median()) # count of missing values from each row df.isnull().sum() ID 1 AMPkHrFactor 1511 AMPkHrTime 1511 AMPkHrVol 1511 AMVolume 1511 EndDate 0 EndDay 0 NumSlots 5486 PMPkHrFactor 1511 PMPkHrTime 1511 PMPkHrVol 1511 PMVolume 1511 PctCars 14549 PctOverPosted 5486 PctOverPosted10 5486 PctTrucks 14549 PostedSpeed 5486 Spd50th 5486 Spd70th 5486 Spd85th 5486 Spd90th 5486 StartDate 0 TwoAxleCF 0 ADTVolume 0 CountLocDesc 0 OBJECTID 0 Y 0 X 0 IntervalLen 0 StartDay 0 ChannelID 0 ExceptType 0 LocationID 0 LocationDesc 0 CountType 0 Duration 0 dtype: int64 # remove all rows with na values, leaving a clean dataset df = df.dropna() df.shape (7072, 36) # final dataframe df.head().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2 3 4 10 31 ID 10010421_N_ 10010422_E_ 10010422_W_ 10010432_S_ 10011224_N_ AMPkHrFactor 0.532 0.798 0.78 0.726 0.885 AMPkHrTime 2010/01/04 06:45:00+00 2010/01/04 07:30:00+00 2010/01/04 06:15:00+00 2010/01/04 06:00:00+00 2010/01/12 08:00:00+00 AMPkHrVol 149.0 217.0 343.0 209.0 170.0 AMVolume 632.0 1227.0 1992.0 895.0 724.0 EndDate 2010/01/06 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 2010/01/15 00:00:00+00 EndDay WED TUE TUE WED FRI NumSlots 13.0 13.0 13.0 13.0 13.0 PMPkHrFactor 0.435 0.771 0.917 0.867 0.964 PMPkHrTime 2010/01/04 18:30:00+00 2010/01/04 15:15:00+00 2010/01/04 14:15:00+00 2010/01/04 12:00:00+00 2010/01/12 17:00:00+00 PMPkHrVol 134.0 333.0 253.0 104.0 189.0 PMVolume 859.0 1806.0 1384.0 528.0 1564.0 PctCars 65.3 63.5 65.6 63.3 98.0 PctOverPosted 20.6 17.9 25.7 22.1 21.1 PctOverPosted10 2.2 0.6 1.5 2.5 0.1 PctTrucks 34.7 36.5 34.4 36.7 2.0 PostedSpeed 40.0 45.0 45.0 40.0 25.0 Spd50th 33.0 39.0 39.0 35.0 22.0 Spd70th 37.0 43.0 44.0 38.0 24.0 Spd85th 42.0 46.0 48.0 42.0 26.0 Spd90th 44.0 47.0 49.0 44.0 27.0 StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/12 00:00:00+00 TwoAxleCF 0.857 0.749 0.773 0.781 0.998 ADTVolume 1524.0 3048.0 3405.0 1465.0 2301.0 CountLocDesc NW FRONT AVE S/NW 61ST AVE N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW FRONT AVE S/NW 61ST AVE SE 20TH AVE N/SE STEPHENS ST OBJECTID 5.0 1.0 2.0 6.0 22.0 Y 5711466.2873 5722466.9793 5722466.9793 5711466.2873 5702148.698 X -13663904.8261 -13666261.3309 -13666261.3309 -13663904.8261 -13652793.7282 IntervalLen 60.0 60.0 60.0 60.0 60.0 StartDay MON MON MON MON TUE ChannelID 505642.0 505638.0 505639.0 505645.0 505676.0 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday LocationID LEG37236 LEG11527 LEG11527 LEG37236 LEG43556 LocationDesc NW FRONT AVE E of 61ST AVE N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW FRONT AVE E of 61ST AVE SE 20TH AVE N of STEPHENS ST CountType SPEED SPEED SPEED SPEED SPEED Duration 50.0 36.0 36.0 49.0 85.0 Verify impact of dropped rows # suppress warnings for datetaime conversion import warnings warnings.filterwarnings(\"ignore\", message=\"Converting to PeriodArray/Index representation will drop timezone information.\") def extract_month_year(origin, df_name, column_name): new_df = pd.DataFrame() new_df['Month_Year'] = pd.to_datetime(origin[column_name]).dt.to_period('M') # count occurrences of each unique month/year combo count_df = new_df['Month_Year'].value_counts().reset_index() count_df.columns = ['Month_Year', 'Count'] # sum the counts sum_count = count_df['Count'].sum() # calculate percentage of sum for each month/year combo count_df['Percentage'] = (count_df['Count'] / sum_count) * 100 # rename column according to dataframe name count_df.rename(columns={'Percentage': f'{df_name}_Percentage'}, inplace=True) return count_df.set_index('Month_Year')[[f'{df_name}_Percentage']] # apply function to each dataframe df_percentages = extract_month_year(df, 'df', 'StartDate') Speed_DF_percentages = extract_month_year(SpeedDF, 'SpeedDF', 'StartDate') Class_DF_percentages = extract_month_year(ClassDF, 'ClassDF', 'StartDate') Volume_DF_percentages = extract_month_year(VolumeDF, 'VolumeDF', 'StartDate') # merge the percentage dataframes freq_df = pd.merge(df_percentages, Speed_DF_percentages, on='Month_Year', how='outer') freq_df = pd.merge(freq_df, Class_DF_percentages, on='Month_Year', how='outer') freq_df = pd.merge(freq_df, Volume_DF_percentages, on='Month_Year', how='outer') # sort the index freq_df.sort_index(inplace=True) # plot the results ax = freq_df.plot(kind='line', figsize=(10, 6)) ax.lines[0].set_alpha(1) for line in ax.lines[1:]: line.set_alpha(0.2) plt.title('Count Comparison Over Time') plt.xlabel('Month/Year') plt.ylabel('Percent of Total Counts for each Data Frame') plt.grid(True) plt.legend() plt.show() Data Exploration # visualizing outliers variables = ['PostedSpeed', 'PctOverPosted'] for variable in variables: plt.figure(figsize=(4, 2)) sns.boxplot(x=df[variable]) plt.title(f\"{variable} Boxplot with Outliers\") plt.show() # dropping outliers using IQR for variable in variables: Q1 = df[variable].quantile(0.25) Q3 = df[variable].quantile(0.75) IQR = Q3 - Q1 lower_threshold = Q1 - 1.5 * IQR upper_threshold = Q3 + 1.5 * IQR # Identify and drop outliers_index = (df[variable] < lower_threshold) | (df[variable] > upper_threshold) df = df[~outliers_index] # new df after dropping outliers for variable in variables: plt.figure(figsize=(4, 2)) sns.boxplot(x=df[variable]) plt.title(f\"{variable} Boxplot, Outliers Removed\") plt.show() Feature Engineering Aggregated Statistics : statistics such as mean, median, maximum, or minimum speed for each vehicle type, location, and time interval. These statistics can provide insights into the typical speeding behavior in different scenarios. # Variables for calculation StatVariables = ['PctOverPosted', 'PctOverPosted10', 'ADTVolume'] # Means, Median, Min, and Max grouped by Start Day StartDayStats_df = pd.DataFrame() for variable in StatVariables: stats_by_day = df.groupby(['StartDay'])[variable].agg(['mean', 'median', 'min', 'max']).reset_index() stats_by_day.rename(columns={ 'mean': f'{variable}_mean', 'median': f'{variable}_median', 'min': f'{variable}_min', 'max': f'{variable}_max' }, inplace=True) stats_by_day = stats_by_day.round(2) StartDayStats_df = pd.concat([StartDayStats_df, stats_by_day], axis=1) output_string = StartDayStats_df.to_string(index=False,) print(output_string) StartDay PctOverPosted_mean PctOverPosted_median PctOverPosted_min PctOverPosted_max StartDay PctOverPosted10_mean PctOverPosted10_median PctOverPosted10_min PctOverPosted10_max StartDay ADTVolume_mean ADTVolume_median ADTVolume_min ADTVolume_max FRI 25.35 23.80 2.7 50.8 FRI 0.30 0.25 0.0 0.8 FRI 2115.17 1777.5 912.0 4795.0 MON 36.75 31.50 0.0 99.6 MON 3.41 0.80 0.0 86.6 MON 3484.60 2436.0 26.0 25705.0 SAT 37.87 37.75 0.2 90.2 SAT 2.92 0.95 0.0 23.1 SAT 1743.56 1199.5 46.0 6540.0 SUN 48.27 49.00 2.3 92.6 SUN 3.88 1.30 0.0 28.7 SUN 1965.43 1320.0 247.0 5777.0 THU 33.52 29.15 0.0 96.5 THU 2.47 0.50 0.0 51.0 THU 1929.78 748.5 19.0 22141.0 TUE 37.64 32.80 0.0 98.7 TUE 3.54 0.80 0.0 89.4 TUE 3007.47 1608.0 39.0 27058.0 WED 36.30 30.40 0.0 96.9 WED 3.58 0.60 0.0 67.6 WED 3006.57 1654.0 14.0 23236.0 StartDay PctOverPosted_mean PctOverPosted_median PctOverPosted_min PctOverPosted_max StartDay PctOverPosted10_mean PctOverPosted10_median PctOverPosted10_min PctOverPosted10_max StartDay ADTVolume_mean ADTVolume_median ADTVolume_min ADTVolume_max FRI 25.35 23.80 2.7 50.8 FRI 0.30 0.25 0.0 0.8 FRI 2115.17 1777.5 912.0 4795.0 MON 36.75 31.50 0.0 99.6 MON 3.41 0.80 0.0 86.6 MON 3484.60 2436.0 26.0 25705.0 SAT 37.87 37.75 0.2 90.2 SAT 2.92 0.95 0.0 23.1 SAT 1743.56 1199.5 46.0 6540.0 SUN 48.27 49.00 2.3 92.6 SUN 3.88 1.30 0.0 28.7 SUN 1965.43 1320.0 247.0 5777.0 THU 33.52 29.15 0.0 96.5 THU 2.47 0.50 0.0 51.0 THU 1929.78 748.5 19.0 22141.0 TUE 37.64 32.80 0.0 98.7 TUE 3.54 0.80 0.0 89.4 TUE 3007.47 1608.0 39.0 27058.0 WED 36.30 30.40 0.0 96.9 WED 3.58 0.60 0.0 67.6 WED 3006.57 1654.0 14.0 23236.0 Speed Deviation from Speed Limit: difference between the observed speed and the posted speed limit for each location and time interval. This can help identify areas where speeding is more prevalent relative to the speed limit. # Calculate differences between posted and percentile speeds def calculate_differences(group): group['Diff_Spd50th'] = group['Spd50th'] - group['PostedSpeed'] group['Diff_Spd70th'] = group['Spd70th'] - group['PostedSpeed'] group['Diff_Spd85th'] = group['Spd85th'] - group['PostedSpeed'] group['Diff_Spd90th'] = group['Spd90th'] - group['PostedSpeed'] return group # Apply the function to each group based on the 'ID' column # Using _extras for this so as not to add unwanted extra columns to the data df_extras = df.groupby('ID').apply(calculate_differences).reset_index(drop=True) # Optional display #df_extras.head().transpose() <positron-console-cell-40>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning. # calculate the avg for each 'CountLocDesc' average_diff_by_countlocdesc = df_extras.groupby('CountLocDesc').agg({ 'Diff_Spd50th': 'mean', 'Diff_Spd70th': 'mean', 'Diff_Spd85th': 'mean', 'Diff_Spd90th': 'mean' }).reset_index() # Optional display average_diff_by_countlocdesc = average_diff_by_countlocdesc.sort_values(by='Diff_Spd90th', ascending=False) #average_diff_by_countlocdesc.head() # Display the top 15 rows for locations with the greatest mean speed over posted top_15_diff_Spd50th = df_extras.nlargest(15, 'Diff_Spd50th') print(tabulate(top_15_diff_Spd50th[['CountLocDesc', 'PostedSpeed', 'Spd50th', 'Diff_Spd50th','PctOverPosted10']], headers=['CountLocDesc', 'PostedSpeed', 'Spd50th', 'Diff_Spd50th','PctOverPosted10'], tablefmt='outline', showindex=False)) +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ | CountLocDesc | PostedSpeed | Spd50th | Diff_Spd50th | PctOverPosted10 | +========================================================+===============+===========+================+===================+ | SE 112TH AVE S of SE OGDEN S | 20 | 37 | 17 | 89.4 | | COLUMBIA WAY - COLUMBIA BLVD RAMP W of N COLUMBIA BLVD | 20 | 36 | 16 | 86.6 | | I-84 EB EXTO NE 68TH AVE W of NE 68TH AVE | 25 | 39 | 14 | 78.3 | | NE 148TH AVE S of NE SACRAMENTO ST | 30 | 43 | 13 | 67.6 | | SW 45TH DR S of SW DOLPH CT | 25 | 37 | 12 | 63.9 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 61.3 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 57.6 | | SE MARKET ST W of SE 104TH AVE | 20 | 31 | 11 | 59.2 | | SW TERWILLIGER BLV N/SW CONDOR LN | 25 | 35 | 10 | 45.8 | | SW 45TH DR N of SW ORCHID ST | 30 | 40 | 10 | 50.6 | | SE MT SCOTT BLVD W of SE 108TH AVE | 35 | 45 | 10 | 51.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 48.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 51.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 45.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 46.6 | +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ | CountLocDesc | PostedSpeed | Spd50th | Diff_Spd50th | PctOverPosted10 | +========================================================+===============+===========+================+===================+ | SE 112TH AVE S of SE OGDEN S | 20 | 37 | 17 | 89.4 | | COLUMBIA WAY - COLUMBIA BLVD RAMP W of N COLUMBIA BLVD | 20 | 36 | 16 | 86.6 | | I-84 EB EXTO NE 68TH AVE W of NE 68TH AVE | 25 | 39 | 14 | 78.3 | | NE 148TH AVE S of NE SACRAMENTO ST | 30 | 43 | 13 | 67.6 | | SW 45TH DR S of SW DOLPH CT | 25 | 37 | 12 | 63.9 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 61.3 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 57.6 | | SE MARKET ST W of SE 104TH AVE | 20 | 31 | 11 | 59.2 | | SW TERWILLIGER BLV N/SW CONDOR LN | 25 | 35 | 10 | 45.8 | | SW 45TH DR N of SW ORCHID ST | 30 | 40 | 10 | 50.6 | | SE MT SCOTT BLVD W of SE 108TH AVE | 35 | 45 | 10 | 51.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 48.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 51.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 45.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 46.6 | +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ # Optional - Display the top 15 rows for locations with the greatest speed for 90th percentile # top_15_diff_Spd50th = df_extras.nlargest(15, 'Diff_Spd90th') # print(tabulate(top_15_diff_Spd50th[['CountLocDesc', 'PostedSpeed', 'Spd90th', 'Diff_Spd90th','PctOverPosted10']], # headers=['CountLocDesc', 'PostedSpeed', 'Spd90th', 'Diff_Spd90th','PctOverPosted10'], # tablefmt='outline', showindex=False)) Historical Features: to capture trends and seasonality in speeding behaviour. # Yearly average over time # Convert StartDate to datetime df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year from StartDate df['Year'] = df['StartDate'].dt.year # Calculate average PctOverPosted10 and PctOverPosted by grouping only by 'Year' yearlyavg_pctover10 = df.groupby('Year')['PctOverPosted10'].mean() yearlyavg_pctover = df.groupby('Year')['PctOverPosted'].mean() # Plotting plt.figure(figsize=(12, 8)) plt.plot(yearlyavg_pctover10.index, yearlyavg_pctover10.values, marker='o', linestyle='-', label='PctOverPosted10') plt.plot(yearlyavg_pctover.index, yearlyavg_pctover.values, marker='o', linestyle='-', label='PctOverPosted') plt.title('Yearly Average PctOverPosted10 and PctOverPosted Over Time') plt.xlabel('Year') plt.ylabel('Average Percentage') plt.grid(True) plt.legend() plt.show() # Monthly aggregation PctOverPosted # Convert StartDate to datetime df = df.copy() df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year and month from StartDate df['Year'] = df['StartDate'].dt.year df['Month'] = df['StartDate'].dt.month # Calculate average PctOverPosted avg_pct_over_time = df.groupby(['Year', 'Month'])['PctOverPosted'].mean().unstack() # Plotting plt.figure(figsize=(12, 8)) for year in avg_pct_over_time.index: plt.plot(range(1, 13), avg_pct_over_time.loc[year], marker='o', label=f'{year}') plt.title('Average PctOverPosted Over Time') plt.xlabel('Month') plt.ylabel('Average PctOverPosted') plt.grid(True) plt.xticks(range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) plt.legend(title='Year', bbox_to_anchor=(1, 1), loc='upper left') plt.show() # Monthly aggregation PctOverPosted10 # Convert StartDate to datetime df = df.copy() df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year and month from StartDate df['Year'] = df['StartDate'].dt.year # Calculate average PctOverPosted10 yearly_avg_pct_over_time10 = df.groupby(['Year','Month'])['PctOverPosted10'].mean().unstack() # Plotting plt.figure(figsize=(12, 8)) for year in yearly_avg_pct_over_time10.index: plt.plot(range(1, 13), yearly_avg_pct_over_time10.loc[year], marker='o', label=f'{year}') plt.title('Yearly Average PctOverPosted10 Over Time') plt.xlabel('Month') plt.ylabel('Average PctOverPosted10') plt.grid(True) plt.xticks(range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) plt.legend(title='Year', bbox_to_anchor=(1., 1), loc='upper left') plt.show() Feature Selection # create new column \"Over10Speeders\" where any PctOver10 > 0 is marked 1 df['10OverSpeeding'] = df['PctOverPosted10'].apply(lambda x: 1 if x > 0 else 0) col = df.pop('10OverSpeeding') df.insert(0, col.name, col) # drop redundant column PctOverPosted10 df.drop(columns=['PctOverPosted10'], inplace=True) # review correlations to target df.corr(numeric_only = True)['10OverSpeeding'].sort_values().round(2) PctCars -0.19 TwoAxleCF -0.16 Month -0.05 ChannelID -0.03 OBJECTID -0.03 Year -0.02 IntervalLen 0.01 NumSlots 0.01 Y 0.04 Duration 0.08 X 0.09 PctTrucks 0.19 PostedSpeed 0.27 AMVolume 0.29 AMPkHrVol 0.30 AMPkHrFactor 0.30 PMPkHrFactor 0.31 PMVolume 0.31 ADTVolume 0.31 PMPkHrVol 0.32 Spd90th 0.41 Spd85th 0.47 Spd70th 0.50 Spd50th 0.50 PctOverPosted 0.52 10OverSpeeding 1.00 Name: 10OverSpeeding, dtype: float64 # Correlation for ALL NUMERICAL numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns correlation_matrix = df[numeric_columns].corr() # Plotting a heatmap using seaborn plt.figure(figsize=(16, 8)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5) plt.title('Correlation Matrix') plt.show() # create subset df_subset = df[['10OverSpeeding', 'AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'StartDay', 'Month']] # Print the updated dataframe information df_subset.info() print('\\n') df_subset['10OverSpeeding'].value_counts() <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 StartDay 7055 non-null object 11 Month 7055 non-null int32 dtypes: float64(9), int32(1), int64(1), object(1) memory usage: 689.0+ KB <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 StartDay 7055 non-null object 11 Month 7055 non-null int32 dtypes: float64(9), int32(1), int64(1), object(1) memory usage: 689.0+ KB 10OverSpeeding 1 5658 0 1397 Name: count, dtype: int64 # set initial predictors list target = '10OverSpeeding' predictors = ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'StartDay', 'Month'] # get dummies df_subset = pd.get_dummies(df_subset, columns=df_subset.select_dtypes(include=['object', 'category']).columns, drop_first=True) df_subset.info() <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 Month 7055 non-null int32 11 StartDay_MON 7055 non-null bool 12 StartDay_SAT 7055 non-null bool 13 StartDay_SUN 7055 non-null bool 14 StartDay_THU 7055 non-null bool 15 StartDay_TUE 7055 non-null bool 16 StartDay_WED 7055 non-null bool dtypes: bool(6), float64(9), int32(1), int64(1) memory usage: 675.2 KB <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 Month 7055 non-null int32 11 StartDay_MON 7055 non-null bool 12 StartDay_SAT 7055 non-null bool 13 StartDay_SUN 7055 non-null bool 14 StartDay_THU 7055 non-null bool 15 StartDay_TUE 7055 non-null bool 16 StartDay_WED 7055 non-null bool dtypes: bool(6), float64(9), int32(1), int64(1) memory usage: 675.2 KB # automated multivariate feature selection X = df_subset.drop(columns=[target]) y = df_subset[target] estimator = DecisionTreeClassifier() selector = RFE(estimator, n_features_to_select=10, step=1).fit(X,y) rnk = pd.DataFrame() rnk['Feature'] = X.columns rnk['Rank']= selector.ranking_ rnk.sort_values('Rank') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feature Rank 0 AMPkHrVol 1 1 AMVolume 1 2 PMPkHrVol 1 3 PMVolume 1 4 PctCars 1 5 PostedSpeed 1 6 X 1 7 ADTVolume 1 8 Y 1 9 Month 1 15 StartDay_WED 2 10 StartDay_MON 3 14 StartDay_TUE 4 13 StartDay_THU 5 12 StartDay_SUN 6 11 StartDay_SAT 7 # View the selected features selected_features = rnk.sort_values('Rank').head(10)['Feature'].tolist() # Display print(\"Top 10 Selected Features:\") print(selected_features) Top 10 Selected Features: ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'Month'] Top 10 Selected Features: ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'Month'] # Check correlation for selected features correlation_matrix = df_subset[[target] + selected_features].corr() # Plotting heatmap plt.figure(figsize=(10, 8)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5) plt.title('Correlation Matrix Between Selected Features') plt.show() # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=17, stratify=y) # determine hyperparameters param_grid = { 'max_depth': [10], 'min_samples_leaf': [20], 'max_features': list(range(2,11)) } gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=17), param_grid, cv=5, n_jobs=-1) gridSearch.fit(X_train, y_train) print('Score: ', gridSearch.best_score_) best_params = gridSearch.best_params_ print('Parameters: ', best_params) Score: 0.8364648311127025 Parameters: {'max_depth': 10, 'max_features': 7, 'min_samples_leaf': 20} Score: 0.8364648311127025 Parameters: {'max_depth': 10, 'max_features': 7, 'min_samples_leaf': 20} Model Building: Decision Tree # Train & Predict tree_model = DecisionTreeClassifier(random_state=17, max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'], max_features=best_params['max_features']) tree_model.fit(X_train, y_train) tree_predictions = tree_model.predict(X_test) # Print decision tree rules tree_rules = export_text(tree_model, feature_names=list(X_train.columns)) print(\"Decision Tree Rules:\") print(tree_rules) Decision Tree Rules: |--- AMVolume <= 262.50 | |--- X <= -13645958.50 | | |--- AMVolume <= 63.50 | | | |--- PMVolume <= 74.50 | | | | |--- Y <= 5702417.00 | | | | | |--- Y <= 5697200.50 | | | | | | |--- class: 0 | | | | | |--- Y > 5697200.50 | | | | | | |--- class: 0 | | | | |--- Y > 5702417.00 | | | | | |--- ADTVolume <= 90.50 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 0 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 0 | | | | | |--- ADTVolume > 90.50 | | | | | | |--- class: 0 | | | |--- PMVolume > 74.50 | | | | |--- Month <= 3.50 | | | | | |--- PostedSpeed <= 22.50 | | | | | | |--- Y <= 5708024.75 | | | | | | | |--- PMVolume <= 121.50 | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 121.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5708024.75 | | | | | | | |--- class: 1 | | | | | |--- PostedSpeed > 22.50 | | | | | | |--- class: 0 | | | | |--- Month > 3.50 | | | | | |--- Y <= 5707250.00 | | | | | | |--- X <= -13650135.00 | | | | | | | |--- Y <= 5701115.00 | | | | | | | | |--- class: 0 | | | | | | | |--- Y > 5701115.00 | | | | | | | | |--- AMVolume <= 51.50 | | | | | | | | | |--- class: 0 | | | | | | | | |--- AMVolume > 51.50 | | | | | | | | | |--- class: 0 | | | | | | |--- X > -13650135.00 | | | | | | | |--- class: 0 | | | | | |--- Y > 5707250.00 | | | | | | |--- Month <= 9.50 | | | | | | | |--- PctCars <= 95.65 | | | | | | | | |--- class: 0 | | | | | | | |--- PctCars > 95.65 | | | | | | | | |--- class: 0 | | | | | | |--- Month > 9.50 | | | | | | | |--- class: 1 | | |--- AMVolume > 63.50 | | | |--- PctCars <= 97.35 | | | | |--- Y <= 5710675.50 | | | | | |--- Y <= 5700892.50 | | | | | | |--- PctCars <= 95.85 | | | | | | | |--- PMPkHrVol <= 36.50 | | | | | | | | |--- Y <= 5697090.25 | | | | | | | | | |--- X <= -13658192.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13658192.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5697090.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 36.50 | | | | | | | | |--- X <= -13659513.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659513.50 | | | | | | | | | |--- PMVolume <= 257.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 257.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PctCars > 95.85 | | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | | |--- Y <= 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PostedSpeed > 22.50 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5700892.50 | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | |--- PMVolume <= 228.50 | | | | | | | | |--- ADTVolume <= 217.50 | | | | | | | | | |--- Y <= 5706837.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Y > 5706837.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- ADTVolume > 217.50 | | | | | | | | | |--- PMVolume <= 169.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMVolume > 169.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 228.50 | | | | | | | | |--- X <= -13656465.50 | | | | | | | | | |--- X <= -13657555.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13657555.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13656465.50 | | | | | | | | | |--- Y <= 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PostedSpeed > 22.50 | | | | | | | |--- PMVolume <= 446.50 | | | | | | | | |--- ADTVolume <= 270.50 | | | | | | | | | |--- Month <= 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Month > 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- ADTVolume > 270.50 | | | | | | | | | |--- PMVolume <= 234.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 234.50 | | | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 446.50 | | | | | | | | |--- class: 1 | | | | |--- Y > 5710675.50 | | | | | |--- PMVolume <= 206.50 | | | | | | |--- Y <= 5714802.50 | | | | | | | |--- AMVolume <= 80.50 | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 80.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714802.50 | | | | | | | |--- class: 1 | | | | | |--- PMVolume > 206.50 | | | | | | |--- PMVolume <= 316.50 | | | | | | | |--- AMPkHrVol <= 33.50 | | | | | | | | |--- X <= -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMPkHrVol > 33.50 | | | | | | | | |--- class: 1 | | | | | | |--- PMVolume > 316.50 | | | | | | | |--- PMPkHrVol <= 80.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 80.00 | | | | | | | | |--- class: 1 | | | |--- PctCars > 97.35 | | | | |--- X <= -13650381.00 | | | | | |--- Y <= 5701628.25 | | | | | | |--- PMPkHrVol <= 43.50 | | | | | | | |--- class: 0 | | | | | | |--- PMPkHrVol > 43.50 | | | | | | | |--- class: 0 | | | | | |--- Y > 5701628.25 | | | | | | |--- ADTVolume <= 279.00 | | | | | | | |--- class: 0 | | | | | | |--- ADTVolume > 279.00 | | | | | | | |--- X <= -13650837.00 | | | | | | | | |--- Y <= 5706611.00 | | | | | | | | | |--- AMPkHrVol <= 38.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 38.00 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5706611.00 | | | | | | | | | |--- class: 1 | | | | | | | |--- X > -13650837.00 | | | | | | | | |--- class: 0 | | | | |--- X > -13650381.00 | | | | | |--- Y <= 5707354.75 | | | | | | |--- Y <= 5703056.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5703056.25 | | | | | | | |--- PctCars <= 97.85 | | | | | | | | |--- class: 1 | | | | | | | |--- PctCars > 97.85 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5707354.75 | | | | | | |--- class: 1 | |--- X > -13645958.50 | | |--- Month <= 5.50 | | | |--- Y <= 5708819.00 | | | | |--- AMPkHrVol <= 25.50 | | | | | |--- AMPkHrVol <= 13.50 | | | | | | |--- class: 1 | | | | | |--- AMPkHrVol > 13.50 | | | | | | |--- class: 1 | | | | |--- AMPkHrVol > 25.50 | | | | | |--- PMVolume <= 314.50 | | | | | | |--- class: 1 | | | | | |--- PMVolume > 314.50 | | | | | | |--- class: 1 | | | |--- Y > 5708819.00 | | | | |--- class: 1 | | |--- Month > 5.50 | | | |--- PctCars <= 96.65 | | | | |--- PMPkHrVol <= 14.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 14.50 | | | | | |--- X <= -13642374.00 | | | | | | |--- X <= -13645112.00 | | | | | | | |--- class: 1 | | | | | | |--- X > -13645112.00 | | | | | | | |--- class: 1 | | | | | |--- X > -13642374.00 | | | | | | |--- Y <= 5706861.75 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5706861.75 | | | | | | | |--- class: 1 | | | |--- PctCars > 96.65 | | | | |--- class: 1 |--- AMVolume > 262.50 | |--- PctCars <= 95.75 | | |--- AMVolume <= 1172.50 | | | |--- X <= -13646288.50 | | | | |--- X <= -13660894.50 | | | | | |--- AMVolume <= 721.50 | | | | | | |--- class: 1 | | | | | |--- AMVolume > 721.50 | | | | | | |--- Y <= 5714154.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714154.25 | | | | | | | |--- class: 1 | | | | |--- X > -13660894.50 | | | | | |--- Month <= 7.50 | | | | | | |--- Y <= 5705188.50 | | | | | | | |--- AMVolume <= 1033.50 | | | | | | | | |--- Y <= 5702413.50 | | | | | | | | | |--- PMPkHrVol <= 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMPkHrVol > 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5702413.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 1033.50 | | | | | | | | |--- AMPkHrVol <= 221.00 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMPkHrVol > 221.00 | | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5705188.50 | | | | | | | |--- AMVolume <= 908.50 | | | | | | | | |--- PctCars <= 92.55 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 92.55 | | | | | | | | | |--- Y <= 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 908.50 | | | | | | | | |--- class: 1 | | | | | |--- Month > 7.50 | | | | | | |--- ADTVolume <= 899.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 899.50 | | | | | | | |--- Y <= 5708754.00 | | | | | | | | |--- Y <= 5704862.50 | | | | | | | | | |--- AMPkHrVol <= 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5704862.50 | | | | | | | | | |--- PMPkHrVol <= 215.00 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMPkHrVol > 215.00 | | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5708754.00 | | | | | | | | |--- class: 1 | | | |--- X > -13646288.50 | | | | |--- PMPkHrVol <= 92.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 92.50 | | | | | |--- X <= -13636547.00 | | | | | | |--- class: 1 | | | | | |--- X > -13636547.00 | | | | | | |--- class: 1 | | |--- AMVolume > 1172.50 | | | |--- Y <= 5694779.75 | | | | |--- class: 1 | | | |--- Y > 5694779.75 | | | | |--- ADTVolume <= 3358.50 | | | | | |--- PctCars <= 94.25 | | | | | | |--- class: 1 | | | | | |--- PctCars > 94.25 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 3358.50 | | | | | |--- Y <= 5696825.75 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 1 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 1 | | | | | |--- Y > 5696825.75 | | | | | | |--- Month <= 11.50 | | | | | | | |--- Month <= 2.50 | | | | | | | | |--- AMVolume <= 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMVolume > 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- Month > 2.50 | | | | | | | | |--- PctCars <= 94.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 94.75 | | | | | | | | | |--- ADTVolume <= 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- ADTVolume > 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- Month > 11.50 | | | | | | | |--- PMVolume <= 3003.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 3003.00 | | | | | | | | |--- class: 1 | |--- PctCars > 95.75 | | |--- PMVolume <= 2022.50 | | | |--- X <= -13649647.50 | | | | |--- X <= -13659285.00 | | | | | |--- PctCars <= 96.35 | | | | | | |--- class: 1 | | | | | |--- PctCars > 96.35 | | | | | | |--- ADTVolume <= 1979.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 1979.50 | | | | | | | |--- class: 1 | | | | |--- X > -13659285.00 | | | | | |--- Y <= 5708073.25 | | | | | | |--- X <= -13658741.00 | | | | | | | |--- class: 0 | | | | | | |--- X > -13658741.00 | | | | | | | |--- Y <= 5699886.00 | | | | | | | | |--- Y <= 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5699886.00 | | | | | | | | |--- X <= -13655722.00 | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13655722.00 | | | | | | | | | |--- PctCars <= 98.35 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PctCars > 98.35 | | | | | | | | | | |--- class: 0 | | | | | |--- Y > 5708073.25 | | | | | | |--- class: 1 | | | |--- X > -13649647.50 | | | | |--- Month <= 8.50 | | | | | |--- PMPkHrVol <= 190.50 | | | | | | |--- class: 1 | | | | | |--- PMPkHrVol > 190.50 | | | | | | |--- class: 1 | | | | |--- Month > 8.50 | | | | | |--- class: 1 | | |--- PMVolume > 2022.50 | | | |--- X <= -13651133.00 | | | | |--- Y <= 5698353.75 | | | | | |--- class: 1 | | | | |--- Y > 5698353.75 | | | | | |--- ADTVolume <= 3321.00 | | | | | | |--- class: 1 | | | | | |--- ADTVolume > 3321.00 | | | | | | |--- PMPkHrVol <= 382.50 | | | | | | | |--- class: 1 | | | | | | |--- PMPkHrVol > 382.50 | | | | | | | |--- ADTVolume <= 6976.00 | | | | | | | | |--- X <= -13659386.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659386.50 | | | | | | | | | |--- X <= -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- ADTVolume > 6976.00 | | | | | | | | |--- class: 1 | | | |--- X > -13651133.00 | | | | |--- ADTVolume <= 7612.00 | | | | | |--- Month <= 9.50 | | | | | | |--- class: 1 | | | | | |--- Month > 9.50 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 7612.00 | | | | | |--- class: 1 Decision Tree Rules: |--- AMVolume <= 262.50 | |--- X <= -13645958.50 | | |--- AMVolume <= 63.50 | | | |--- PMVolume <= 74.50 | | | | |--- Y <= 5702417.00 | | | | | |--- Y <= 5697200.50 | | | | | | |--- class: 0 | | | | | |--- Y > 5697200.50 | | | | | | |--- class: 0 | | | | |--- Y > 5702417.00 | | | | | |--- ADTVolume <= 90.50 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 0 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 0 | | | | | |--- ADTVolume > 90.50 | | | | | | |--- class: 0 | | | |--- PMVolume > 74.50 | | | | |--- Month <= 3.50 | | | | | |--- PostedSpeed <= 22.50 | | | | | | |--- Y <= 5708024.75 | | | | | | | |--- PMVolume <= 121.50 | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 121.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5708024.75 | | | | | | | |--- class: 1 | | | | | |--- PostedSpeed > 22.50 | | | | | | |--- class: 0 | | | | |--- Month > 3.50 | | | | | |--- Y <= 5707250.00 | | | | | | |--- X <= -13650135.00 | | | | | | | |--- Y <= 5701115.00 | | | | | | | | |--- class: 0 | | | | | | | |--- Y > 5701115.00 | | | | | | | | |--- AMVolume <= 51.50 | | | | | | | | | |--- class: 0 | | | | | | | | |--- AMVolume > 51.50 | | | | | | | | | |--- class: 0 | | | | | | |--- X > -13650135.00 | | | | | | | |--- class: 0 | | | | | |--- Y > 5707250.00 | | | | | | |--- Month <= 9.50 | | | | | | | |--- PctCars <= 95.65 | | | | | | | | |--- class: 0 | | | | | | | |--- PctCars > 95.65 | | | | | | | | |--- class: 0 | | | | | | |--- Month > 9.50 | | | | | | | |--- class: 1 | | |--- AMVolume > 63.50 | | | |--- PctCars <= 97.35 | | | | |--- Y <= 5710675.50 | | | | | |--- Y <= 5700892.50 | | | | | | |--- PctCars <= 95.85 | | | | | | | |--- PMPkHrVol <= 36.50 | | | | | | | | |--- Y <= 5697090.25 | | | | | | | | | |--- X <= -13658192.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13658192.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5697090.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 36.50 | | | | | | | | |--- X <= -13659513.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659513.50 | | | | | | | | | |--- PMVolume <= 257.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 257.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PctCars > 95.85 | | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | | |--- Y <= 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PostedSpeed > 22.50 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5700892.50 | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | |--- PMVolume <= 228.50 | | | | | | | | |--- ADTVolume <= 217.50 | | | | | | | | | |--- Y <= 5706837.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Y > 5706837.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- ADTVolume > 217.50 | | | | | | | | | |--- PMVolume <= 169.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMVolume > 169.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 228.50 | | | | | | | | |--- X <= -13656465.50 | | | | | | | | | |--- X <= -13657555.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13657555.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13656465.50 | | | | | | | | | |--- Y <= 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PostedSpeed > 22.50 | | | | | | | |--- PMVolume <= 446.50 | | | | | | | | |--- ADTVolume <= 270.50 | | | | | | | | | |--- Month <= 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Month > 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- ADTVolume > 270.50 | | | | | | | | | |--- PMVolume <= 234.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 234.50 | | | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 446.50 | | | | | | | | |--- class: 1 | | | | |--- Y > 5710675.50 | | | | | |--- PMVolume <= 206.50 | | | | | | |--- Y <= 5714802.50 | | | | | | | |--- AMVolume <= 80.50 | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 80.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714802.50 | | | | | | | |--- class: 1 | | | | | |--- PMVolume > 206.50 | | | | | | |--- PMVolume <= 316.50 | | | | | | | |--- AMPkHrVol <= 33.50 | | | | | | | | |--- X <= -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMPkHrVol > 33.50 | | | | | | | | |--- class: 1 | | | | | | |--- PMVolume > 316.50 | | | | | | | |--- PMPkHrVol <= 80.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 80.00 | | | | | | | | |--- class: 1 | | | |--- PctCars > 97.35 | | | | |--- X <= -13650381.00 | | | | | |--- Y <= 5701628.25 | | | | | | |--- PMPkHrVol <= 43.50 | | | | | | | |--- class: 0 | | | | | | |--- PMPkHrVol > 43.50 | | | | | | | |--- class: 0 | | | | | |--- Y > 5701628.25 | | | | | | |--- ADTVolume <= 279.00 | | | | | | | |--- class: 0 | | | | | | |--- ADTVolume > 279.00 | | | | | | | |--- X <= -13650837.00 | | | | | | | | |--- Y <= 5706611.00 | | | | | | | | | |--- AMPkHrVol <= 38.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 38.00 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5706611.00 | | | | | | | | | |--- class: 1 | | | | | | | |--- X > -13650837.00 | | | | | | | | |--- class: 0 | | | | |--- X > -13650381.00 | | | | | |--- Y <= 5707354.75 | | | | | | |--- Y <= 5703056.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5703056.25 | | | | | | | |--- PctCars <= 97.85 | | | | | | | | |--- class: 1 | | | | | | | |--- PctCars > 97.85 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5707354.75 | | | | | | |--- class: 1 | |--- X > -13645958.50 | | |--- Month <= 5.50 | | | |--- Y <= 5708819.00 | | | | |--- AMPkHrVol <= 25.50 | | | | | |--- AMPkHrVol <= 13.50 | | | | | | |--- class: 1 | | | | | |--- AMPkHrVol > 13.50 | | | | | | |--- class: 1 | | | | |--- AMPkHrVol > 25.50 | | | | | |--- PMVolume <= 314.50 | | | | | | |--- class: 1 | | | | | |--- PMVolume > 314.50 | | | | | | |--- class: 1 | | | |--- Y > 5708819.00 | | | | |--- class: 1 | | |--- Month > 5.50 | | | |--- PctCars <= 96.65 | | | | |--- PMPkHrVol <= 14.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 14.50 | | | | | |--- X <= -13642374.00 | | | | | | |--- X <= -13645112.00 | | | | | | | |--- class: 1 | | | | | | |--- X > -13645112.00 | | | | | | | |--- class: 1 | | | | | |--- X > -13642374.00 | | | | | | |--- Y <= 5706861.75 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5706861.75 | | | | | | | |--- class: 1 | | | |--- PctCars > 96.65 | | | | |--- class: 1 |--- AMVolume > 262.50 | |--- PctCars <= 95.75 | | |--- AMVolume <= 1172.50 | | | |--- X <= -13646288.50 | | | | |--- X <= -13660894.50 | | | | | |--- AMVolume <= 721.50 | | | | | | |--- class: 1 | | | | | |--- AMVolume > 721.50 | | | | | | |--- Y <= 5714154.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714154.25 | | | | | | | |--- class: 1 | | | | |--- X > -13660894.50 | | | | | |--- Month <= 7.50 | | | | | | |--- Y <= 5705188.50 | | | | | | | |--- AMVolume <= 1033.50 | | | | | | | | |--- Y <= 5702413.50 | | | | | | | | | |--- PMPkHrVol <= 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMPkHrVol > 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5702413.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 1033.50 | | | | | | | | |--- AMPkHrVol <= 221.00 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMPkHrVol > 221.00 | | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5705188.50 | | | | | | | |--- AMVolume <= 908.50 | | | | | | | | |--- PctCars <= 92.55 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 92.55 | | | | | | | | | |--- Y <= 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 908.50 | | | | | | | | |--- class: 1 | | | | | |--- Month > 7.50 | | | | | | |--- ADTVolume <= 899.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 899.50 | | | | | | | |--- Y <= 5708754.00 | | | | | | | | |--- Y <= 5704862.50 | | | | | | | | | |--- AMPkHrVol <= 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5704862.50 | | | | | | | | | |--- PMPkHrVol <= 215.00 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMPkHrVol > 215.00 | | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5708754.00 | | | | | | | | |--- class: 1 | | | |--- X > -13646288.50 | | | | |--- PMPkHrVol <= 92.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 92.50 | | | | | |--- X <= -13636547.00 | | | | | | |--- class: 1 | | | | | |--- X > -13636547.00 | | | | | | |--- class: 1 | | |--- AMVolume > 1172.50 | | | |--- Y <= 5694779.75 | | | | |--- class: 1 | | | |--- Y > 5694779.75 | | | | |--- ADTVolume <= 3358.50 | | | | | |--- PctCars <= 94.25 | | | | | | |--- class: 1 | | | | | |--- PctCars > 94.25 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 3358.50 | | | | | |--- Y <= 5696825.75 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 1 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 1 | | | | | |--- Y > 5696825.75 | | | | | | |--- Month <= 11.50 | | | | | | | |--- Month <= 2.50 | | | | | | | | |--- AMVolume <= 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMVolume > 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- Month > 2.50 | | | | | | | | |--- PctCars <= 94.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 94.75 | | | | | | | | | |--- ADTVolume <= 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- ADTVolume > 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- Month > 11.50 | | | | | | | |--- PMVolume <= 3003.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 3003.00 | | | | | | | | |--- class: 1 | |--- PctCars > 95.75 | | |--- PMVolume <= 2022.50 | | | |--- X <= -13649647.50 | | | | |--- X <= -13659285.00 | | | | | |--- PctCars <= 96.35 | | | | | | |--- class: 1 | | | | | |--- PctCars > 96.35 | | | | | | |--- ADTVolume <= 1979.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 1979.50 | | | | | | | |--- class: 1 | | | | |--- X > -13659285.00 | | | | | |--- Y <= 5708073.25 | | | | | | |--- X <= -13658741.00 | | | | | | | |--- class: 0 | | | | | | |--- X > -13658741.00 | | | | | | | |--- Y <= 5699886.00 | | | | | | | | |--- Y <= 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5699886.00 | | | | | | | | |--- X <= -13655722.00 | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13655722.00 | | | | | | | | | |--- PctCars <= 98.35 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PctCars > 98.35 | | | | | | | | | | |--- class: 0 | | | | | |--- Y > 5708073.25 | | | | | | |--- class: 1 | | | |--- X > -13649647.50 | | | | |--- Month <= 8.50 | | | | | |--- PMPkHrVol <= 190.50 | | | | | | |--- class: 1 | | | | | |--- PMPkHrVol > 190.50 | | | | | | |--- class: 1 | | | | |--- Month > 8.50 | | | | | |--- class: 1 | | |--- PMVolume > 2022.50 | | | |--- X <= -13651133.00 | | | | |--- Y <= 5698353.75 | | | | | |--- class: 1 | | | | |--- Y > 5698353.75 | | | | | |--- ADTVolume <= 3321.00 | | | | | | |--- class: 1 | | | | | |--- ADTVolume > 3321.00 | | | | | | |--- PMPkHrVol <= 382.50 | | | | | | | |--- class: 1 | | | | | | |--- PMPkHrVol > 382.50 | | | | | | | |--- ADTVolume <= 6976.00 | | | | | | | | |--- X <= -13659386.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659386.50 | | | | | | | | | |--- X <= -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- ADTVolume > 6976.00 | | | | | | | | |--- class: 1 | | | |--- X > -13651133.00 | | | | |--- ADTVolume <= 7612.00 | | | | | |--- Month <= 9.50 | | | | | | |--- class: 1 | | | | | |--- Month > 9.50 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 7612.00 | | | | | |--- class: 1 # Tree Rules Figure plt.figure(figsize=(24, 16)) plot_tree(tree_model, feature_names=list(X_train.columns), class_names=['Over10Speeing', 'NoSpeedingOver10'], filled=True, rounded=True, proportion=True) plt.show() # Sets for iteration datasets = [(X_train, y_train, \"Training\"), (X_test, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Predictions tree_predictions = tree_model.predict(data) tree_conf_matrix = confusion_matrix(labels, tree_predictions) # Performance Metrics accuracy = round(accuracy_score(labels, tree_predictions), 3) precision = round(precision_score(labels, tree_predictions), 3) recall = round(recall_score(labels, tree_predictions), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {accuracy}\") print(f\"Precision: {precision}\") print(f\"Recall: {recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in tree_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in tree_conf_matrix.flatten() / np.sum(tree_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(tree_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Decision Tree Confusion Matrix - {dataset_name} Data') plt.show() Training Performance: Accuracy: 0.872 Precision: 0.899 Recall: 0.948 Training Performance: Accuracy: 0.872 Precision: 0.899 Recall: 0.948 Testing Performance: Accuracy: 0.826 Precision: 0.874 Recall: 0.916 Testing Performance: Accuracy: 0.826 Precision: 0.874 Recall: 0.916 # Identify the most important features importances = tree_model.feature_importances_ top_features = np.argsort(importances)[::-1][:5] # Where 5 is the number of features you want to select # Print the top features print(\"Top Features:\") for feature_index in top_features: print(X_train.columns[feature_index]) # keep top features & scale the data scaler = StandardScaler() X_train_final = scaler.fit_transform(X_train.iloc[:, top_features]) X_test_final = scaler.transform(X_test.iloc[:, top_features]) Top Features: AMVolume Y X PctCars PMVolume Top Features: AMVolume Y X PctCars PMVolume New top 5 predictors based on the decision tree, utilised for each model from here on # Save the top features into a new predictor X X_top_features = X.iloc[:, top_features] # Split the data into training and testing sets using new predictors selected in decision tree X_train, X_test, y_train, y_test = train_test_split(X_top_features, y, test_size=0.2, random_state=17, stratify=y) Random Forest # Train & Predict rf_model = RandomForestRegressor(random_state=17) rf_model.fit(X_train, y_train) train_predictions = rf_model.predict(X_train) test_predictions = rf_model.predict(X_test) # Performance print(\"Performance Metrics:\\n\") # Training Set print(\"Training Performance:\") print(f\"MSE: {mean_squared_error(y_train, train_predictions)}\") print(f\"R-squared: {r2_score(y_train, train_predictions)}\") # Testing Set print(\"\\nTesting Performance:\") print(f\"MSE: {mean_squared_error(y_test, test_predictions)}\") print(f\"R-squared: {r2_score(y_test, test_predictions)}\") Performance Metrics: Training Performance: MSE: 0.014233238837703754 R-squared: 0.9103972386141846 Testing Performance: MSE: 0.11146888731396173 R-squared: 0.29732085628886595 Performance Metrics: Training Performance: MSE: 0.014233238837703754 R-squared: 0.9103972386141846 Testing Performance: MSE: 0.11146888731396173 R-squared: 0.29732085628886595 Logistic Regression # Train the model logistic_model = LogisticRegression(random_state=17) logistic_model.fit(X_train, y_train) logistic_train_predictions = logistic_model.predict(X_train) logistic_test_predictions = logistic_model.predict(X_test) # Print Intercept print(f\"Y-Intercept Value: {logistic_model.intercept_[0]}\") # Print Coefficients coefficients_df = pd.DataFrame({'Predictor': X_top_features.columns, 'Coefficient': logistic_model.coef_[0]}) print(\"\\nCoefficients:\") print(coefficients_df) Y-Intercept Value: 3.430078655417783e-09 Coefficients: Predictor Coefficient 0 AMVolume 0.000619 1 Y 0.000033 2 X 0.000014 3 PctCars -0.000060 4 PMVolume 0.000718 Y-Intercept Value: 3.430078655417783e-09 Coefficients: Predictor Coefficient 0 AMVolume 0.000619 1 Y 0.000033 2 X 0.000014 3 PctCars -0.000060 4 PMVolume 0.000718 # LOGISTIC REGRESSION PERFORMANCE print(\"Logistic Regression Performance Metrics:\\n\") # Sets for iteration datasets = [(X_train, y_train, \"Training\"), (X_test, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Predictions logistic_predictions = logistic_model.predict(data) logistic_conf_matrix = confusion_matrix(labels, logistic_predictions) # Performance Metrics logistic_accuracy = round(accuracy_score(labels, logistic_predictions), 3) logistic_precision = round(precision_score(labels, logistic_predictions), 3) logistic_recall = round(recall_score(labels, logistic_predictions), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {logistic_accuracy}\") print(f\"Precision: {logistic_precision}\") print(f\"Recall: {logistic_recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in logistic_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in logistic_conf_matrix.flatten() / np.sum(logistic_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(logistic_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Logistic Regression Confusion Matrix - {dataset_name} Data') plt.show() Logistic Regression Performance Metrics: Training Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.998 Logistic Regression Performance Metrics: Training Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.998 Testing Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.997 Testing Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.997 K-NN Classifier # Standardize scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the KNN classifier knn_classifier = KNeighborsClassifier(n_neighbors=5) knn_classifier.fit(X_train_scaled, y_train) # Predict train_predictions = knn_classifier.predict(X_train_scaled) test_predictions = knn_classifier.predict(X_test_scaled) # KNN PERFORMANCE print(\"KNN Performance Metrics:\\n\") # Sets for iteration datasets = [(X_train_scaled, y_train, \"Training\"), (X_test_scaled, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Convert labels to integers labels = labels.astype(int) # Predictions knn_predictions = knn_classifier.predict(data) knn_conf_matrix = confusion_matrix(labels, knn_predictions) # Performance Metrics knn_accuracy = round(accuracy_score(labels, knn_predictions), 3) knn_precision = round(precision_score(labels, knn_predictions, average='binary'), 3) knn_recall = round(recall_score(labels, knn_predictions, average='binary'), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {knn_accuracy}\") print(f\"Precision: {knn_precision}\") print(f\"Recall: {knn_recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in knn_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in knn_conf_matrix.flatten() / np.sum(knn_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(knn_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'KNN Classifier Confusion Matrix - {dataset_name} Data') plt.show() KNN Performance Metrics: Training Performance: Accuracy: 0.881 Precision: 0.91 Recall: 0.944 KNN Performance Metrics: Training Performance: Accuracy: 0.881 Precision: 0.91 Recall: 0.944 Testing Performance: Accuracy: 0.811 Precision: 0.869 Recall: 0.9 Testing Performance: Accuracy: 0.811 Precision: 0.869 Recall: 0.9 All Models Comparison # Initialize lists to store metrics model_names = [\"Decision Tree\", \"Logistic Regression\", \"KNN CLassifier\"] accuracy_list = [] precision_list = [] recall_list = [] # Append DT metrics to lists accuracy_list.append(accuracy) precision_list.append(precision) recall_list.append(recall) # Append Logistic metrics to lists accuracy_list.append(logistic_accuracy) precision_list.append(logistic_precision) recall_list.append(logistic_recall) # Append KNN metrics to lists accuracy_list.append(knn_accuracy) precision_list.append(knn_precision) recall_list.append(knn_recall) table = PrettyTable() table.field_names = [\"Model\", \"Dataset\", \"Accuracy\", \"Precision\", \"Recall\"] for model_name, accuracy, precision, recall in zip(model_names, accuracy_list, precision_list, recall_list): table.add_row([model_name, \"Testing\", accuracy, precision, recall]) print(table) +---------------------+---------+----------+-----------+--------+ | Model | Dataset | Accuracy | Precision | Recall | +---------------------+---------+----------+-----------+--------+ | Decision Tree | Testing | 0.826 | 0.874 | 0.916 | | Logistic Regression | Testing | 0.802 | 0.803 | 0.997 | | KNN CLassifier | Testing | 0.811 | 0.869 | 0.9 | +---------------------+---------+----------+-----------+--------+ +---------------------+---------+----------+-----------+--------+ | Model | Dataset | Accuracy | Precision | Recall | +---------------------+---------+----------+-----------+--------+ | Decision Tree | Testing | 0.826 | 0.874 | 0.916 | | Logistic Regression | Testing | 0.802 | 0.803 | 0.997 | | KNN CLassifier | Testing | 0.811 | 0.869 | 0.9 | +---------------------+---------+----------+-----------+--------+ Stacked Ensemble # set base models to stack base_models = [ ('decision_tree', RandomForestRegressor(random_state=17)), ('logistic_regression', LogisticRegression(random_state=17)), ('knn_classifier', KNeighborsClassifier(n_neighbors=5)) ] # define the meta-learner meta_learner = LogisticRegression(random_state=42) # create & train the stacking ensemble stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5) stacking_ensemble.fit(X_train, y_train) # make predictions and evaluate the ensemble model stacked_predictions = stacking_ensemble.predict(X_test) stacked_accuracy = round(accuracy_score(y_test, stacked_predictions), 3) stacked_precision = round(precision_score(y_test, stacked_predictions), 3) stacked_recall = round(recall_score(y_test, stacked_predictions), 3) # print the results print(\"Stacking Ensemble Performance:\") print(f\" Accuracy: {stacked_accuracy}\") print(f\" Precision: {stacked_precision}\") print(f\" Recall: {stacked_recall}\") /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( Stacking Ensemble Performance: Accuracy: 0.844 Precision: 0.883 Recall: 0.928 Stacking Ensemble Performance: Accuracy: 0.844 Precision: 0.883 Recall: 0.928 stacked_conf_matrix = confusion_matrix(y_test, stacked_predictions) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(stacked_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Stacked Ensemble Confusion Matrix - Testing Data') plt.show()","title":"Traffic Analysis"},{"location":"Traffic_Analysis/Traffic_Analysis/#speedwatch-predictive-analysis-of-speeding-incidences-by-vehicle-type-location-and-time","text":"Authors:Morgan Mohan, David Moroney, Ashley Nilson, Anita Pathak","title":"SpeedWatch: Predictive Analysis of Speeding Incidences by Vehicle Type, Location, and Time"},{"location":"Traffic_Analysis/Traffic_Analysis/#executive-summary","text":"Portland's roadways are grappling with a significant challenge: an uptick in speeding incidents despite the efforts of the Portland Transportation Department to enforce speed limit regulations. These incidents present a clear threat to road safety, resulting in accidents, injuries, and even fatalities. To tackle this pressing issue, our project undertook a comprehensive analysis of data sourced from the Portland Transportation Department, focusing on speed, volume, and classes of vehicles. Our methodology encompassed several key stages. Initially, we merged datasets related to speed, traffic volume, and vehicle classes. Through extensive exploratory data analysis, we sought to understand the distribution of speed, traffic behavior by day, disparities between posted speed limits and actual speeds, and historical trends in overspeeding. Feature selection was then carried out to identify relevant variables for modeling and an initial Decision Tree model was used to further reduce predictor varaibles. Subsequently, we implemented Logistic Regression, K-Nearest Neighbors, and a Stacked Ensemble model to predict speeding incidents. Our analysis yielded critical insights into the nature and extent of speeding incidents on Portland's roadways. We identified the top locations where vehicles consistently exceeded posted speed limits and observed fluctuations in overspeeding trends over the years. Notably, certain areas such as SE Division St E of SE 33rd Ave ranked highest for mean speed exceeding limits, indicating areas of particular concern. Moreover, we identified key predictors for speeding behaviors, including traffic volume and time of day. Predictive modeling using Decision Tree Classifier and Logistic Regression demonstrated strong performance metrics, with the logistic regression model achieving an accuracy of 0.802 and a recall of 1 for both training and testing datasets. These findings have significant implications for improving road safety and transportation management in Portland. By pinpointing areas with high instances of speeding and understanding the factors driving these behaviors, transportation authorities can implement targeted interventions to mitigate risks and enhance road safety. Possible interventions include deploying speed enforcement measures such as speed cameras or increased police patrols in identified hotspot areas. Additionally, insights gleaned from predictive modeling can inform the development of proactive strategies to anticipate and address future challenges related to speeding incidents. Our project underscores the importance of data-driven approaches in tackling speeding incidents and enhancing road safety. By leveraging comprehensive analysis of traffic data, stakeholders can make informed decisions to optimize transportation management and improve the quality of life for Portland residents. Moving forward, continued collaboration between transportation authorities, law enforcement agencies, and urban planners will be essential for the successful implementation of road safety initiatives.","title":"Executive Summary"},{"location":"Traffic_Analysis/Traffic_Analysis/#introduction","text":"Speeding incidents on Portland's roadways have become a prominent and persistent challenge, despite concerted efforts by the Portland Transportation Department to enforce speed limit regulations. These incidents not only pose a threat to public safety but also impede effective mitigation measures. In response to this ongoing issue, our project endeavors to conduct a thorough analysis of data sourced from the Portland Transportation Department, focusing specifically on variables such as speed, volume, and vehicle classifications. The urgency of addressing this issue cannot be overstated, as speeding incidents contribute significantly to road safety concerns, including a rise in accidents, injuries, and fatalities. Despite the department's efforts, instances of vehicles surpassing assigned speed limits persist, necessitating a deeper understanding of the underlying factors driving such behavior. By merging and analyzing datasets related to speed, volume, and vehicle classes, our project aims to identify key areas where drivers tend to overspeed and discern trends in overspeeding over time. Additionally, we seek to explore correlations between speeding incidents and various factors such as location, types of vehicles, time of day, and speed limits. Ultimately, the objective of our analysis is to provide actionable insights that can support the Portland Transportation Department's efforts in reducing speeding incidents, enhancing road safety, and ultimately improving the quality of life for Portland residents. Through a data-driven approach, we aim to uncover valuable information that can inform targeted interventions and strategies to address the root causes of speeding behaviors on Portland's roadways. By emphasizing the gravity of the issue and the necessity for a comprehensive analysis, this introduction sets the stage for the subsequent sections of our project, which delve into our methodology, findings, and recommendations.","title":"Introduction"},{"location":"Traffic_Analysis/Traffic_Analysis/#problem-statement","text":"What are the significant threats to road safety posed by the prevalence of speeding vehicles in Portland despite existing speed limit regulations, instances of vehicles exceeding assigned speed limits persist, leading to a range of road safety issues including accidents, injuries, and fatalities. What are the underlying factors contributing to speeding behaviors hampers effective mitigation efforts by the Portland Transportation Department. The underlying factors contributing to speeding behaviors are multifaceted, encompassing individual, environmental, and societal factors. These include driver attitudes, perceptions of risk, roadway characteristics, inadequate signage, societal norms, and limited enforcement resources. These factors collectively create a complex landscape that hampers the department's ability to effectively address speeding incidents. In light of these challenges, the problem statement for this report is to comprehensively analyze data sourced from the Portland Transportation Department to identify areas where drivers tend to overspeed and discern trends in overspeeding over the years. Additionally, the report aims to explore correlations between speeding and various factors such as location, types of vehicles, time, and speed limits. Ultimately, the analysis aims to provide actionable insights to support efforts in reducing speeding incidents, enhancing road safety, and improving the quality of life for Portland residents.","title":"Problem Statement"},{"location":"Traffic_Analysis/Traffic_Analysis/#methodology","text":"","title":"Methodology"},{"location":"Traffic_Analysis/Traffic_Analysis/#data-source-description","text":"The data was compiled from three CSV files, all of which were obtained from the City of Portland, Oregon's PortlandMaps Open Data system , powered by ArcGIS. The three CSVs were from: Traffic Volume Count : Information on the volume of traffic flow observed at various locations within Portland. It includes data on the number of vehicles passing through specific points over designated time periods, offering insights into traffic patterns and congestion levels. Vehicle Class Count : The distribution of vehicles by class or type observed on Portland's roadways. It categorizes vehicles into different classes such as cars, trucks, motorcycles, and bicycles, providing valuable information on the composition of the traffic fleet. Traffic Speed Count Count : Data on the speeds at which vehicles are traveling along different segments of Portland's road network. It includes information on both average and individual vehicle speeds, allowing for the analysis of speeding trends and patterns across various locations and times. Each CSV had multiple shared variables, mostly related to the date, location, and other identifying elements of each sample count. The different CSV's each had their own unique variables, as well, that were specific to what was being counted. For example, Traffic Volume Count provides details on AM volume vs. PM volumes, Vehicle Class Count includes percentages of trucks vs. cars, and Traffic Speed Count Count provides percentages of passing traffic that fall into ranges based on the local speed limit.","title":"Data Source &amp; Description"},{"location":"Traffic_Analysis/Traffic_Analysis/#data-preparation","text":"Prior to conducting any sort of analysis, we needed to clean and combine the three CSV files. This process incolved the following steps:","title":"Data Preparation"},{"location":"Traffic_Analysis/Traffic_Analysis/#read-in-data","text":"After reading in each CSV as its own dataframe, we dropped the following columns in each dataframe: * StartTime * EndTime * ChannelNum * NumChannels * Comment * LocationClass * Conditions * DeviceRef These columns were specifically selected because they were either duplicative, had minimal value to our end analysis, or frequently had no data.","title":"Read in Data"},{"location":"Traffic_Analysis/Traffic_Analysis/#establish-ids","text":"In reviewing the dataframes we were able to establish that the same sample counts were used to provide the data for each CSV. This was determined because the values for different identifiers, such as location and date, were the same when the samples between different dataframe had matching values for the beginning of CountID (a CSV-specific identifier composed of 8 digits followed by by a CSV label) and Bound (the traffic direction). With that information, we were able to establish identifiers that connected samples between the three dataframes. To do so, we cleaned the Bound values as they had varying capitalization and other structural differences, then connected that value to the beginning digits of the CountID. For example, the same sample count in each dataframe had the following CountID and Bound values: * Speed: 10010422.SP2 \u2013 E * Class: 10010422.CL2 \u2013 E * Volume: 10010422.VL2 \u2013 E We were then able to create a unique ID in each dataframe for these samples that matched across the three dataframes: 10010422_E We needed to include the Bound element because the same CountID would be used multiple times for cross-directional traffic. For example, in the instance provided above, each dataframe had another sample with an identical CountID, but the Bound value was W (rather than E). As such, that sample was given the ID of 10010422_W.","title":"Establish IDs"},{"location":"Traffic_Analysis/Traffic_Analysis/#merge-dataframes","text":"After the unique identifiers were established, we could merge the three dataframes into one. For this, we chose the newly created ID column, along with StartDate and EndDate as the merge points, while all other duplicated columns were given a suffix that included the source dataframe's column name. This ensured we could continue reviewing the unique information in each dataframe prior to removing duplicative details. After the initial merge, there were 440 samples with duplicate IDs from a total of 22,736 samples. As such, we dropped the rows with duplicate IDs. Following the removal of the duplicated IDs, we iterated through the dataframe to fill in NAN values in columns repeated in multiple dataframes with the values from the other dataframes (i.e. if the Speed dataframe was missing a value in LocationID, but the Volume dataframe contained that value, then the value from Volume replaced the NAN for Speed). Once we ensured there were no missing values in any of the duplicated variables, and ran through the input values to check for consistancy, we selected the columns imported from the Speed dataframe as the \"source of truth\" and removed the redundant columns. Following this, we reviewed samples with missing values that could be calculated and filled in those values where it made sense. For example, the values in PctCars and PctTrucks always added up to 1, so if one value was missing it could be calculated by subtracting the provided value from 1. Lastly, we removed remaining samples with NAN values. This had the impact of reducing the final dataframe from about 20,000 samples to just over 7,000. However, a large reason for this is that the Class dataframe had just over 7,000 samples so there was a large discrepency between that dataframe's samples and the other two. To ensure we didn't have large gaps in our timeline data, though, we also ran an analysis comparing the four dataframes (Speed, Volume, Class, and the merged dataframe) and their relative counts by turning the counts for each month into a percent of the total count for that dataframe. As shown in the gragh below, the darker blue line representing the merged dataframe has the same approximate amount of counts each month as the other three, even though the final total count is much smaller:","title":"Merge Dataframes"},{"location":"Traffic_Analysis/Traffic_Analysis/#data-exploration","text":"Prior to feature selection the newly merged data frame was explored for further understanding. Firstly, the distribution of values within the Posted Speed and PctOverPosted columns were explored utilizing the pyplot package for boxplots and simple IQR calculations to remove outliers. The following boxplots depict PctOverPosted and PostedSpeed ebfore and after outlier removal: Next, aggregate statistics were calculated by Start Day for PctOverPosted, PctOverPosted10, and ADTVolume to explore the variation in speeding and traffic behavior by day. In order to better understand the difference between posted speed limits and the reported speed percentiles, differences were calculated for each data point. The top 15 largest differences are displayed below, grouped by location description: Finally, the following graphs depicting PctOverPosted and PctOverPosted10 over month and year timeframes were created using pyplot to uncover any potential historical or seasonal trends within the data:","title":"Data Exploration"},{"location":"Traffic_Analysis/Traffic_Analysis/#feature-selection","text":"PctOverPosted10 was identified as the initial target variable for this analysis however, after early modelling, the team determined to create a new binary classification from PctOverPosted10 for greater simplicity and explainability. This binary variable is 1 for any row with a PctOverPosted10 greater than 0, otherwise 0. In other words, classifying any measurements where speeds 10MPH or greater over the speed limit were measured. This new binary variable was named 10OverSpeeding and was utilized as the target variable in the final analysis. Future research using this same dataset could vary the cuytoff threshold for this binary variable in order to better target the most severe speeding cases. An initial list of features of interest was defined including the following variables: 10OverSpeeding, AMPkHrVol, AMVolume, PMPkHrVol, PMVolume, PctCars, PostedSpeed, X, ADTVolume, Y, StartDay, and Month. Dummy variables were created for StartDay using the pandas get_dummies() function. These features were then run through automated selection where scikit-learn was used to rank them by importance in the Recursive Feature Elimination (RFE) process with a Decision Tree classifier as the estimator. The top 10 features were maintained and the following final correlation matrix was generated using matplotlib and seaborn for heatmap visualization:","title":"Feature Selection"},{"location":"Traffic_Analysis/Traffic_Analysis/#modeling","text":"","title":"Modeling"},{"location":"Traffic_Analysis/Traffic_Analysis/#decision-tree","text":"The first model analyzed in this project was a Decision Tree Classifier. First, the data were split into training and testing sets using scikit-learn's train_test_split function for the features defined in the previous section. The data was divided into 80% training and 20% testing with stratification to ensure the class distribution between sets was not imbalanced. Next, scikit-learn's Grid Search Cross-Validation was used to determine the ideal parameters for the Decision Tree. The Tree was then fit on the training data, and predicted on the training and testing sets. The accuracy of the model was assessed using scikit-learn's accuracy_score, precision_score, and recall_score for both training and testing sets. A confusion matrix was also generated for both sets using pyplot and seaborn packages, as shown here: Based on the Decision Tree rules, the predictive features for the next models were narrowed down from 10 to just the top 5. The feature's importance was calculated and then ranked. The data was then scaled using scikit-learn's StandardScaler and the training and testing sets were redefined using the new top 5 features.","title":"Decision Tree"},{"location":"Traffic_Analysis/Traffic_Analysis/#logistic-regression","text":"A Logistic Regression model was fit on the training data for the top 5 features defined above. The data was divided into 80% training and 20% testing sets with stratification just like the previous Decision Tree model. Predictions were then made on both the training and testing sets. To evaluate the model's performance and ability to generalize on new data confusion matrices plus accuracy, precision, and recall scores were calculated and printed for both sets. The following confusion matrix depicts the predictive outcomes from the Logistic Regressions's testing set:","title":"Logistic Regression"},{"location":"Traffic_Analysis/Traffic_Analysis/#k-nearest-neighbors","text":"Following the procedure of the Logistic Regression analysis, a K-NN Classification model was also utilized. The K-NN model was fit using the same training set as defined above and then predictions were made on both the training and testing sets to assess performance and generalizability. Predictions from the model were then visualized using confusion matrices, with the testing matrix shown here:","title":"K-Nearest Neighbors"},{"location":"Traffic_Analysis/Traffic_Analysis/#stacked-ensemble","text":"The final model was a stacked ensemble, which combined the three preceding models to develop a robust model encompassing the strengths of the prior three. With the model, we were able to attain the following results: * Accuracy: .863 * Precision: .89 * Recall: .947 The confusion matrix shown here demonstrates the overall model reliability: The final performance indicators for each model, shown below, demonstrate the benefit of using the stacked ensemble, with the prior three models as its backbone, for our final model: \\begin{array}{ c c c c } \\textbf{Model} & \\textbf{Accuracy} & \\textbf{Precision} & \\textbf{Recall} \\ Decision Tree & 0.836 & 0.883 & 0.917 \\ Logistic Regression & 0.802 & 0.802 & 1.0 \\ KNN Classifier & 0.828 & 0.884 & 0.905 \\ Stacked Ensemble & 0.863 & 0.890 & 0.947 \\end{array}","title":"Stacked Ensemble"},{"location":"Traffic_Analysis/Traffic_Analysis/#analysis-findings","text":"The analysis and findings reveal critical insights into speeding incidents on Portland's roadways, derived from comprehensive examination of traffic volume, vehicle class, and traffic speed data sets. Through this analysis, we identified the top 15 locations with the greatest mean speed exceeding posted limits, comparing them with overspeed percentiles. Notably, SE Division St E of SE 33rd Ave ranked highest, with SE 112th Ave S of SE Ogden St and Columbia Way - Columbia Blvd Ramp W of N Columbia Blvd following closely. Additionally, we examined the top 15 locations with the highest speeds for the 90th percentile, further highlighting areas of concern for overspeeding. Moreover, historical trends from 2010 to 2023 indicated fluctuations in overspeeding, with peak incidents occurring during AM and PM rush hours. Predictive modeling using decision trees, logistic regression, K-NN, and stacked ensembles identified key predictors such as AM Volume, X, Y, ADT Volume, and Pct Cars. Our models demonstrated strong performance metrics, with logistic regression showing an accuracy of 0.802, precision of 0.802, and recall of 1 for both training and testing data sets. Similarly, the KNN classifier exhibited robust performance with comparable metrics. The greatest overall accuracy was seen in the stacked ensemble model which had an accuracy of 0.863, precision of 0.890, and recall of 0.947. The comparison of all models yielded valuable insights, with the stacked ensemble having the greatest overall performance. This high performance on the testing datasets indicates that the models are able to generalize well to new data. This could make them useful tools for predicting whether overspeeding incidences will occur based on key traffic and location metrics. Overall, these findings underscore the importance of data-driven approaches in understanding and addressing speeding behaviors to enhance road safety in Portland.","title":"Analysis &amp; Findings"},{"location":"Traffic_Analysis/Traffic_Analysis/#business-applications","text":"The findings and analysis presented in this project hold significant implications for improving road safety and transportation management in Portland. By identifying key locations with high instances of speeding and understanding the underlying factors contributing to these behaviors, transportation authorities can implement targeted interventions to mitigate risks and enhance road safety. These interventions may include the deployment of speed enforcement measures, such as speed cameras or increased police patrols, in identified hotspot areas. Additionally, the insights gained from historical trends and predictive modeling can inform the development of proactive strategies to anticipate and address future challenges related to speeding incidents. The identification of key predictors for speeding behaviors, such as traffic volume and time of day, can inform urban planning and infrastructure development initiatives. For instance, city planners can use this information to optimize traffic flow and design safer roadways that effectively accommodate varying traffic conditions. Moreover, the predictive models developed in this project can serve as valuable decision support tools for transportation agencies, enabling them to allocate resources more efficiently and prioritize interventions based on the likelihood of speeding incidents occurring in specific locations.","title":"Business Applications"},{"location":"Traffic_Analysis/Traffic_Analysis/#recommendations","text":"Based on the analysis and findings presented in this project, several key recommendations can be made to Portland's Bureau of Transportation to improve road safety and address the issue of speeding incidents. Firstly, targeted enforcement measures may be implemented using the insights gained from the identification of high-speeding locations. Enforcement measures could include speed cameras or increased police presence in hotspot areas aimed at deterring speeding behaviors. Secondly, utilizing insights from traffic volume and vehicle classes, infrastructure investments may be made in targeted areas to calm traffic using features such as speed bumps or traffic islands. Most importantly, this project acts as a proof of concept that data driven decision-making is possible from the wealth of data collected by the City of Portland. Our key recommendation is that PBOT continues to update and maintain this data, and further supplementing it with other relevant factors to inform decision-making processes related to road safety and transportation management. While the data currently available provides some helpful insights, any additional data which can be incorporated for speed enforcement measures, infrastructure updates, or other key influences will be crucial for measuring and predicting future success. Finally, PBOT should continue to collaborate closely with community stakeholders and provide regular data updates and reports. Collaborative efforts can help build consensus around proposed interventions and ensure their effectiveness in addressing community concerns.","title":"Recommendations"},{"location":"Traffic_Analysis/Traffic_Analysis/#conclusion","text":"The project \"Predictive Analysis of Speeding Incidents by Vehicle Type, Location, and Time\" has provided valuable insights into the pressing issue of speeding incidents on Portland's roadways. Through comprehensive data analysis and predictive modeling, we have identified key factors contributing to speeding behaviors, highlighted high-speeding locations, and assessed historical trends in speeding incidents over time. Our findings underscore the urgent need for targeted interventions to address speeding and enhance road safety in Portland. This project has laid the foundation for evidence-based interventions to improve road safety and enhance the quality of life for Portland residents and visitors. By leveraging data analytics and predictive modeling, we can work towards creating a safer and more sustainable transportation system that benefits all members of the community.","title":"Conclusion"},{"location":"Traffic_Analysis/Traffic_Analysis/#references","text":"Traffic Counts Portland.gov NE Ainsworth Traffic Calming Projects","title":"References"},{"location":"Traffic_Analysis/Traffic_Analysis/#code-appendix","text":"","title":"Code Appendix"},{"location":"Traffic_Analysis/Traffic_Analysis/#preliminaries","text":"# import packages import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from tabulate import tabulate from sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor from datetime import datetime as dt from sklearn.feature_selection import RFE from sklearn.ensemble import StackingClassifier from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, precision_score, recall_score from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from prettytable import PrettyTable # set analysis date/time now = dt.now() print (\"Analysis on\", now.strftime(\"%Y-%m-%d\"), \"at\", now.strftime(\"%H:%M\")) Analysis on 2024-07-27 at 10:52 Analysis on 2024-07-27 at 10:52","title":"Preliminaries"},{"location":"Traffic_Analysis/Traffic_Analysis/#data-preparation_1","text":"# read in and name speed csv SpeedDF = pd.read_csv('data/TRAFFIC-Speed_Counts.csv') SpeedDF.name = 'SpeedDF' SpeedDF.head().transpose() <positron-console-cell-8>:2: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663930.6133 -13663930.6133 -13663904.8261 Y 5722466.9793 5722466.9793 5711463.6992 5711463.6992 5711466.2873 OBJECTID 1 2 3 4 5 CountID 10010422.SP2 10010422.SP2 10010430.sp2 10010430.sp2 10010421.SP1 ChannelID 505638 505639 505648 505649 505642 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW 61ST AVE S of FRONT AVE NW 61ST AVE S of FRONT AVE NW FRONT AVE E of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 StartDay MON MON MON MON MON StartTime 12:00:00 12:00:00 13:00:00 13:00:00 13:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 EndDay TUE TUE TUE TUE WED EndTime 23:00:00 23:00:00 23:00:00 23:00:00 14:00:00 ADTVolume 3048 3405 847 852 1524 Spd50th 39 39 20 19 33 Spd70th 43 44 23 22 37 Spd85th 46 48 26 25 42 Spd90th 47 49 27 26 44 PostedSpeed 45 45 25 25 40 PctOverPosted 17.9 25.7 17.7 14.1 20.6 PctOverPosted10 0.6 1.5 0.2 0.2 2.2 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 2 2 1 ChannelNum 1 2 1 2 1 NumSlots 13 13 13 13 13 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 36 36 35 35 50 IntervalLen 60 60 60 60 60 DeviceRef 0022 0022 0030 0030 0021 LocationID LEG11527 LEG11527 LEG36313 LEG36313 LEG37236 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW 61ST AVE W/NW FRONT AVE NW 61ST AVE W/NW FRONT AVE NW FRONT AVE S/NW 61ST AVE CountType SPEED SPEED SPEED SPEED SPEED # read in and name class csv ClassDF = pd.read_csv('data/TRAFFIC-Class_Counts.csv') ClassDF.name = 'ClassDF' ClassDF.head().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663904.8261 -13663904.8261 -13663933.6073 Y 5722466.9793 5722466.9793 5711466.2873 5711466.2873 5711498.9549 OBJECTID 1 2 3 4 5 CountID 10010422.CL2 10010422.CL2 10010421.CL1 10010432.CL1 10011227.CL2 ChannelID 500769 500770 500773 500776 500781 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW FRONT AVE E of 61ST AVE NW FRONT AVE E of 61ST AVE NW FRONT AVE N of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/12 00:00:00+00 StartDay MON MON MON MON TUE StartTime 12:00:00 12:00:00 13:00:00 13:00:00 15:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 2010/01/06 00:00:00+00 2010/01/14 00:00:00+00 EndDay TUE TUE WED WED THU EndTime 23:00:00 23:00:00 14:00:00 13:00:00 07:00:00 ADTVolume 3049 3409 1523 1450 640 PctCars 63.5 65.6 65.3 63.3 92.1 PctTrucks 36.5 34.4 34.7 36.7 7.9 TwoAxleCF 0.749 0.773 0.857 0.781 0.972 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 1 1 2 ChannelNum 1 2 1 1 1 NumCategories 14 14 14 14 14 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 36 36 50 49 41 IntervalLen 60 60 60 60 60 DeviceRef 0022 0022 0021 0032 0027 LocationID LEG11527 LEG11527 LEG37236 LEG37236 LEG37233 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW FRONT AVE S/NW 61ST AVE NW FRONT AVE S/NW 61ST AVE NW FRONT AVE N/NW 61ST AVE CountType CLASS CLASS CLASS CLASS CLASS # read in and name volume csv VolumeDF = pd.read_csv('data/TRAFFIC-Volume_Counts.csv') VolumeDF.name = 'VolumeDF' VolumeDF.head().transpose() <positron-console-cell-10>:2: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 X -13666261.3309 -13666261.3309 -13663930.6133 -13663930.6133 -13663904.8261 Y 5722466.9793 5722466.9793 5711463.6992 5711463.6992 5711466.2873 OBJECTID 1 2 3 4 5 CountID 10010422.VL2 10010422.VL2 10010430.vl2 10010430.vl2 10010421.VL1 ChannelID 506851 506852 506872 506873 506855 LocationDesc N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW 61ST AVE S of FRONT AVE NW 61ST AVE S of FRONT AVE NW FRONT AVE E of 61ST AVE Bound E W N S N StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 StartDay MON MON MON MON MON StartTime 11:15:00 11:15:00 12:30:00 12:30:00 13:00:00 EndDate 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 EndDay TUE TUE TUE TUE WED EndTime 23:45:00 23:45:00 23:45:00 23:45:00 15:15:00 ADTVolume 3033 3376 839 852 1491 AMVolume 1227 1992 432 512 632 AMPkHrVol 217 343 109 107 149 AMPkHrTime 2010/01/04 07:30:00+00 2010/01/04 06:15:00+00 2010/01/04 10:30:00+00 2010/01/04 09:30:00+00 2010/01/04 06:45:00+00 AMPkHrFactor 0.798 0.78 0.619 0.622 0.532 PMVolume 1806 1384 407 340 859 PMPkHrVol 333 253 77 79 134 PMPkHrTime 2010/01/04 15:15:00+00 2010/01/04 14:15:00+00 2010/01/04 12:15:00+00 2010/01/04 12:30:00+00 2010/01/04 18:30:00+00 PMPkHrFactor 0.771 0.917 0.962 0.859 0.435 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday NumChannels 2 2 2 2 1 ChannelNum 1 2 1 2 1 Conditions NaN NaN NaN NaN NaN Comment 15925 15925 NaN NaN NaN Duration 147 147 142 142 202 IntervalLen 15 15 15 15 15 DeviceRef 0022 0022 0030 0030 0021 LocationID LEG11527 LEG11527 LEG36313 LEG36313 LEG37236 LocationClass NODELEG NODELEG NODELEG NODELEG NODELEG CountLocDesc N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW 61ST AVE W/NW FRONT AVE NW 61ST AVE W/NW FRONT AVE NW FRONT AVE S/NW 61ST AVE CountType VOLUME VOLUME VOLUME VOLUME VOLUME # establish dataframe iterable DFs = [VolumeDF, ClassDF, SpeedDF] # drop columns known to not be needed for i in DFs: i.drop(columns=['StartTime', 'EndTime', 'ChannelNum', 'NumChannels', 'Comment', 'LocationClass', 'Conditions', 'DeviceRef'], inplace=True) i.info() print('\\n') <class 'pandas.core.frame.DataFrame'> RangeIndex: 21056 entries, 0 to 21055 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 21056 non-null float64 1 Y 21056 non-null float64 2 OBJECTID 21056 non-null int64 3 CountID 21056 non-null object 4 ChannelID 21056 non-null int64 5 LocationDesc 21056 non-null object 6 Bound 21056 non-null object 7 StartDate 21056 non-null object 8 StartDay 21056 non-null object 9 EndDate 21056 non-null object 10 EndDay 21056 non-null object 11 ADTVolume 21056 non-null int64 12 AMVolume 21056 non-null int64 13 AMPkHrVol 21056 non-null int64 14 AMPkHrTime 21056 non-null object 15 AMPkHrFactor 21056 non-null float64 16 PMVolume 21056 non-null int64 17 PMPkHrVol 21056 non-null int64 18 PMPkHrTime 21056 non-null object 19 PMPkHrFactor 21056 non-null float64 20 ExceptType 21056 non-null object 21 Duration 21056 non-null int64 22 IntervalLen 21056 non-null int64 23 LocationID 21056 non-null object 24 CountLocDesc 21056 non-null object 25 CountType 21056 non-null object dtypes: float64(4), int64(9), object(13) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 7849 entries, 0 to 7848 Data columns (total 22 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 7849 non-null float64 1 Y 7849 non-null float64 2 OBJECTID 7849 non-null int64 3 CountID 7849 non-null object 4 ChannelID 7849 non-null int64 5 LocationDesc 7849 non-null object 6 Bound 7844 non-null object 7 StartDate 7849 non-null object 8 StartDay 7849 non-null object 9 EndDate 7849 non-null object 10 EndDay 7849 non-null object 11 ADTVolume 7849 non-null int64 12 PctCars 7849 non-null float64 13 PctTrucks 7849 non-null float64 14 TwoAxleCF 7849 non-null float64 15 ExceptType 7849 non-null object 16 NumCategories 7849 non-null int64 17 Duration 7849 non-null int64 18 IntervalLen 7849 non-null int64 19 LocationID 7849 non-null object 20 CountLocDesc 7849 non-null object 21 CountType 7849 non-null object dtypes: float64(5), int64(6), object(11) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 17016 entries, 0 to 17015 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 17016 non-null float64 1 Y 17016 non-null float64 2 OBJECTID 17016 non-null int64 3 CountID 17016 non-null object 4 ChannelID 17016 non-null int64 5 LocationDesc 17016 non-null object 6 Bound 17011 non-null object 7 StartDate 17016 non-null object 8 StartDay 17016 non-null object 9 EndDate 17016 non-null object 10 EndDay 17016 non-null object 11 ADTVolume 17016 non-null int64 12 Spd50th 17016 non-null int64 13 Spd70th 17016 non-null int64 14 Spd85th 17016 non-null int64 15 Spd90th 17016 non-null int64 16 PostedSpeed 17016 non-null int64 17 PctOverPosted 17016 non-null float64 18 PctOverPosted10 17016 non-null float64 19 ExceptType 17016 non-null object 20 NumSlots 17016 non-null int64 21 Duration 17016 non-null int64 22 IntervalLen 17016 non-null int64 23 LocationID 17016 non-null object 24 CountLocDesc 17016 non-null object 25 CountType 17016 non-null object dtypes: float64(4), int64(11), object(11) memory usage: 3.4+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 21056 entries, 0 to 21055 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 21056 non-null float64 1 Y 21056 non-null float64 2 OBJECTID 21056 non-null int64 3 CountID 21056 non-null object 4 ChannelID 21056 non-null int64 5 LocationDesc 21056 non-null object 6 Bound 21056 non-null object 7 StartDate 21056 non-null object 8 StartDay 21056 non-null object 9 EndDate 21056 non-null object 10 EndDay 21056 non-null object 11 ADTVolume 21056 non-null int64 12 AMVolume 21056 non-null int64 13 AMPkHrVol 21056 non-null int64 14 AMPkHrTime 21056 non-null object 15 AMPkHrFactor 21056 non-null float64 16 PMVolume 21056 non-null int64 17 PMPkHrVol 21056 non-null int64 18 PMPkHrTime 21056 non-null object 19 PMPkHrFactor 21056 non-null float64 20 ExceptType 21056 non-null object 21 Duration 21056 non-null int64 22 IntervalLen 21056 non-null int64 23 LocationID 21056 non-null object 24 CountLocDesc 21056 non-null object 25 CountType 21056 non-null object dtypes: float64(4), int64(9), object(13) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 7849 entries, 0 to 7848 Data columns (total 22 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 7849 non-null float64 1 Y 7849 non-null float64 2 OBJECTID 7849 non-null int64 3 CountID 7849 non-null object 4 ChannelID 7849 non-null int64 5 LocationDesc 7849 non-null object 6 Bound 7844 non-null object 7 StartDate 7849 non-null object 8 StartDay 7849 non-null object 9 EndDate 7849 non-null object 10 EndDay 7849 non-null object 11 ADTVolume 7849 non-null int64 12 PctCars 7849 non-null float64 13 PctTrucks 7849 non-null float64 14 TwoAxleCF 7849 non-null float64 15 ExceptType 7849 non-null object 16 NumCategories 7849 non-null int64 17 Duration 7849 non-null int64 18 IntervalLen 7849 non-null int64 19 LocationID 7849 non-null object 20 CountLocDesc 7849 non-null object 21 CountType 7849 non-null object dtypes: float64(5), int64(6), object(11) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> RangeIndex: 17016 entries, 0 to 17015 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 X 17016 non-null float64 1 Y 17016 non-null float64 2 OBJECTID 17016 non-null int64 3 CountID 17016 non-null object 4 ChannelID 17016 non-null int64 5 LocationDesc 17016 non-null object 6 Bound 17011 non-null object 7 StartDate 17016 non-null object 8 StartDay 17016 non-null object 9 EndDate 17016 non-null object 10 EndDay 17016 non-null object 11 ADTVolume 17016 non-null int64 12 Spd50th 17016 non-null int64 13 Spd70th 17016 non-null int64 14 Spd85th 17016 non-null int64 15 Spd90th 17016 non-null int64 16 PostedSpeed 17016 non-null int64 17 PctOverPosted 17016 non-null float64 18 PctOverPosted10 17016 non-null float64 19 ExceptType 17016 non-null object 20 NumSlots 17016 non-null int64 21 Duration 17016 non-null int64 22 IntervalLen 17016 non-null int64 23 LocationID 17016 non-null object 24 CountLocDesc 17016 non-null object 25 CountType 17016 non-null object dtypes: float64(4), int64(11), object(11) memory usage: 3.4+ MB Here, the DataFrame in the list 'DFs' and generates value counts for the 'Bound', 'StartDay', and 'EndDay' columns, providing insights into the distribution of these variables. # review values in the bound column for i in DFs: print(i['Bound'].value_counts()) print(i['StartDay'].value_counts()) print(i['EndDay'].value_counts(),'\\n') Bound E 5703 W 5691 N 4819 S 4735 D 19 s 19 n 18 e 16 w 16 E-S 7 L 4 T 2 / 2 SE 1 NW 1 B 1 C 1 M 1 Name: count, dtype: int64 StartDay MON 5728 TUE 5470 WED 4878 THU 3556 SAT 664 SUN 554 FRI 206 Name: count, dtype: int64 EndDay FRI 10404 THU 4298 WED 3458 TUE 1499 SUN 638 SAT 590 MON 169 Name: count, dtype: int64 Bound E 2123 W 2084 N 1808 S 1779 NB LL 8 EB 5 NB RL 4 BND 4 S RL 4 30 2 EW 2 S.BND 2 SB 2 WB 2 WBND 2 .W 2 NS 1 NB 1 s 1 n 1 20 1 35 1 NBND 1 EBND 1 /S 1 e 1 B 1 Name: count, dtype: int64 StartDay MON 2325 TUE 2229 WED 1906 THU 1180 SAT 106 SUN 85 FRI 18 Name: count, dtype: int64 EndDay FRI 4082 THU 1777 WED 1314 TUE 454 SUN 100 SAT 98 MON 24 Name: count, dtype: int64 Bound E 4489 W 4454 N 4018 S 4002 BND 11 EB 6 .W 3 e 3 w 2 EW 2 SB 2 B 2 25 2 NS 2 n 1 s 1 NB 1 NBND 1 EBND 1 30 1 WBND 1 /S 1 NB LL 1 WB 1 SE 1 NW 1 S.BND 1 Name: count, dtype: int64 StartDay MON 4722 TUE 4644 WED 4078 THU 2878 SAT 365 SUN 261 FRI 68 Name: count, dtype: int64 EndDay FRI 8656 THU 3612 WED 2873 TUE 1172 SUN 337 SAT 308 MON 58 Name: count, dtype: int64 Bound E 5703 W 5691 N 4819 S 4735 D 19 s 19 n 18 e 16 w 16 E-S 7 L 4 T 2 / 2 SE 1 NW 1 B 1 C 1 M 1 Name: count, dtype: int64 StartDay MON 5728 TUE 5470 WED 4878 THU 3556 SAT 664 SUN 554 FRI 206 Name: count, dtype: int64 EndDay FRI 10404 THU 4298 WED 3458 TUE 1499 SUN 638 SAT 590 MON 169 Name: count, dtype: int64 Bound E 2123 W 2084 N 1808 S 1779 NB LL 8 EB 5 NB RL 4 BND 4 S RL 4 30 2 EW 2 S.BND 2 SB 2 WB 2 WBND 2 .W 2 NS 1 NB 1 s 1 n 1 20 1 35 1 NBND 1 EBND 1 /S 1 e 1 B 1 Name: count, dtype: int64 StartDay MON 2325 TUE 2229 WED 1906 THU 1180 SAT 106 SUN 85 FRI 18 Name: count, dtype: int64 EndDay FRI 4082 THU 1777 WED 1314 TUE 454 SUN 100 SAT 98 MON 24 Name: count, dtype: int64 Bound E 4489 W 4454 N 4018 S 4002 BND 11 EB 6 .W 3 e 3 w 2 EW 2 SB 2 B 2 25 2 NS 2 n 1 s 1 NB 1 NBND 1 EBND 1 30 1 WBND 1 /S 1 NB LL 1 WB 1 SE 1 NW 1 S.BND 1 Name: count, dtype: int64 StartDay MON 4722 TUE 4644 WED 4078 THU 2878 SAT 365 SUN 261 FRI 68 Name: count, dtype: int64 EndDay FRI 8656 THU 3612 WED 2873 TUE 1172 SUN 337 SAT 308 MON 58 Name: count, dtype: int64 # update bound values for uniformity for i in DFs: i['Bound'] = i['Bound'].str.upper() i['Bound'] = i['Bound'].str.replace('E-S', 'SE', regex=False) i['Bound'] = i['Bound'].str.replace('EB.*', 'E', regex=True) i['Bound'] = i['Bound'].str.replace('\\?.WB.*', 'W', regex=True) i['Bound'] = i['Bound'].str.replace('(NB.*)|(BND)', 'N', regex=True) i['Bound'] = i['Bound'].str.replace('\\?/S?/s?\\.B.*', 'S', regex=True) # establish df to input duplicate ids duplicateIDs = pd.DataFrame() # iterate through dfs to create ids & check for duplicates for i in DFs: i['ID'] = i['CountID'].str.split('.', n=1, expand=True)[0]+'_'+i['Bound']+'_' # generate IDs i.insert(0, 'ID', i.pop('ID')) # move IDs to first columns duplicateIDs = pd.concat([duplicateIDs,i[i['ID'].duplicated()]]) # add duplicated ID in single df to duplicateIDs i.drop(columns=['CountID', 'Bound'], inplace=True) print(i.name,':') print(' ',len(i.index),'rows') print(' ',i['ID'].nunique(),'unique IDs\\n') <positron-console-cell-13>:6: SyntaxWarning: invalid escape sequence '\\?' <positron-console-cell-13>:8: SyntaxWarning: invalid escape sequence '\\?' VolumeDF : 21056 rows 21040 unique IDs ClassDF : 7849 rows 7839 unique IDs VolumeDF : 21056 rows 21040 unique IDs ClassDF : 7849 rows 7839 unique IDs SpeedDF : 17016 rows 17005 unique IDs SpeedDF : 17016 rows 17005 unique IDs # check size of duplicate ids duplicateIDs.shape (35, 39) # drop duplicate id rows from each df (if low impact) for i in DFs: i.drop(i[i['ID'].duplicated()].index, axis=0, inplace=True) # list column names used for merge connections = ['ID', 'StartDate', 'EndDate'] # set name suffixes for columns suffixes = ['_Volume', '_Class', '_Speed'] # same order as DFs variable # establish list of repeated column names repeats = list(set(SpeedDF.columns) & set(VolumeDF.columns) & set(ClassDF.columns)) repeats = [i for i in repeats if i not in connections] # iterate through dfs & repeated columns to add suffixes y = 0 for i in DFs: for x in repeats: i.rename(columns={x: x + suffixes[y]}, inplace=True) i.info() y += 1 <class 'pandas.core.frame.DataFrame'> Index: 21040 entries, 0 to 21055 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 21040 non-null object 1 X_Volume 21040 non-null float64 2 Y_Volume 21040 non-null float64 3 OBJECTID_Volume 21040 non-null int64 4 ChannelID_Volume 21040 non-null int64 5 LocationDesc_Volume 21040 non-null object 6 StartDate 21040 non-null object 7 StartDay_Volume 21040 non-null object 8 EndDate 21040 non-null object 9 EndDay_Volume 21040 non-null object 10 ADTVolume_Volume 21040 non-null int64 11 AMVolume 21040 non-null int64 12 AMPkHrVol 21040 non-null int64 13 AMPkHrTime 21040 non-null object 14 AMPkHrFactor 21040 non-null float64 15 PMVolume 21040 non-null int64 16 PMPkHrVol 21040 non-null int64 17 PMPkHrTime 21040 non-null object 18 PMPkHrFactor 21040 non-null float64 19 ExceptType_Volume 21040 non-null object 20 Duration_Volume 21040 non-null int64 21 IntervalLen_Volume 21040 non-null int64 22 LocationID_Volume 21040 non-null object 23 CountLocDesc_Volume 21040 non-null object 24 CountType_Volume 21040 non-null object dtypes: float64(4), int64(9), object(12) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 7840 entries, 0 to 7848 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 7839 non-null object 1 X_Class 7840 non-null float64 2 Y_Class 7840 non-null float64 3 OBJECTID_Class 7840 non-null int64 4 ChannelID_Class 7840 non-null int64 5 LocationDesc_Class 7840 non-null object 6 StartDate 7840 non-null object 7 StartDay_Class 7840 non-null object 8 EndDate 7840 non-null object 9 EndDay_Class 7840 non-null object 10 ADTVolume_Class 7840 non-null int64 11 PctCars 7840 non-null float64 12 PctTrucks 7840 non-null float64 13 TwoAxleCF 7840 non-null float64 14 ExceptType_Class 7840 non-null object 15 NumCategories 7840 non-null int64 16 Duration_Class 7840 non-null int64 17 IntervalLen_Class 7840 non-null int64 18 LocationID_Class 7840 non-null object 19 CountLocDesc_Class 7840 non-null object 20 CountType_Class 7840 non-null object dtypes: float64(5), int64(6), object(10) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> Index: 17006 entries, 0 to 17015 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 17005 non-null object 1 X_Speed 17006 non-null float64 2 Y_Speed 17006 non-null float64 3 OBJECTID_Speed 17006 non-null int64 4 ChannelID_Speed 17006 non-null int64 5 LocationDesc_Speed 17006 non-null object 6 StartDate 17006 non-null object 7 StartDay_Speed 17006 non-null object 8 EndDate 17006 non-null object 9 EndDay_Speed 17006 non-null object 10 ADTVolume_Speed 17006 non-null int64 11 Spd50th 17006 non-null int64 12 Spd70th 17006 non-null int64 13 Spd85th 17006 non-null int64 14 Spd90th 17006 non-null int64 15 PostedSpeed 17006 non-null int64 16 PctOverPosted 17006 non-null float64 17 PctOverPosted10 17006 non-null float64 18 ExceptType_Speed 17006 non-null object 19 NumSlots 17006 non-null int64 20 Duration_Speed 17006 non-null int64 21 IntervalLen_Speed 17006 non-null int64 22 LocationID_Speed 17006 non-null object 23 CountLocDesc_Speed 17006 non-null object 24 CountType_Speed 17006 non-null object dtypes: float64(4), int64(11), object(10) memory usage: 3.4+ MB <class 'pandas.core.frame.DataFrame'> Index: 21040 entries, 0 to 21055 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 21040 non-null object 1 X_Volume 21040 non-null float64 2 Y_Volume 21040 non-null float64 3 OBJECTID_Volume 21040 non-null int64 4 ChannelID_Volume 21040 non-null int64 5 LocationDesc_Volume 21040 non-null object 6 StartDate 21040 non-null object 7 StartDay_Volume 21040 non-null object 8 EndDate 21040 non-null object 9 EndDay_Volume 21040 non-null object 10 ADTVolume_Volume 21040 non-null int64 11 AMVolume 21040 non-null int64 12 AMPkHrVol 21040 non-null int64 13 AMPkHrTime 21040 non-null object 14 AMPkHrFactor 21040 non-null float64 15 PMVolume 21040 non-null int64 16 PMPkHrVol 21040 non-null int64 17 PMPkHrTime 21040 non-null object 18 PMPkHrFactor 21040 non-null float64 19 ExceptType_Volume 21040 non-null object 20 Duration_Volume 21040 non-null int64 21 IntervalLen_Volume 21040 non-null int64 22 LocationID_Volume 21040 non-null object 23 CountLocDesc_Volume 21040 non-null object 24 CountType_Volume 21040 non-null object dtypes: float64(4), int64(9), object(12) memory usage: 4.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 7840 entries, 0 to 7848 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 7839 non-null object 1 X_Class 7840 non-null float64 2 Y_Class 7840 non-null float64 3 OBJECTID_Class 7840 non-null int64 4 ChannelID_Class 7840 non-null int64 5 LocationDesc_Class 7840 non-null object 6 StartDate 7840 non-null object 7 StartDay_Class 7840 non-null object 8 EndDate 7840 non-null object 9 EndDay_Class 7840 non-null object 10 ADTVolume_Class 7840 non-null int64 11 PctCars 7840 non-null float64 12 PctTrucks 7840 non-null float64 13 TwoAxleCF 7840 non-null float64 14 ExceptType_Class 7840 non-null object 15 NumCategories 7840 non-null int64 16 Duration_Class 7840 non-null int64 17 IntervalLen_Class 7840 non-null int64 18 LocationID_Class 7840 non-null object 19 CountLocDesc_Class 7840 non-null object 20 CountType_Class 7840 non-null object dtypes: float64(5), int64(6), object(10) memory usage: 1.3+ MB <class 'pandas.core.frame.DataFrame'> Index: 17006 entries, 0 to 17015 Data columns (total 25 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 17005 non-null object 1 X_Speed 17006 non-null float64 2 Y_Speed 17006 non-null float64 3 OBJECTID_Speed 17006 non-null int64 4 ChannelID_Speed 17006 non-null int64 5 LocationDesc_Speed 17006 non-null object 6 StartDate 17006 non-null object 7 StartDay_Speed 17006 non-null object 8 EndDate 17006 non-null object 9 EndDay_Speed 17006 non-null object 10 ADTVolume_Speed 17006 non-null int64 11 Spd50th 17006 non-null int64 12 Spd70th 17006 non-null int64 13 Spd85th 17006 non-null int64 14 Spd90th 17006 non-null int64 15 PostedSpeed 17006 non-null int64 16 PctOverPosted 17006 non-null float64 17 PctOverPosted10 17006 non-null float64 18 ExceptType_Speed 17006 non-null object 19 NumSlots 17006 non-null int64 20 Duration_Speed 17006 non-null int64 21 IntervalLen_Speed 17006 non-null int64 22 LocationID_Speed 17006 non-null object 23 CountLocDesc_Speed 17006 non-null object 24 CountType_Speed 17006 non-null object dtypes: float64(4), int64(11), object(10) memory usage: 3.4+ MB # merge dfs into one big one, keeping all values df = SpeedDF.merge(ClassDF, how='outer', on=connections).merge(VolumeDF, how='outer', on=connections) # check how many duplicated ids there are in the big df df[df['ID'].duplicated()].shape (440, 65) # compare the size of the whole df df.shape (22736, 65) # drop duplicated id rows if it makes sense df.drop(df[df['ID'].duplicated()].index, axis=0, inplace=True) df[df['ID'].duplicated()] # verify .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID X_Speed Y_Speed OBJECTID_Speed ChannelID_Speed LocationDesc_Speed StartDate StartDay_Speed EndDate EndDay_Speed ADTVolume_Speed Spd50th Spd70th Spd85th Spd90th PostedSpeed PctOverPosted PctOverPosted10 ExceptType_Speed NumSlots Duration_Speed IntervalLen_Speed LocationID_Speed CountLocDesc_Speed CountType_Speed X_Class Y_Class OBJECTID_Class ChannelID_Class LocationDesc_Class StartDay_Class EndDay_Class ADTVolume_Class PctCars PctTrucks TwoAxleCF ExceptType_Class NumCategories Duration_Class IntervalLen_Class LocationID_Class CountLocDesc_Class CountType_Class X_Volume Y_Volume OBJECTID_Volume ChannelID_Volume LocationDesc_Volume StartDay_Volume EndDay_Volume ADTVolume_Volume AMVolume AMPkHrVol AMPkHrTime AMPkHrFactor PMVolume PMPkHrVol PMPkHrTime PMPkHrFactor ExceptType_Volume Duration_Volume IntervalLen_Volume LocationID_Volume CountLocDesc_Volume CountType_Volume # check for null counts df.isnull().sum() ID 1 X_Speed 5486 Y_Speed 5486 OBJECTID_Speed 5486 ChannelID_Speed 5486 ... Duration_Volume 1511 IntervalLen_Volume 1511 LocationID_Volume 1511 CountLocDesc_Volume 1511 CountType_Volume 1511 Length: 65, dtype: int64 # fill na in for repeated columns using values from other dfs for i in repeats: df[i+'_Speed'] = df[i+'_Speed'].fillna(df[i+'_Volume']) df[i+'_Volume'] = df[i+'_Volume'].fillna(df[i+'_Class']) df[i+'_Class'] = df[i+'_Class'].fillna(df[i+'_Speed']) df[i+'_Speed'] = df[i+'_Speed'].fillna(df[i+'_Volume']) df[i+'_Volume'] = df[i+'_Volume'].fillna(df[i+'_Class']) df[i+'_Class'] = df[i+'_Class'].fillna(df[i+'_Speed']) # sort columns alphabetically df = df.reindex(sorted(df.columns), axis=1) df.insert(0, 'ID', df.pop('ID')) # move ID to first column # initial condensed df df.info() <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 65 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 ADTVolume_Class 22296 non-null float64 2 ADTVolume_Speed 22296 non-null float64 3 ADTVolume_Volume 22296 non-null float64 4 AMPkHrFactor 20785 non-null float64 5 AMPkHrTime 20785 non-null object 6 AMPkHrVol 20785 non-null float64 7 AMVolume 20785 non-null float64 8 ChannelID_Class 22296 non-null float64 9 ChannelID_Speed 22296 non-null float64 10 ChannelID_Volume 22296 non-null float64 11 CountLocDesc_Class 22296 non-null object 12 CountLocDesc_Speed 22296 non-null object 13 CountLocDesc_Volume 22296 non-null object 14 CountType_Class 22296 non-null object 15 CountType_Speed 22296 non-null object 16 CountType_Volume 22296 non-null object 17 Duration_Class 22296 non-null float64 18 Duration_Speed 22296 non-null float64 19 Duration_Volume 22296 non-null float64 20 EndDate 22296 non-null object 21 EndDay_Class 22296 non-null object 22 EndDay_Speed 22296 non-null object 23 EndDay_Volume 22296 non-null object 24 ExceptType_Class 22296 non-null object 25 ExceptType_Speed 22296 non-null object 26 ExceptType_Volume 22296 non-null object 27 IntervalLen_Class 22296 non-null float64 28 IntervalLen_Speed 22296 non-null float64 29 IntervalLen_Volume 22296 non-null float64 30 LocationDesc_Class 22296 non-null object 31 LocationDesc_Speed 22296 non-null object 32 LocationDesc_Volume 22296 non-null object 33 LocationID_Class 22296 non-null object 34 LocationID_Speed 22296 non-null object 35 LocationID_Volume 22296 non-null object 36 NumCategories 7747 non-null float64 37 NumSlots 16810 non-null float64 38 OBJECTID_Class 22296 non-null float64 39 OBJECTID_Speed 22296 non-null float64 40 OBJECTID_Volume 22296 non-null float64 41 PMPkHrFactor 20785 non-null float64 42 PMPkHrTime 20785 non-null object 43 PMPkHrVol 20785 non-null float64 44 PMVolume 20785 non-null float64 45 PctCars 7747 non-null float64 46 PctOverPosted 16810 non-null float64 47 PctOverPosted10 16810 non-null float64 48 PctTrucks 7747 non-null float64 49 PostedSpeed 16810 non-null float64 50 Spd50th 16810 non-null float64 51 Spd70th 16810 non-null float64 52 Spd85th 16810 non-null float64 53 Spd90th 16810 non-null float64 54 StartDate 22296 non-null object 55 StartDay_Class 22296 non-null object 56 StartDay_Speed 22296 non-null object 57 StartDay_Volume 22296 non-null object 58 TwoAxleCF 7747 non-null float64 59 X_Class 22296 non-null float64 60 X_Speed 22296 non-null float64 61 X_Volume 22296 non-null float64 62 Y_Class 22296 non-null float64 63 Y_Speed 22296 non-null float64 64 Y_Volume 22296 non-null float64 dtypes: float64(39), object(26) memory usage: 11.2+ MB <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 65 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 ADTVolume_Class 22296 non-null float64 2 ADTVolume_Speed 22296 non-null float64 3 ADTVolume_Volume 22296 non-null float64 4 AMPkHrFactor 20785 non-null float64 5 AMPkHrTime 20785 non-null object 6 AMPkHrVol 20785 non-null float64 7 AMVolume 20785 non-null float64 8 ChannelID_Class 22296 non-null float64 9 ChannelID_Speed 22296 non-null float64 10 ChannelID_Volume 22296 non-null float64 11 CountLocDesc_Class 22296 non-null object 12 CountLocDesc_Speed 22296 non-null object 13 CountLocDesc_Volume 22296 non-null object 14 CountType_Class 22296 non-null object 15 CountType_Speed 22296 non-null object 16 CountType_Volume 22296 non-null object 17 Duration_Class 22296 non-null float64 18 Duration_Speed 22296 non-null float64 19 Duration_Volume 22296 non-null float64 20 EndDate 22296 non-null object 21 EndDay_Class 22296 non-null object 22 EndDay_Speed 22296 non-null object 23 EndDay_Volume 22296 non-null object 24 ExceptType_Class 22296 non-null object 25 ExceptType_Speed 22296 non-null object 26 ExceptType_Volume 22296 non-null object 27 IntervalLen_Class 22296 non-null float64 28 IntervalLen_Speed 22296 non-null float64 29 IntervalLen_Volume 22296 non-null float64 30 LocationDesc_Class 22296 non-null object 31 LocationDesc_Speed 22296 non-null object 32 LocationDesc_Volume 22296 non-null object 33 LocationID_Class 22296 non-null object 34 LocationID_Speed 22296 non-null object 35 LocationID_Volume 22296 non-null object 36 NumCategories 7747 non-null float64 37 NumSlots 16810 non-null float64 38 OBJECTID_Class 22296 non-null float64 39 OBJECTID_Speed 22296 non-null float64 40 OBJECTID_Volume 22296 non-null float64 41 PMPkHrFactor 20785 non-null float64 42 PMPkHrTime 20785 non-null object 43 PMPkHrVol 20785 non-null float64 44 PMVolume 20785 non-null float64 45 PctCars 7747 non-null float64 46 PctOverPosted 16810 non-null float64 47 PctOverPosted10 16810 non-null float64 48 PctTrucks 7747 non-null float64 49 PostedSpeed 16810 non-null float64 50 Spd50th 16810 non-null float64 51 Spd70th 16810 non-null float64 52 Spd85th 16810 non-null float64 53 Spd90th 16810 non-null float64 54 StartDate 22296 non-null object 55 StartDay_Class 22296 non-null object 56 StartDay_Speed 22296 non-null object 57 StartDay_Volume 22296 non-null object 58 TwoAxleCF 7747 non-null float64 59 X_Class 22296 non-null float64 60 X_Speed 22296 non-null float64 61 X_Volume 22296 non-null float64 62 Y_Class 22296 non-null float64 63 Y_Speed 22296 non-null float64 64 Y_Volume 22296 non-null float64 dtypes: float64(39), object(26) memory usage: 11.2+ MB # iterate to find columns that are perfectly identical for i in repeats: if df[i+'_Speed'].equals(df[i+'_Volume']) is True: print(i,'Speed and Volume match') if df[i+'_Class'].equals(df[i+'_Volume']) is True: print(i,'Class and Volume match') if df[i+'_Speed'].equals(df[i+'_Class']) is True: print(i,'Speed and Class match') EndDay Speed and Volume match EndDay Class and Volume match EndDay Speed and Class match EndDay Speed and Volume match EndDay Class and Volume match EndDay Speed and Class match # clear up the low-hanging fruit (columns that perfectly match) df.drop(columns=['EndDay_Speed', 'EndDay_Class'], inplace=True) df.rename(columns={'EndDay_Volume':'EndDay'}, inplace=True) repeats.remove('EndDay') # figure out how many rows have mismatched values in repeated columns rep = {} for i in repeats: print(i,':') speed_volume = df.iloc[np.where((df[i+'_Speed'] != df[i+'_Volume']))] speed_class = df.iloc[np.where((df[i+'_Speed'] != df[i+'_Class']))] class_volume = df.iloc[np.where((df[i+'_Class'] != df[i+'_Volume']))] mask = pd.concat([speed_volume, speed_class, class_volume]) mask = mask.drop_duplicates() rep[i] = {i+'_Speed': mask[i+'_Speed'], i+'_Volume': mask[i+'_Volume'], i+'_Class': mask[i+'_Class']} print(' ',len(mask),'\\n') ADTVolume : 14130 CountLocDesc : 197 OBJECTID : ADTVolume : 14130 CountLocDesc : 197 OBJECTID : 15968 Y : 55 X : 55 IntervalLen : 15530 StartDay : 96 ChannelID : 15968 Y : 55 X : 55 IntervalLen : 15530 StartDay : 96 ChannelID : 15974 ExceptType : 57 LocationID : 55 LocationDesc : 47 CountType : 15974 ExceptType : 57 LocationID : 55 LocationDesc : 47 CountType : 15974 Duration : 15591 15974 Duration : 15591 # select speed as the df of truth for repeated columns, drop others for i in repeats: df[i] = df[i+'_Speed'] df.drop(i+'_Speed', axis=1, inplace=True) df.drop(i+'_Volume', axis=1, inplace=True) df.drop(i+'_Class', axis=1, inplace=True) df.info() <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 37 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 AMPkHrFactor 20785 non-null float64 2 AMPkHrTime 20785 non-null object 3 AMPkHrVol 20785 non-null float64 4 AMVolume 20785 non-null float64 5 EndDate 22296 non-null object 6 EndDay 22296 non-null object 7 NumCategories 7747 non-null float64 8 NumSlots 16810 non-null float64 9 PMPkHrFactor 20785 non-null float64 10 PMPkHrTime 20785 non-null object 11 PMPkHrVol 20785 non-null float64 12 PMVolume 20785 non-null float64 13 PctCars 7747 non-null float64 14 PctOverPosted 16810 non-null float64 15 PctOverPosted10 16810 non-null float64 16 PctTrucks 7747 non-null float64 17 PostedSpeed 16810 non-null float64 18 Spd50th 16810 non-null float64 19 Spd70th 16810 non-null float64 20 Spd85th 16810 non-null float64 21 Spd90th 16810 non-null float64 22 StartDate 22296 non-null object 23 TwoAxleCF 7747 non-null float64 24 ADTVolume 22296 non-null float64 25 CountLocDesc 22296 non-null object 26 OBJECTID 22296 non-null float64 27 Y 22296 non-null float64 28 X 22296 non-null float64 29 IntervalLen 22296 non-null float64 30 StartDay 22296 non-null object 31 ChannelID 22296 non-null float64 32 ExceptType 22296 non-null object 33 LocationID 22296 non-null object 34 LocationDesc 22296 non-null object 35 CountType 22296 non-null object 36 Duration 22296 non-null float64 dtypes: float64(25), object(12) memory usage: 6.5+ MB <class 'pandas.core.frame.DataFrame'> Index: 22296 entries, 0 to 22734 Data columns (total 37 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ID 22295 non-null object 1 AMPkHrFactor 20785 non-null float64 2 AMPkHrTime 20785 non-null object 3 AMPkHrVol 20785 non-null float64 4 AMVolume 20785 non-null float64 5 EndDate 22296 non-null object 6 EndDay 22296 non-null object 7 NumCategories 7747 non-null float64 8 NumSlots 16810 non-null float64 9 PMPkHrFactor 20785 non-null float64 10 PMPkHrTime 20785 non-null object 11 PMPkHrVol 20785 non-null float64 12 PMVolume 20785 non-null float64 13 PctCars 7747 non-null float64 14 PctOverPosted 16810 non-null float64 15 PctOverPosted10 16810 non-null float64 16 PctTrucks 7747 non-null float64 17 PostedSpeed 16810 non-null float64 18 Spd50th 16810 non-null float64 19 Spd70th 16810 non-null float64 20 Spd85th 16810 non-null float64 21 Spd90th 16810 non-null float64 22 StartDate 22296 non-null object 23 TwoAxleCF 7747 non-null float64 24 ADTVolume 22296 non-null float64 25 CountLocDesc 22296 non-null object 26 OBJECTID 22296 non-null float64 27 Y 22296 non-null float64 28 X 22296 non-null float64 29 IntervalLen 22296 non-null float64 30 StartDay 22296 non-null object 31 ChannelID 22296 non-null float64 32 ExceptType 22296 non-null object 33 LocationID 22296 non-null object 34 LocationDesc 22296 non-null object 35 CountType 22296 non-null object 36 Duration 22296 non-null float64 dtypes: float64(25), object(12) memory usage: 6.5+ MB # remove any other columns known to not be needed df = df.drop(columns=['NumCategories']) # identify where one pct exists but the other doens't pct_missingcars = df[(df['PctTrucks'].notnull()) & (df['PctCars'].isnull())] pct_missingtrucks = df[(df['PctCars'].notnull()) & (df['PctTrucks'].isnull())] pct_missingall = df[(df['PctCars'].isnull()) & (df['PctTrucks'].isnull())] # fill in missing pct df.loc[pct_missingcars.index, 'PctCars'] = 100-df['PctTrucks'] df.loc[pct_missingtrucks.index, 'PctTrucks'] = 100-df['PctCars'] # check how many are missing both pcts pct_missingall.shape (14549, 36) # review values in TwoAxleCF df['TwoAxleCF'].describe() count 7747.000000 mean 0.985774 std 0.033495 min 0.632000 25% 0.990000 50% 0.995000 75% 0.998000 max 1.125000 Name: TwoAxleCF, dtype: float64 # fill missing TwoAxleCF values with median df['TwoAxleCF'] = df['TwoAxleCF'].fillna(df['TwoAxleCF'].median()) # count of missing values from each row df.isnull().sum() ID 1 AMPkHrFactor 1511 AMPkHrTime 1511 AMPkHrVol 1511 AMVolume 1511 EndDate 0 EndDay 0 NumSlots 5486 PMPkHrFactor 1511 PMPkHrTime 1511 PMPkHrVol 1511 PMVolume 1511 PctCars 14549 PctOverPosted 5486 PctOverPosted10 5486 PctTrucks 14549 PostedSpeed 5486 Spd50th 5486 Spd70th 5486 Spd85th 5486 Spd90th 5486 StartDate 0 TwoAxleCF 0 ADTVolume 0 CountLocDesc 0 OBJECTID 0 Y 0 X 0 IntervalLen 0 StartDay 0 ChannelID 0 ExceptType 0 LocationID 0 LocationDesc 0 CountType 0 Duration 0 dtype: int64 # remove all rows with na values, leaving a clean dataset df = df.dropna() df.shape (7072, 36) # final dataframe df.head().transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2 3 4 10 31 ID 10010421_N_ 10010422_E_ 10010422_W_ 10010432_S_ 10011224_N_ AMPkHrFactor 0.532 0.798 0.78 0.726 0.885 AMPkHrTime 2010/01/04 06:45:00+00 2010/01/04 07:30:00+00 2010/01/04 06:15:00+00 2010/01/04 06:00:00+00 2010/01/12 08:00:00+00 AMPkHrVol 149.0 217.0 343.0 209.0 170.0 AMVolume 632.0 1227.0 1992.0 895.0 724.0 EndDate 2010/01/06 00:00:00+00 2010/01/05 00:00:00+00 2010/01/05 00:00:00+00 2010/01/06 00:00:00+00 2010/01/15 00:00:00+00 EndDay WED TUE TUE WED FRI NumSlots 13.0 13.0 13.0 13.0 13.0 PMPkHrFactor 0.435 0.771 0.917 0.867 0.964 PMPkHrTime 2010/01/04 18:30:00+00 2010/01/04 15:15:00+00 2010/01/04 14:15:00+00 2010/01/04 12:00:00+00 2010/01/12 17:00:00+00 PMPkHrVol 134.0 333.0 253.0 104.0 189.0 PMVolume 859.0 1806.0 1384.0 528.0 1564.0 PctCars 65.3 63.5 65.6 63.3 98.0 PctOverPosted 20.6 17.9 25.7 22.1 21.1 PctOverPosted10 2.2 0.6 1.5 2.5 0.1 PctTrucks 34.7 36.5 34.4 36.7 2.0 PostedSpeed 40.0 45.0 45.0 40.0 25.0 Spd50th 33.0 39.0 39.0 35.0 22.0 Spd70th 37.0 43.0 44.0 38.0 24.0 Spd85th 42.0 46.0 48.0 42.0 26.0 Spd90th 44.0 47.0 49.0 44.0 27.0 StartDate 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/04 00:00:00+00 2010/01/12 00:00:00+00 TwoAxleCF 0.857 0.749 0.773 0.781 0.998 ADTVolume 1524.0 3048.0 3405.0 1465.0 2301.0 CountLocDesc NW FRONT AVE S/NW 61ST AVE N LOMBARD ST E/N SIMMONS RD N LOMBARD ST E/N SIMMONS RD NW FRONT AVE S/NW 61ST AVE SE 20TH AVE N/SE STEPHENS ST OBJECTID 5.0 1.0 2.0 6.0 22.0 Y 5711466.2873 5722466.9793 5722466.9793 5711466.2873 5702148.698 X -13663904.8261 -13666261.3309 -13666261.3309 -13663904.8261 -13652793.7282 IntervalLen 60.0 60.0 60.0 60.0 60.0 StartDay MON MON MON MON TUE ChannelID 505642.0 505638.0 505639.0 505645.0 505676.0 ExceptType Normal Weekday Normal Weekday Normal Weekday Normal Weekday Normal Weekday LocationID LEG37236 LEG11527 LEG11527 LEG37236 LEG43556 LocationDesc NW FRONT AVE E of 61ST AVE N LOMBARD ST E of SIMMONS RD N LOMBARD ST E of SIMMONS RD NW FRONT AVE E of 61ST AVE SE 20TH AVE N of STEPHENS ST CountType SPEED SPEED SPEED SPEED SPEED Duration 50.0 36.0 36.0 49.0 85.0","title":"Data Preparation"},{"location":"Traffic_Analysis/Traffic_Analysis/#verify-impact-of-dropped-rows","text":"# suppress warnings for datetaime conversion import warnings warnings.filterwarnings(\"ignore\", message=\"Converting to PeriodArray/Index representation will drop timezone information.\") def extract_month_year(origin, df_name, column_name): new_df = pd.DataFrame() new_df['Month_Year'] = pd.to_datetime(origin[column_name]).dt.to_period('M') # count occurrences of each unique month/year combo count_df = new_df['Month_Year'].value_counts().reset_index() count_df.columns = ['Month_Year', 'Count'] # sum the counts sum_count = count_df['Count'].sum() # calculate percentage of sum for each month/year combo count_df['Percentage'] = (count_df['Count'] / sum_count) * 100 # rename column according to dataframe name count_df.rename(columns={'Percentage': f'{df_name}_Percentage'}, inplace=True) return count_df.set_index('Month_Year')[[f'{df_name}_Percentage']] # apply function to each dataframe df_percentages = extract_month_year(df, 'df', 'StartDate') Speed_DF_percentages = extract_month_year(SpeedDF, 'SpeedDF', 'StartDate') Class_DF_percentages = extract_month_year(ClassDF, 'ClassDF', 'StartDate') Volume_DF_percentages = extract_month_year(VolumeDF, 'VolumeDF', 'StartDate') # merge the percentage dataframes freq_df = pd.merge(df_percentages, Speed_DF_percentages, on='Month_Year', how='outer') freq_df = pd.merge(freq_df, Class_DF_percentages, on='Month_Year', how='outer') freq_df = pd.merge(freq_df, Volume_DF_percentages, on='Month_Year', how='outer') # sort the index freq_df.sort_index(inplace=True) # plot the results ax = freq_df.plot(kind='line', figsize=(10, 6)) ax.lines[0].set_alpha(1) for line in ax.lines[1:]: line.set_alpha(0.2) plt.title('Count Comparison Over Time') plt.xlabel('Month/Year') plt.ylabel('Percent of Total Counts for each Data Frame') plt.grid(True) plt.legend() plt.show()","title":"Verify impact of dropped rows"},{"location":"Traffic_Analysis/Traffic_Analysis/#data-exploration_1","text":"# visualizing outliers variables = ['PostedSpeed', 'PctOverPosted'] for variable in variables: plt.figure(figsize=(4, 2)) sns.boxplot(x=df[variable]) plt.title(f\"{variable} Boxplot with Outliers\") plt.show() # dropping outliers using IQR for variable in variables: Q1 = df[variable].quantile(0.25) Q3 = df[variable].quantile(0.75) IQR = Q3 - Q1 lower_threshold = Q1 - 1.5 * IQR upper_threshold = Q3 + 1.5 * IQR # Identify and drop outliers_index = (df[variable] < lower_threshold) | (df[variable] > upper_threshold) df = df[~outliers_index] # new df after dropping outliers for variable in variables: plt.figure(figsize=(4, 2)) sns.boxplot(x=df[variable]) plt.title(f\"{variable} Boxplot, Outliers Removed\") plt.show()","title":"Data Exploration"},{"location":"Traffic_Analysis/Traffic_Analysis/#feature-engineering","text":"Aggregated Statistics : statistics such as mean, median, maximum, or minimum speed for each vehicle type, location, and time interval. These statistics can provide insights into the typical speeding behavior in different scenarios. # Variables for calculation StatVariables = ['PctOverPosted', 'PctOverPosted10', 'ADTVolume'] # Means, Median, Min, and Max grouped by Start Day StartDayStats_df = pd.DataFrame() for variable in StatVariables: stats_by_day = df.groupby(['StartDay'])[variable].agg(['mean', 'median', 'min', 'max']).reset_index() stats_by_day.rename(columns={ 'mean': f'{variable}_mean', 'median': f'{variable}_median', 'min': f'{variable}_min', 'max': f'{variable}_max' }, inplace=True) stats_by_day = stats_by_day.round(2) StartDayStats_df = pd.concat([StartDayStats_df, stats_by_day], axis=1) output_string = StartDayStats_df.to_string(index=False,) print(output_string) StartDay PctOverPosted_mean PctOverPosted_median PctOverPosted_min PctOverPosted_max StartDay PctOverPosted10_mean PctOverPosted10_median PctOverPosted10_min PctOverPosted10_max StartDay ADTVolume_mean ADTVolume_median ADTVolume_min ADTVolume_max FRI 25.35 23.80 2.7 50.8 FRI 0.30 0.25 0.0 0.8 FRI 2115.17 1777.5 912.0 4795.0 MON 36.75 31.50 0.0 99.6 MON 3.41 0.80 0.0 86.6 MON 3484.60 2436.0 26.0 25705.0 SAT 37.87 37.75 0.2 90.2 SAT 2.92 0.95 0.0 23.1 SAT 1743.56 1199.5 46.0 6540.0 SUN 48.27 49.00 2.3 92.6 SUN 3.88 1.30 0.0 28.7 SUN 1965.43 1320.0 247.0 5777.0 THU 33.52 29.15 0.0 96.5 THU 2.47 0.50 0.0 51.0 THU 1929.78 748.5 19.0 22141.0 TUE 37.64 32.80 0.0 98.7 TUE 3.54 0.80 0.0 89.4 TUE 3007.47 1608.0 39.0 27058.0 WED 36.30 30.40 0.0 96.9 WED 3.58 0.60 0.0 67.6 WED 3006.57 1654.0 14.0 23236.0 StartDay PctOverPosted_mean PctOverPosted_median PctOverPosted_min PctOverPosted_max StartDay PctOverPosted10_mean PctOverPosted10_median PctOverPosted10_min PctOverPosted10_max StartDay ADTVolume_mean ADTVolume_median ADTVolume_min ADTVolume_max FRI 25.35 23.80 2.7 50.8 FRI 0.30 0.25 0.0 0.8 FRI 2115.17 1777.5 912.0 4795.0 MON 36.75 31.50 0.0 99.6 MON 3.41 0.80 0.0 86.6 MON 3484.60 2436.0 26.0 25705.0 SAT 37.87 37.75 0.2 90.2 SAT 2.92 0.95 0.0 23.1 SAT 1743.56 1199.5 46.0 6540.0 SUN 48.27 49.00 2.3 92.6 SUN 3.88 1.30 0.0 28.7 SUN 1965.43 1320.0 247.0 5777.0 THU 33.52 29.15 0.0 96.5 THU 2.47 0.50 0.0 51.0 THU 1929.78 748.5 19.0 22141.0 TUE 37.64 32.80 0.0 98.7 TUE 3.54 0.80 0.0 89.4 TUE 3007.47 1608.0 39.0 27058.0 WED 36.30 30.40 0.0 96.9 WED 3.58 0.60 0.0 67.6 WED 3006.57 1654.0 14.0 23236.0 Speed Deviation from Speed Limit: difference between the observed speed and the posted speed limit for each location and time interval. This can help identify areas where speeding is more prevalent relative to the speed limit. # Calculate differences between posted and percentile speeds def calculate_differences(group): group['Diff_Spd50th'] = group['Spd50th'] - group['PostedSpeed'] group['Diff_Spd70th'] = group['Spd70th'] - group['PostedSpeed'] group['Diff_Spd85th'] = group['Spd85th'] - group['PostedSpeed'] group['Diff_Spd90th'] = group['Spd90th'] - group['PostedSpeed'] return group # Apply the function to each group based on the 'ID' column # Using _extras for this so as not to add unwanted extra columns to the data df_extras = df.groupby('ID').apply(calculate_differences).reset_index(drop=True) # Optional display #df_extras.head().transpose() <positron-console-cell-40>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning. # calculate the avg for each 'CountLocDesc' average_diff_by_countlocdesc = df_extras.groupby('CountLocDesc').agg({ 'Diff_Spd50th': 'mean', 'Diff_Spd70th': 'mean', 'Diff_Spd85th': 'mean', 'Diff_Spd90th': 'mean' }).reset_index() # Optional display average_diff_by_countlocdesc = average_diff_by_countlocdesc.sort_values(by='Diff_Spd90th', ascending=False) #average_diff_by_countlocdesc.head() # Display the top 15 rows for locations with the greatest mean speed over posted top_15_diff_Spd50th = df_extras.nlargest(15, 'Diff_Spd50th') print(tabulate(top_15_diff_Spd50th[['CountLocDesc', 'PostedSpeed', 'Spd50th', 'Diff_Spd50th','PctOverPosted10']], headers=['CountLocDesc', 'PostedSpeed', 'Spd50th', 'Diff_Spd50th','PctOverPosted10'], tablefmt='outline', showindex=False)) +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ | CountLocDesc | PostedSpeed | Spd50th | Diff_Spd50th | PctOverPosted10 | +========================================================+===============+===========+================+===================+ | SE 112TH AVE S of SE OGDEN S | 20 | 37 | 17 | 89.4 | | COLUMBIA WAY - COLUMBIA BLVD RAMP W of N COLUMBIA BLVD | 20 | 36 | 16 | 86.6 | | I-84 EB EXTO NE 68TH AVE W of NE 68TH AVE | 25 | 39 | 14 | 78.3 | | NE 148TH AVE S of NE SACRAMENTO ST | 30 | 43 | 13 | 67.6 | | SW 45TH DR S of SW DOLPH CT | 25 | 37 | 12 | 63.9 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 61.3 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 57.6 | | SE MARKET ST W of SE 104TH AVE | 20 | 31 | 11 | 59.2 | | SW TERWILLIGER BLV N/SW CONDOR LN | 25 | 35 | 10 | 45.8 | | SW 45TH DR N of SW ORCHID ST | 30 | 40 | 10 | 50.6 | | SE MT SCOTT BLVD W of SE 108TH AVE | 35 | 45 | 10 | 51.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 48.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 51.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 45.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 46.6 | +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ | CountLocDesc | PostedSpeed | Spd50th | Diff_Spd50th | PctOverPosted10 | +========================================================+===============+===========+================+===================+ | SE 112TH AVE S of SE OGDEN S | 20 | 37 | 17 | 89.4 | | COLUMBIA WAY - COLUMBIA BLVD RAMP W of N COLUMBIA BLVD | 20 | 36 | 16 | 86.6 | | I-84 EB EXTO NE 68TH AVE W of NE 68TH AVE | 25 | 39 | 14 | 78.3 | | NE 148TH AVE S of NE SACRAMENTO ST | 30 | 43 | 13 | 67.6 | | SW 45TH DR S of SW DOLPH CT | 25 | 37 | 12 | 63.9 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 61.3 | | SE FLAVEL DR W of SE 60TH AVE | 20 | 31 | 11 | 57.6 | | SE MARKET ST W of SE 104TH AVE | 20 | 31 | 11 | 59.2 | | SW TERWILLIGER BLV N/SW CONDOR LN | 25 | 35 | 10 | 45.8 | | SW 45TH DR N of SW ORCHID ST | 30 | 40 | 10 | 50.6 | | SE MT SCOTT BLVD W of SE 108TH AVE | 35 | 45 | 10 | 51.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 48.9 | | NE SHAVER ST W of NE 125TH PL | 20 | 30 | 10 | 51.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 45.9 | | SE FLAVEL DR E of SE TENINO ST | 20 | 30 | 10 | 46.6 | +--------------------------------------------------------+---------------+-----------+----------------+-------------------+ # Optional - Display the top 15 rows for locations with the greatest speed for 90th percentile # top_15_diff_Spd50th = df_extras.nlargest(15, 'Diff_Spd90th') # print(tabulate(top_15_diff_Spd50th[['CountLocDesc', 'PostedSpeed', 'Spd90th', 'Diff_Spd90th','PctOverPosted10']], # headers=['CountLocDesc', 'PostedSpeed', 'Spd90th', 'Diff_Spd90th','PctOverPosted10'], # tablefmt='outline', showindex=False)) Historical Features: to capture trends and seasonality in speeding behaviour. # Yearly average over time # Convert StartDate to datetime df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year from StartDate df['Year'] = df['StartDate'].dt.year # Calculate average PctOverPosted10 and PctOverPosted by grouping only by 'Year' yearlyavg_pctover10 = df.groupby('Year')['PctOverPosted10'].mean() yearlyavg_pctover = df.groupby('Year')['PctOverPosted'].mean() # Plotting plt.figure(figsize=(12, 8)) plt.plot(yearlyavg_pctover10.index, yearlyavg_pctover10.values, marker='o', linestyle='-', label='PctOverPosted10') plt.plot(yearlyavg_pctover.index, yearlyavg_pctover.values, marker='o', linestyle='-', label='PctOverPosted') plt.title('Yearly Average PctOverPosted10 and PctOverPosted Over Time') plt.xlabel('Year') plt.ylabel('Average Percentage') plt.grid(True) plt.legend() plt.show() # Monthly aggregation PctOverPosted # Convert StartDate to datetime df = df.copy() df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year and month from StartDate df['Year'] = df['StartDate'].dt.year df['Month'] = df['StartDate'].dt.month # Calculate average PctOverPosted avg_pct_over_time = df.groupby(['Year', 'Month'])['PctOverPosted'].mean().unstack() # Plotting plt.figure(figsize=(12, 8)) for year in avg_pct_over_time.index: plt.plot(range(1, 13), avg_pct_over_time.loc[year], marker='o', label=f'{year}') plt.title('Average PctOverPosted Over Time') plt.xlabel('Month') plt.ylabel('Average PctOverPosted') plt.grid(True) plt.xticks(range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) plt.legend(title='Year', bbox_to_anchor=(1, 1), loc='upper left') plt.show() # Monthly aggregation PctOverPosted10 # Convert StartDate to datetime df = df.copy() df['StartDate'] = pd.to_datetime(df['StartDate']) # Extract the year and month from StartDate df['Year'] = df['StartDate'].dt.year # Calculate average PctOverPosted10 yearly_avg_pct_over_time10 = df.groupby(['Year','Month'])['PctOverPosted10'].mean().unstack() # Plotting plt.figure(figsize=(12, 8)) for year in yearly_avg_pct_over_time10.index: plt.plot(range(1, 13), yearly_avg_pct_over_time10.loc[year], marker='o', label=f'{year}') plt.title('Yearly Average PctOverPosted10 Over Time') plt.xlabel('Month') plt.ylabel('Average PctOverPosted10') plt.grid(True) plt.xticks(range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) plt.legend(title='Year', bbox_to_anchor=(1., 1), loc='upper left') plt.show()","title":"Feature Engineering"},{"location":"Traffic_Analysis/Traffic_Analysis/#feature-selection_1","text":"# create new column \"Over10Speeders\" where any PctOver10 > 0 is marked 1 df['10OverSpeeding'] = df['PctOverPosted10'].apply(lambda x: 1 if x > 0 else 0) col = df.pop('10OverSpeeding') df.insert(0, col.name, col) # drop redundant column PctOverPosted10 df.drop(columns=['PctOverPosted10'], inplace=True) # review correlations to target df.corr(numeric_only = True)['10OverSpeeding'].sort_values().round(2) PctCars -0.19 TwoAxleCF -0.16 Month -0.05 ChannelID -0.03 OBJECTID -0.03 Year -0.02 IntervalLen 0.01 NumSlots 0.01 Y 0.04 Duration 0.08 X 0.09 PctTrucks 0.19 PostedSpeed 0.27 AMVolume 0.29 AMPkHrVol 0.30 AMPkHrFactor 0.30 PMPkHrFactor 0.31 PMVolume 0.31 ADTVolume 0.31 PMPkHrVol 0.32 Spd90th 0.41 Spd85th 0.47 Spd70th 0.50 Spd50th 0.50 PctOverPosted 0.52 10OverSpeeding 1.00 Name: 10OverSpeeding, dtype: float64 # Correlation for ALL NUMERICAL numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns correlation_matrix = df[numeric_columns].corr() # Plotting a heatmap using seaborn plt.figure(figsize=(16, 8)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5) plt.title('Correlation Matrix') plt.show() # create subset df_subset = df[['10OverSpeeding', 'AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'StartDay', 'Month']] # Print the updated dataframe information df_subset.info() print('\\n') df_subset['10OverSpeeding'].value_counts() <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 StartDay 7055 non-null object 11 Month 7055 non-null int32 dtypes: float64(9), int32(1), int64(1), object(1) memory usage: 689.0+ KB <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 StartDay 7055 non-null object 11 Month 7055 non-null int32 dtypes: float64(9), int32(1), int64(1), object(1) memory usage: 689.0+ KB 10OverSpeeding 1 5658 0 1397 Name: count, dtype: int64 # set initial predictors list target = '10OverSpeeding' predictors = ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'StartDay', 'Month'] # get dummies df_subset = pd.get_dummies(df_subset, columns=df_subset.select_dtypes(include=['object', 'category']).columns, drop_first=True) df_subset.info() <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 Month 7055 non-null int32 11 StartDay_MON 7055 non-null bool 12 StartDay_SAT 7055 non-null bool 13 StartDay_SUN 7055 non-null bool 14 StartDay_THU 7055 non-null bool 15 StartDay_TUE 7055 non-null bool 16 StartDay_WED 7055 non-null bool dtypes: bool(6), float64(9), int32(1), int64(1) memory usage: 675.2 KB <class 'pandas.core.frame.DataFrame'> Index: 7055 entries, 2 to 22589 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 10OverSpeeding 7055 non-null int64 1 AMPkHrVol 7055 non-null float64 2 AMVolume 7055 non-null float64 3 PMPkHrVol 7055 non-null float64 4 PMVolume 7055 non-null float64 5 PctCars 7055 non-null float64 6 PostedSpeed 7055 non-null float64 7 X 7055 non-null float64 8 ADTVolume 7055 non-null float64 9 Y 7055 non-null float64 10 Month 7055 non-null int32 11 StartDay_MON 7055 non-null bool 12 StartDay_SAT 7055 non-null bool 13 StartDay_SUN 7055 non-null bool 14 StartDay_THU 7055 non-null bool 15 StartDay_TUE 7055 non-null bool 16 StartDay_WED 7055 non-null bool dtypes: bool(6), float64(9), int32(1), int64(1) memory usage: 675.2 KB # automated multivariate feature selection X = df_subset.drop(columns=[target]) y = df_subset[target] estimator = DecisionTreeClassifier() selector = RFE(estimator, n_features_to_select=10, step=1).fit(X,y) rnk = pd.DataFrame() rnk['Feature'] = X.columns rnk['Rank']= selector.ranking_ rnk.sort_values('Rank') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Feature Rank 0 AMPkHrVol 1 1 AMVolume 1 2 PMPkHrVol 1 3 PMVolume 1 4 PctCars 1 5 PostedSpeed 1 6 X 1 7 ADTVolume 1 8 Y 1 9 Month 1 15 StartDay_WED 2 10 StartDay_MON 3 14 StartDay_TUE 4 13 StartDay_THU 5 12 StartDay_SUN 6 11 StartDay_SAT 7 # View the selected features selected_features = rnk.sort_values('Rank').head(10)['Feature'].tolist() # Display print(\"Top 10 Selected Features:\") print(selected_features) Top 10 Selected Features: ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'Month'] Top 10 Selected Features: ['AMPkHrVol', 'AMVolume', 'PMPkHrVol', 'PMVolume', 'PctCars', 'PostedSpeed', 'X', 'ADTVolume', 'Y', 'Month'] # Check correlation for selected features correlation_matrix = df_subset[[target] + selected_features].corr() # Plotting heatmap plt.figure(figsize=(10, 8)) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5) plt.title('Correlation Matrix Between Selected Features') plt.show() # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=17, stratify=y) # determine hyperparameters param_grid = { 'max_depth': [10], 'min_samples_leaf': [20], 'max_features': list(range(2,11)) } gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=17), param_grid, cv=5, n_jobs=-1) gridSearch.fit(X_train, y_train) print('Score: ', gridSearch.best_score_) best_params = gridSearch.best_params_ print('Parameters: ', best_params) Score: 0.8364648311127025 Parameters: {'max_depth': 10, 'max_features': 7, 'min_samples_leaf': 20} Score: 0.8364648311127025 Parameters: {'max_depth': 10, 'max_features': 7, 'min_samples_leaf': 20}","title":"Feature Selection"},{"location":"Traffic_Analysis/Traffic_Analysis/#model-building","text":"","title":"Model Building:"},{"location":"Traffic_Analysis/Traffic_Analysis/#decision-tree_1","text":"# Train & Predict tree_model = DecisionTreeClassifier(random_state=17, max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'], max_features=best_params['max_features']) tree_model.fit(X_train, y_train) tree_predictions = tree_model.predict(X_test) # Print decision tree rules tree_rules = export_text(tree_model, feature_names=list(X_train.columns)) print(\"Decision Tree Rules:\") print(tree_rules) Decision Tree Rules: |--- AMVolume <= 262.50 | |--- X <= -13645958.50 | | |--- AMVolume <= 63.50 | | | |--- PMVolume <= 74.50 | | | | |--- Y <= 5702417.00 | | | | | |--- Y <= 5697200.50 | | | | | | |--- class: 0 | | | | | |--- Y > 5697200.50 | | | | | | |--- class: 0 | | | | |--- Y > 5702417.00 | | | | | |--- ADTVolume <= 90.50 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 0 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 0 | | | | | |--- ADTVolume > 90.50 | | | | | | |--- class: 0 | | | |--- PMVolume > 74.50 | | | | |--- Month <= 3.50 | | | | | |--- PostedSpeed <= 22.50 | | | | | | |--- Y <= 5708024.75 | | | | | | | |--- PMVolume <= 121.50 | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 121.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5708024.75 | | | | | | | |--- class: 1 | | | | | |--- PostedSpeed > 22.50 | | | | | | |--- class: 0 | | | | |--- Month > 3.50 | | | | | |--- Y <= 5707250.00 | | | | | | |--- X <= -13650135.00 | | | | | | | |--- Y <= 5701115.00 | | | | | | | | |--- class: 0 | | | | | | | |--- Y > 5701115.00 | | | | | | | | |--- AMVolume <= 51.50 | | | | | | | | | |--- class: 0 | | | | | | | | |--- AMVolume > 51.50 | | | | | | | | | |--- class: 0 | | | | | | |--- X > -13650135.00 | | | | | | | |--- class: 0 | | | | | |--- Y > 5707250.00 | | | | | | |--- Month <= 9.50 | | | | | | | |--- PctCars <= 95.65 | | | | | | | | |--- class: 0 | | | | | | | |--- PctCars > 95.65 | | | | | | | | |--- class: 0 | | | | | | |--- Month > 9.50 | | | | | | | |--- class: 1 | | |--- AMVolume > 63.50 | | | |--- PctCars <= 97.35 | | | | |--- Y <= 5710675.50 | | | | | |--- Y <= 5700892.50 | | | | | | |--- PctCars <= 95.85 | | | | | | | |--- PMPkHrVol <= 36.50 | | | | | | | | |--- Y <= 5697090.25 | | | | | | | | | |--- X <= -13658192.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13658192.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5697090.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 36.50 | | | | | | | | |--- X <= -13659513.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659513.50 | | | | | | | | | |--- PMVolume <= 257.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 257.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PctCars > 95.85 | | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | | |--- Y <= 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PostedSpeed > 22.50 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5700892.50 | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | |--- PMVolume <= 228.50 | | | | | | | | |--- ADTVolume <= 217.50 | | | | | | | | | |--- Y <= 5706837.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Y > 5706837.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- ADTVolume > 217.50 | | | | | | | | | |--- PMVolume <= 169.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMVolume > 169.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 228.50 | | | | | | | | |--- X <= -13656465.50 | | | | | | | | | |--- X <= -13657555.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13657555.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13656465.50 | | | | | | | | | |--- Y <= 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PostedSpeed > 22.50 | | | | | | | |--- PMVolume <= 446.50 | | | | | | | | |--- ADTVolume <= 270.50 | | | | | | | | | |--- Month <= 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Month > 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- ADTVolume > 270.50 | | | | | | | | | |--- PMVolume <= 234.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 234.50 | | | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 446.50 | | | | | | | | |--- class: 1 | | | | |--- Y > 5710675.50 | | | | | |--- PMVolume <= 206.50 | | | | | | |--- Y <= 5714802.50 | | | | | | | |--- AMVolume <= 80.50 | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 80.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714802.50 | | | | | | | |--- class: 1 | | | | | |--- PMVolume > 206.50 | | | | | | |--- PMVolume <= 316.50 | | | | | | | |--- AMPkHrVol <= 33.50 | | | | | | | | |--- X <= -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMPkHrVol > 33.50 | | | | | | | | |--- class: 1 | | | | | | |--- PMVolume > 316.50 | | | | | | | |--- PMPkHrVol <= 80.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 80.00 | | | | | | | | |--- class: 1 | | | |--- PctCars > 97.35 | | | | |--- X <= -13650381.00 | | | | | |--- Y <= 5701628.25 | | | | | | |--- PMPkHrVol <= 43.50 | | | | | | | |--- class: 0 | | | | | | |--- PMPkHrVol > 43.50 | | | | | | | |--- class: 0 | | | | | |--- Y > 5701628.25 | | | | | | |--- ADTVolume <= 279.00 | | | | | | | |--- class: 0 | | | | | | |--- ADTVolume > 279.00 | | | | | | | |--- X <= -13650837.00 | | | | | | | | |--- Y <= 5706611.00 | | | | | | | | | |--- AMPkHrVol <= 38.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 38.00 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5706611.00 | | | | | | | | | |--- class: 1 | | | | | | | |--- X > -13650837.00 | | | | | | | | |--- class: 0 | | | | |--- X > -13650381.00 | | | | | |--- Y <= 5707354.75 | | | | | | |--- Y <= 5703056.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5703056.25 | | | | | | | |--- PctCars <= 97.85 | | | | | | | | |--- class: 1 | | | | | | | |--- PctCars > 97.85 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5707354.75 | | | | | | |--- class: 1 | |--- X > -13645958.50 | | |--- Month <= 5.50 | | | |--- Y <= 5708819.00 | | | | |--- AMPkHrVol <= 25.50 | | | | | |--- AMPkHrVol <= 13.50 | | | | | | |--- class: 1 | | | | | |--- AMPkHrVol > 13.50 | | | | | | |--- class: 1 | | | | |--- AMPkHrVol > 25.50 | | | | | |--- PMVolume <= 314.50 | | | | | | |--- class: 1 | | | | | |--- PMVolume > 314.50 | | | | | | |--- class: 1 | | | |--- Y > 5708819.00 | | | | |--- class: 1 | | |--- Month > 5.50 | | | |--- PctCars <= 96.65 | | | | |--- PMPkHrVol <= 14.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 14.50 | | | | | |--- X <= -13642374.00 | | | | | | |--- X <= -13645112.00 | | | | | | | |--- class: 1 | | | | | | |--- X > -13645112.00 | | | | | | | |--- class: 1 | | | | | |--- X > -13642374.00 | | | | | | |--- Y <= 5706861.75 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5706861.75 | | | | | | | |--- class: 1 | | | |--- PctCars > 96.65 | | | | |--- class: 1 |--- AMVolume > 262.50 | |--- PctCars <= 95.75 | | |--- AMVolume <= 1172.50 | | | |--- X <= -13646288.50 | | | | |--- X <= -13660894.50 | | | | | |--- AMVolume <= 721.50 | | | | | | |--- class: 1 | | | | | |--- AMVolume > 721.50 | | | | | | |--- Y <= 5714154.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714154.25 | | | | | | | |--- class: 1 | | | | |--- X > -13660894.50 | | | | | |--- Month <= 7.50 | | | | | | |--- Y <= 5705188.50 | | | | | | | |--- AMVolume <= 1033.50 | | | | | | | | |--- Y <= 5702413.50 | | | | | | | | | |--- PMPkHrVol <= 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMPkHrVol > 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5702413.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 1033.50 | | | | | | | | |--- AMPkHrVol <= 221.00 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMPkHrVol > 221.00 | | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5705188.50 | | | | | | | |--- AMVolume <= 908.50 | | | | | | | | |--- PctCars <= 92.55 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 92.55 | | | | | | | | | |--- Y <= 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 908.50 | | | | | | | | |--- class: 1 | | | | | |--- Month > 7.50 | | | | | | |--- ADTVolume <= 899.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 899.50 | | | | | | | |--- Y <= 5708754.00 | | | | | | | | |--- Y <= 5704862.50 | | | | | | | | | |--- AMPkHrVol <= 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5704862.50 | | | | | | | | | |--- PMPkHrVol <= 215.00 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMPkHrVol > 215.00 | | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5708754.00 | | | | | | | | |--- class: 1 | | | |--- X > -13646288.50 | | | | |--- PMPkHrVol <= 92.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 92.50 | | | | | |--- X <= -13636547.00 | | | | | | |--- class: 1 | | | | | |--- X > -13636547.00 | | | | | | |--- class: 1 | | |--- AMVolume > 1172.50 | | | |--- Y <= 5694779.75 | | | | |--- class: 1 | | | |--- Y > 5694779.75 | | | | |--- ADTVolume <= 3358.50 | | | | | |--- PctCars <= 94.25 | | | | | | |--- class: 1 | | | | | |--- PctCars > 94.25 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 3358.50 | | | | | |--- Y <= 5696825.75 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 1 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 1 | | | | | |--- Y > 5696825.75 | | | | | | |--- Month <= 11.50 | | | | | | | |--- Month <= 2.50 | | | | | | | | |--- AMVolume <= 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMVolume > 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- Month > 2.50 | | | | | | | | |--- PctCars <= 94.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 94.75 | | | | | | | | | |--- ADTVolume <= 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- ADTVolume > 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- Month > 11.50 | | | | | | | |--- PMVolume <= 3003.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 3003.00 | | | | | | | | |--- class: 1 | |--- PctCars > 95.75 | | |--- PMVolume <= 2022.50 | | | |--- X <= -13649647.50 | | | | |--- X <= -13659285.00 | | | | | |--- PctCars <= 96.35 | | | | | | |--- class: 1 | | | | | |--- PctCars > 96.35 | | | | | | |--- ADTVolume <= 1979.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 1979.50 | | | | | | | |--- class: 1 | | | | |--- X > -13659285.00 | | | | | |--- Y <= 5708073.25 | | | | | | |--- X <= -13658741.00 | | | | | | | |--- class: 0 | | | | | | |--- X > -13658741.00 | | | | | | | |--- Y <= 5699886.00 | | | | | | | | |--- Y <= 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5699886.00 | | | | | | | | |--- X <= -13655722.00 | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13655722.00 | | | | | | | | | |--- PctCars <= 98.35 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PctCars > 98.35 | | | | | | | | | | |--- class: 0 | | | | | |--- Y > 5708073.25 | | | | | | |--- class: 1 | | | |--- X > -13649647.50 | | | | |--- Month <= 8.50 | | | | | |--- PMPkHrVol <= 190.50 | | | | | | |--- class: 1 | | | | | |--- PMPkHrVol > 190.50 | | | | | | |--- class: 1 | | | | |--- Month > 8.50 | | | | | |--- class: 1 | | |--- PMVolume > 2022.50 | | | |--- X <= -13651133.00 | | | | |--- Y <= 5698353.75 | | | | | |--- class: 1 | | | | |--- Y > 5698353.75 | | | | | |--- ADTVolume <= 3321.00 | | | | | | |--- class: 1 | | | | | |--- ADTVolume > 3321.00 | | | | | | |--- PMPkHrVol <= 382.50 | | | | | | | |--- class: 1 | | | | | | |--- PMPkHrVol > 382.50 | | | | | | | |--- ADTVolume <= 6976.00 | | | | | | | | |--- X <= -13659386.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659386.50 | | | | | | | | | |--- X <= -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- ADTVolume > 6976.00 | | | | | | | | |--- class: 1 | | | |--- X > -13651133.00 | | | | |--- ADTVolume <= 7612.00 | | | | | |--- Month <= 9.50 | | | | | | |--- class: 1 | | | | | |--- Month > 9.50 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 7612.00 | | | | | |--- class: 1 Decision Tree Rules: |--- AMVolume <= 262.50 | |--- X <= -13645958.50 | | |--- AMVolume <= 63.50 | | | |--- PMVolume <= 74.50 | | | | |--- Y <= 5702417.00 | | | | | |--- Y <= 5697200.50 | | | | | | |--- class: 0 | | | | | |--- Y > 5697200.50 | | | | | | |--- class: 0 | | | | |--- Y > 5702417.00 | | | | | |--- ADTVolume <= 90.50 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 0 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 0 | | | | | |--- ADTVolume > 90.50 | | | | | | |--- class: 0 | | | |--- PMVolume > 74.50 | | | | |--- Month <= 3.50 | | | | | |--- PostedSpeed <= 22.50 | | | | | | |--- Y <= 5708024.75 | | | | | | | |--- PMVolume <= 121.50 | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 121.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5708024.75 | | | | | | | |--- class: 1 | | | | | |--- PostedSpeed > 22.50 | | | | | | |--- class: 0 | | | | |--- Month > 3.50 | | | | | |--- Y <= 5707250.00 | | | | | | |--- X <= -13650135.00 | | | | | | | |--- Y <= 5701115.00 | | | | | | | | |--- class: 0 | | | | | | | |--- Y > 5701115.00 | | | | | | | | |--- AMVolume <= 51.50 | | | | | | | | | |--- class: 0 | | | | | | | | |--- AMVolume > 51.50 | | | | | | | | | |--- class: 0 | | | | | | |--- X > -13650135.00 | | | | | | | |--- class: 0 | | | | | |--- Y > 5707250.00 | | | | | | |--- Month <= 9.50 | | | | | | | |--- PctCars <= 95.65 | | | | | | | | |--- class: 0 | | | | | | | |--- PctCars > 95.65 | | | | | | | | |--- class: 0 | | | | | | |--- Month > 9.50 | | | | | | | |--- class: 1 | | |--- AMVolume > 63.50 | | | |--- PctCars <= 97.35 | | | | |--- Y <= 5710675.50 | | | | | |--- Y <= 5700892.50 | | | | | | |--- PctCars <= 95.85 | | | | | | | |--- PMPkHrVol <= 36.50 | | | | | | | | |--- Y <= 5697090.25 | | | | | | | | | |--- X <= -13658192.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13658192.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5697090.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 36.50 | | | | | | | | |--- X <= -13659513.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659513.50 | | | | | | | | | |--- PMVolume <= 257.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 257.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PctCars > 95.85 | | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | | |--- Y <= 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696863.25 | | | | | | | | | |--- class: 1 | | | | | | | |--- PostedSpeed > 22.50 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5700892.50 | | | | | | |--- PostedSpeed <= 22.50 | | | | | | | |--- PMVolume <= 228.50 | | | | | | | | |--- ADTVolume <= 217.50 | | | | | | | | | |--- Y <= 5706837.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Y > 5706837.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- ADTVolume > 217.50 | | | | | | | | | |--- PMVolume <= 169.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMVolume > 169.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 228.50 | | | | | | | | |--- X <= -13656465.50 | | | | | | | | | |--- X <= -13657555.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13657555.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13656465.50 | | | | | | | | | |--- Y <= 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5708405.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- PostedSpeed > 22.50 | | | | | | | |--- PMVolume <= 446.50 | | | | | | | | |--- ADTVolume <= 270.50 | | | | | | | | | |--- Month <= 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- Month > 4.50 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- ADTVolume > 270.50 | | | | | | | | | |--- PMVolume <= 234.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMVolume > 234.50 | | | | | | | | | | |--- class: 0 | | | | | | | |--- PMVolume > 446.50 | | | | | | | | |--- class: 1 | | | | |--- Y > 5710675.50 | | | | | |--- PMVolume <= 206.50 | | | | | | |--- Y <= 5714802.50 | | | | | | | |--- AMVolume <= 80.50 | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 80.50 | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714802.50 | | | | | | | |--- class: 1 | | | | | |--- PMVolume > 206.50 | | | | | | |--- PMVolume <= 316.50 | | | | | | | |--- AMPkHrVol <= 33.50 | | | | | | | | |--- X <= -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13660621.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMPkHrVol > 33.50 | | | | | | | | |--- class: 1 | | | | | | |--- PMVolume > 316.50 | | | | | | | |--- PMPkHrVol <= 80.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMPkHrVol > 80.00 | | | | | | | | |--- class: 1 | | | |--- PctCars > 97.35 | | | | |--- X <= -13650381.00 | | | | | |--- Y <= 5701628.25 | | | | | | |--- PMPkHrVol <= 43.50 | | | | | | | |--- class: 0 | | | | | | |--- PMPkHrVol > 43.50 | | | | | | | |--- class: 0 | | | | | |--- Y > 5701628.25 | | | | | | |--- ADTVolume <= 279.00 | | | | | | | |--- class: 0 | | | | | | |--- ADTVolume > 279.00 | | | | | | | |--- X <= -13650837.00 | | | | | | | | |--- Y <= 5706611.00 | | | | | | | | | |--- AMPkHrVol <= 38.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 38.00 | | | | | | | | | | |--- class: 0 | | | | | | | | |--- Y > 5706611.00 | | | | | | | | | |--- class: 1 | | | | | | | |--- X > -13650837.00 | | | | | | | | |--- class: 0 | | | | |--- X > -13650381.00 | | | | | |--- Y <= 5707354.75 | | | | | | |--- Y <= 5703056.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5703056.25 | | | | | | | |--- PctCars <= 97.85 | | | | | | | | |--- class: 1 | | | | | | | |--- PctCars > 97.85 | | | | | | | | |--- class: 0 | | | | | |--- Y > 5707354.75 | | | | | | |--- class: 1 | |--- X > -13645958.50 | | |--- Month <= 5.50 | | | |--- Y <= 5708819.00 | | | | |--- AMPkHrVol <= 25.50 | | | | | |--- AMPkHrVol <= 13.50 | | | | | | |--- class: 1 | | | | | |--- AMPkHrVol > 13.50 | | | | | | |--- class: 1 | | | | |--- AMPkHrVol > 25.50 | | | | | |--- PMVolume <= 314.50 | | | | | | |--- class: 1 | | | | | |--- PMVolume > 314.50 | | | | | | |--- class: 1 | | | |--- Y > 5708819.00 | | | | |--- class: 1 | | |--- Month > 5.50 | | | |--- PctCars <= 96.65 | | | | |--- PMPkHrVol <= 14.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 14.50 | | | | | |--- X <= -13642374.00 | | | | | | |--- X <= -13645112.00 | | | | | | | |--- class: 1 | | | | | | |--- X > -13645112.00 | | | | | | | |--- class: 1 | | | | | |--- X > -13642374.00 | | | | | | |--- Y <= 5706861.75 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5706861.75 | | | | | | | |--- class: 1 | | | |--- PctCars > 96.65 | | | | |--- class: 1 |--- AMVolume > 262.50 | |--- PctCars <= 95.75 | | |--- AMVolume <= 1172.50 | | | |--- X <= -13646288.50 | | | | |--- X <= -13660894.50 | | | | | |--- AMVolume <= 721.50 | | | | | | |--- class: 1 | | | | | |--- AMVolume > 721.50 | | | | | | |--- Y <= 5714154.25 | | | | | | | |--- class: 1 | | | | | | |--- Y > 5714154.25 | | | | | | | |--- class: 1 | | | | |--- X > -13660894.50 | | | | | |--- Month <= 7.50 | | | | | | |--- Y <= 5705188.50 | | | | | | | |--- AMVolume <= 1033.50 | | | | | | | | |--- Y <= 5702413.50 | | | | | | | | | |--- PMPkHrVol <= 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PMPkHrVol > 294.00 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5702413.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 1033.50 | | | | | | | | |--- AMPkHrVol <= 221.00 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMPkHrVol > 221.00 | | | | | | | | | |--- class: 1 | | | | | | |--- Y > 5705188.50 | | | | | | | |--- AMVolume <= 908.50 | | | | | | | | |--- PctCars <= 92.55 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 92.55 | | | | | | | | | |--- Y <= 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- Y > 5711040.75 | | | | | | | | | | |--- class: 1 | | | | | | | |--- AMVolume > 908.50 | | | | | | | | |--- class: 1 | | | | | |--- Month > 7.50 | | | | | | |--- ADTVolume <= 899.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 899.50 | | | | | | | |--- Y <= 5708754.00 | | | | | | | | |--- Y <= 5704862.50 | | | | | | | | | |--- AMPkHrVol <= 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- AMPkHrVol > 225.50 | | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5704862.50 | | | | | | | | | |--- PMPkHrVol <= 215.00 | | | | | | | | | | |--- class: 0 | | | | | | | | | |--- PMPkHrVol > 215.00 | | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5708754.00 | | | | | | | | |--- class: 1 | | | |--- X > -13646288.50 | | | | |--- PMPkHrVol <= 92.50 | | | | | |--- class: 1 | | | | |--- PMPkHrVol > 92.50 | | | | | |--- X <= -13636547.00 | | | | | | |--- class: 1 | | | | | |--- X > -13636547.00 | | | | | | |--- class: 1 | | |--- AMVolume > 1172.50 | | | |--- Y <= 5694779.75 | | | | |--- class: 1 | | | |--- Y > 5694779.75 | | | | |--- ADTVolume <= 3358.50 | | | | | |--- PctCars <= 94.25 | | | | | | |--- class: 1 | | | | | |--- PctCars > 94.25 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 3358.50 | | | | | |--- Y <= 5696825.75 | | | | | | |--- Month <= 5.50 | | | | | | | |--- class: 1 | | | | | | |--- Month > 5.50 | | | | | | | |--- class: 1 | | | | | |--- Y > 5696825.75 | | | | | | |--- Month <= 11.50 | | | | | | | |--- Month <= 2.50 | | | | | | | | |--- AMVolume <= 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- AMVolume > 1614.50 | | | | | | | | | |--- class: 1 | | | | | | | |--- Month > 2.50 | | | | | | | | |--- PctCars <= 94.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- PctCars > 94.75 | | | | | | | | | |--- ADTVolume <= 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- ADTVolume > 9736.50 | | | | | | | | | | |--- class: 1 | | | | | | |--- Month > 11.50 | | | | | | | |--- PMVolume <= 3003.00 | | | | | | | | |--- class: 1 | | | | | | | |--- PMVolume > 3003.00 | | | | | | | | |--- class: 1 | |--- PctCars > 95.75 | | |--- PMVolume <= 2022.50 | | | |--- X <= -13649647.50 | | | | |--- X <= -13659285.00 | | | | | |--- PctCars <= 96.35 | | | | | | |--- class: 1 | | | | | |--- PctCars > 96.35 | | | | | | |--- ADTVolume <= 1979.50 | | | | | | | |--- class: 1 | | | | | | |--- ADTVolume > 1979.50 | | | | | | | |--- class: 1 | | | | |--- X > -13659285.00 | | | | | |--- Y <= 5708073.25 | | | | | | |--- X <= -13658741.00 | | | | | | | |--- class: 0 | | | | | | |--- X > -13658741.00 | | | | | | | |--- Y <= 5699886.00 | | | | | | | | |--- Y <= 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | | |--- Y > 5696777.75 | | | | | | | | | |--- class: 1 | | | | | | | |--- Y > 5699886.00 | | | | | | | | |--- X <= -13655722.00 | | | | | | | | | |--- class: 0 | | | | | | | | |--- X > -13655722.00 | | | | | | | | | |--- PctCars <= 98.35 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- PctCars > 98.35 | | | | | | | | | | |--- class: 0 | | | | | |--- Y > 5708073.25 | | | | | | |--- class: 1 | | | |--- X > -13649647.50 | | | | |--- Month <= 8.50 | | | | | |--- PMPkHrVol <= 190.50 | | | | | | |--- class: 1 | | | | | |--- PMPkHrVol > 190.50 | | | | | | |--- class: 1 | | | | |--- Month > 8.50 | | | | | |--- class: 1 | | |--- PMVolume > 2022.50 | | | |--- X <= -13651133.00 | | | | |--- Y <= 5698353.75 | | | | | |--- class: 1 | | | | |--- Y > 5698353.75 | | | | | |--- ADTVolume <= 3321.00 | | | | | | |--- class: 1 | | | | | |--- ADTVolume > 3321.00 | | | | | | |--- PMPkHrVol <= 382.50 | | | | | | | |--- class: 1 | | | | | | |--- PMPkHrVol > 382.50 | | | | | | | |--- ADTVolume <= 6976.00 | | | | | | | | |--- X <= -13659386.50 | | | | | | | | | |--- class: 1 | | | | | | | | |--- X > -13659386.50 | | | | | | | | | |--- X <= -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | | | |--- X > -13655631.50 | | | | | | | | | | |--- class: 1 | | | | | | | |--- ADTVolume > 6976.00 | | | | | | | | |--- class: 1 | | | |--- X > -13651133.00 | | | | |--- ADTVolume <= 7612.00 | | | | | |--- Month <= 9.50 | | | | | | |--- class: 1 | | | | | |--- Month > 9.50 | | | | | | |--- class: 1 | | | | |--- ADTVolume > 7612.00 | | | | | |--- class: 1 # Tree Rules Figure plt.figure(figsize=(24, 16)) plot_tree(tree_model, feature_names=list(X_train.columns), class_names=['Over10Speeing', 'NoSpeedingOver10'], filled=True, rounded=True, proportion=True) plt.show() # Sets for iteration datasets = [(X_train, y_train, \"Training\"), (X_test, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Predictions tree_predictions = tree_model.predict(data) tree_conf_matrix = confusion_matrix(labels, tree_predictions) # Performance Metrics accuracy = round(accuracy_score(labels, tree_predictions), 3) precision = round(precision_score(labels, tree_predictions), 3) recall = round(recall_score(labels, tree_predictions), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {accuracy}\") print(f\"Precision: {precision}\") print(f\"Recall: {recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in tree_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in tree_conf_matrix.flatten() / np.sum(tree_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(tree_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Decision Tree Confusion Matrix - {dataset_name} Data') plt.show() Training Performance: Accuracy: 0.872 Precision: 0.899 Recall: 0.948 Training Performance: Accuracy: 0.872 Precision: 0.899 Recall: 0.948 Testing Performance: Accuracy: 0.826 Precision: 0.874 Recall: 0.916 Testing Performance: Accuracy: 0.826 Precision: 0.874 Recall: 0.916 # Identify the most important features importances = tree_model.feature_importances_ top_features = np.argsort(importances)[::-1][:5] # Where 5 is the number of features you want to select # Print the top features print(\"Top Features:\") for feature_index in top_features: print(X_train.columns[feature_index]) # keep top features & scale the data scaler = StandardScaler() X_train_final = scaler.fit_transform(X_train.iloc[:, top_features]) X_test_final = scaler.transform(X_test.iloc[:, top_features]) Top Features: AMVolume Y X PctCars PMVolume Top Features: AMVolume Y X PctCars PMVolume","title":"Decision Tree"},{"location":"Traffic_Analysis/Traffic_Analysis/#new-top-5-predictors-based-on-the-decision-tree-utilised-for-each-model-from-here-on","text":"# Save the top features into a new predictor X X_top_features = X.iloc[:, top_features] # Split the data into training and testing sets using new predictors selected in decision tree X_train, X_test, y_train, y_test = train_test_split(X_top_features, y, test_size=0.2, random_state=17, stratify=y)","title":"New top 5 predictors based on the decision tree, utilised for each model from here on"},{"location":"Traffic_Analysis/Traffic_Analysis/#random-forest","text":"# Train & Predict rf_model = RandomForestRegressor(random_state=17) rf_model.fit(X_train, y_train) train_predictions = rf_model.predict(X_train) test_predictions = rf_model.predict(X_test) # Performance print(\"Performance Metrics:\\n\") # Training Set print(\"Training Performance:\") print(f\"MSE: {mean_squared_error(y_train, train_predictions)}\") print(f\"R-squared: {r2_score(y_train, train_predictions)}\") # Testing Set print(\"\\nTesting Performance:\") print(f\"MSE: {mean_squared_error(y_test, test_predictions)}\") print(f\"R-squared: {r2_score(y_test, test_predictions)}\") Performance Metrics: Training Performance: MSE: 0.014233238837703754 R-squared: 0.9103972386141846 Testing Performance: MSE: 0.11146888731396173 R-squared: 0.29732085628886595 Performance Metrics: Training Performance: MSE: 0.014233238837703754 R-squared: 0.9103972386141846 Testing Performance: MSE: 0.11146888731396173 R-squared: 0.29732085628886595","title":"Random Forest"},{"location":"Traffic_Analysis/Traffic_Analysis/#logistic-regression_1","text":"# Train the model logistic_model = LogisticRegression(random_state=17) logistic_model.fit(X_train, y_train) logistic_train_predictions = logistic_model.predict(X_train) logistic_test_predictions = logistic_model.predict(X_test) # Print Intercept print(f\"Y-Intercept Value: {logistic_model.intercept_[0]}\") # Print Coefficients coefficients_df = pd.DataFrame({'Predictor': X_top_features.columns, 'Coefficient': logistic_model.coef_[0]}) print(\"\\nCoefficients:\") print(coefficients_df) Y-Intercept Value: 3.430078655417783e-09 Coefficients: Predictor Coefficient 0 AMVolume 0.000619 1 Y 0.000033 2 X 0.000014 3 PctCars -0.000060 4 PMVolume 0.000718 Y-Intercept Value: 3.430078655417783e-09 Coefficients: Predictor Coefficient 0 AMVolume 0.000619 1 Y 0.000033 2 X 0.000014 3 PctCars -0.000060 4 PMVolume 0.000718 # LOGISTIC REGRESSION PERFORMANCE print(\"Logistic Regression Performance Metrics:\\n\") # Sets for iteration datasets = [(X_train, y_train, \"Training\"), (X_test, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Predictions logistic_predictions = logistic_model.predict(data) logistic_conf_matrix = confusion_matrix(labels, logistic_predictions) # Performance Metrics logistic_accuracy = round(accuracy_score(labels, logistic_predictions), 3) logistic_precision = round(precision_score(labels, logistic_predictions), 3) logistic_recall = round(recall_score(labels, logistic_predictions), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {logistic_accuracy}\") print(f\"Precision: {logistic_precision}\") print(f\"Recall: {logistic_recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in logistic_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in logistic_conf_matrix.flatten() / np.sum(logistic_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(logistic_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Logistic Regression Confusion Matrix - {dataset_name} Data') plt.show() Logistic Regression Performance Metrics: Training Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.998 Logistic Regression Performance Metrics: Training Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.998 Testing Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.997 Testing Performance: Accuracy: 0.802 Precision: 0.803 Recall: 0.997","title":"Logistic Regression"},{"location":"Traffic_Analysis/Traffic_Analysis/#k-nn-classifier","text":"# Standardize scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the KNN classifier knn_classifier = KNeighborsClassifier(n_neighbors=5) knn_classifier.fit(X_train_scaled, y_train) # Predict train_predictions = knn_classifier.predict(X_train_scaled) test_predictions = knn_classifier.predict(X_test_scaled) # KNN PERFORMANCE print(\"KNN Performance Metrics:\\n\") # Sets for iteration datasets = [(X_train_scaled, y_train, \"Training\"), (X_test_scaled, y_test, \"Testing\")] for data, labels, dataset_name in datasets: # Convert labels to integers labels = labels.astype(int) # Predictions knn_predictions = knn_classifier.predict(data) knn_conf_matrix = confusion_matrix(labels, knn_predictions) # Performance Metrics knn_accuracy = round(accuracy_score(labels, knn_predictions), 3) knn_precision = round(precision_score(labels, knn_predictions, average='binary'), 3) knn_recall = round(recall_score(labels, knn_predictions, average='binary'), 3) print(f\"\\n{dataset_name} Performance:\") print(f\"Accuracy: {knn_accuracy}\") print(f\"Precision: {knn_precision}\") print(f\"Recall: {knn_recall}\") # Confusion Matrix group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos'] group_counts = [\"{0:0.0f}\".format(value) for value in knn_conf_matrix.flatten()] group_percentages = [\"{0:.0%}\".format(value) for value in knn_conf_matrix.flatten() / np.sum(knn_conf_matrix)] labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)] labels = np.asarray(labels).reshape(2, 2) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(knn_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'KNN Classifier Confusion Matrix - {dataset_name} Data') plt.show() KNN Performance Metrics: Training Performance: Accuracy: 0.881 Precision: 0.91 Recall: 0.944 KNN Performance Metrics: Training Performance: Accuracy: 0.881 Precision: 0.91 Recall: 0.944 Testing Performance: Accuracy: 0.811 Precision: 0.869 Recall: 0.9 Testing Performance: Accuracy: 0.811 Precision: 0.869 Recall: 0.9","title":"K-NN Classifier"},{"location":"Traffic_Analysis/Traffic_Analysis/#all-models-comparison","text":"# Initialize lists to store metrics model_names = [\"Decision Tree\", \"Logistic Regression\", \"KNN CLassifier\"] accuracy_list = [] precision_list = [] recall_list = [] # Append DT metrics to lists accuracy_list.append(accuracy) precision_list.append(precision) recall_list.append(recall) # Append Logistic metrics to lists accuracy_list.append(logistic_accuracy) precision_list.append(logistic_precision) recall_list.append(logistic_recall) # Append KNN metrics to lists accuracy_list.append(knn_accuracy) precision_list.append(knn_precision) recall_list.append(knn_recall) table = PrettyTable() table.field_names = [\"Model\", \"Dataset\", \"Accuracy\", \"Precision\", \"Recall\"] for model_name, accuracy, precision, recall in zip(model_names, accuracy_list, precision_list, recall_list): table.add_row([model_name, \"Testing\", accuracy, precision, recall]) print(table) +---------------------+---------+----------+-----------+--------+ | Model | Dataset | Accuracy | Precision | Recall | +---------------------+---------+----------+-----------+--------+ | Decision Tree | Testing | 0.826 | 0.874 | 0.916 | | Logistic Regression | Testing | 0.802 | 0.803 | 0.997 | | KNN CLassifier | Testing | 0.811 | 0.869 | 0.9 | +---------------------+---------+----------+-----------+--------+ +---------------------+---------+----------+-----------+--------+ | Model | Dataset | Accuracy | Precision | Recall | +---------------------+---------+----------+-----------+--------+ | Decision Tree | Testing | 0.826 | 0.874 | 0.916 | | Logistic Regression | Testing | 0.802 | 0.803 | 0.997 | | KNN CLassifier | Testing | 0.811 | 0.869 | 0.9 | +---------------------+---------+----------+-----------+--------+","title":"All Models Comparison"},{"location":"Traffic_Analysis/Traffic_Analysis/#stacked-ensemble_1","text":"# set base models to stack base_models = [ ('decision_tree', RandomForestRegressor(random_state=17)), ('logistic_regression', LogisticRegression(random_state=17)), ('knn_classifier', KNeighborsClassifier(n_neighbors=5)) ] # define the meta-learner meta_learner = LogisticRegression(random_state=42) # create & train the stacking ensemble stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5) stacking_ensemble.fit(X_train, y_train) # make predictions and evaluate the ensemble model stacked_predictions = stacking_ensemble.predict(X_test) stacked_accuracy = round(accuracy_score(y_test, stacked_predictions), 3) stacked_precision = round(precision_score(y_test, stacked_predictions), 3) stacked_recall = round(recall_score(y_test, stacked_predictions), 3) # print the results print(\"Stacking Ensemble Performance:\") print(f\" Accuracy: {stacked_accuracy}\") print(f\" Precision: {stacked_precision}\") print(f\" Recall: {stacked_recall}\") /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( Stacking Ensemble Performance: Accuracy: 0.844 Precision: 0.883 Recall: 0.928 Stacking Ensemble Performance: Accuracy: 0.844 Precision: 0.883 Recall: 0.928 stacked_conf_matrix = confusion_matrix(y_test, stacked_predictions) # Plot Confusion Matrix plt.figure(figsize=(8, 6)) sns.heatmap(stacked_conf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False) plt.title(f'Stacked Ensemble Confusion Matrix - Testing Data') plt.show()","title":"Stacked Ensemble"}]}